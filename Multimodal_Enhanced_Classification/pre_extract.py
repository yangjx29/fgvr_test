# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import argparse
# å¯¼å…¥PILå›¾åƒå¤„ç†åº“
from PIL import Image
# å¯¼å…¥è¿›åº¦æ¡æ˜¾ç¤ºåº“
from tqdm import tqdm
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—
import os
import json
import sys
# å¯¼å…¥YAMLé…ç½®æ–‡ä»¶å¤„ç†æ¨¡å—
import yaml

# å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch
import torch.nn.parallel
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
# å¯¼å…¥PyTorchçš„å›¾åƒå˜æ¢æ¨¡å—
import torchvision.transforms as transforms

# å°è¯•ä»torchvisionå¯¼å…¥æ’å€¼æ¨¡å¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
try:
    from torchvision.transforms import InterpolationMode
    BICUBIC = InterpolationMode.BICUBIC
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ˆæ—§ç‰ˆæœ¬ï¼‰ï¼Œä½¿ç”¨PILçš„åŒä¸‰æ¬¡æ’å€¼
    BICUBIC = Image.BICUBIC

# å¯¼å…¥æ•°æ®å¢å¼ºå™¨å’Œæ•°æ®é›†æ„å»ºå‡½æ•°
from data.datautils import Augmenter, build_dataset
# å¯¼å…¥éšæœºç§å­è®¾ç½®å·¥å…·
from utils.tools import set_random_seed

# å¯¼å…¥CLIPæ¨¡å‹
from clip import clip

# è¯»å–é…ç½®æ–‡ä»¶
def load_config(config_path="./config.yaml"):
    """
    è¯»å–YAMLé…ç½®æ–‡ä»¶
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        print(f"âš ï¸  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "k_shot_image_processing": "average",
            "similarity_processing": "image_text_pair"
        }
# åŠ è½½CLIPæ¨¡å‹åˆ°CPU
# å‚æ•°:
#   arch: æ¨¡å‹æ¶æ„åç§°ï¼ˆå¦‚'ViT-B/16'ï¼‰
# è¿”å›:
#   model: åŠ è½½å¥½çš„CLIPæ¨¡å‹
def load_clip_to_cpu(arch):
    # è·å–æ¨¡å‹çš„ä¸‹è½½URL
    url = clip._MODELS[arch]
    # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    model_path = clip._download(url)
    try:
        # å°è¯•åŠ è½½JITç¼–è¯‘çš„æ¨¡å‹
        model = torch.jit.load(model_path, map_location="cpu").eval()
        state_dict = None
    except RuntimeError:
        # å¦‚æœJITåŠ è½½å¤±è´¥ï¼Œåˆ™åŠ è½½çŠ¶æ€å­—å…¸
        state_dict = torch.load(model_path, map_location="cpu")
    # æ„å»ºCLIPæ¨¡å‹
    model = clip.build_model(state_dict or model.state_dict())
    return model

# ç®€åŒ–çš„ç‰¹å¾æå–å‡½æ•°ï¼Œä¸“ä¸ºdiscovering.pyé›†æˆè®¾è®¡
@torch.no_grad()
def pre_extract_multimodal_feature(retrieved_loader, test_loader, clip_model, args):
    """
    ç®€åŒ–çš„å¤šæ¨¡æ€ç‰¹å¾æå–å‡½æ•°
    ä¸“ä¸ºä¸discovering.pyå¿«æ…¢æ€è€ƒç³»ç»Ÿé›†æˆè€Œè®¾è®¡
    
    ä¿®æ”¹æ”¯æŒï¼š
    - æ¯ä¸ªç±»åˆ«å¤„ç†kå¼ å›¾åƒï¼ˆä»category_image_paths.jsonè·å–ï¼‰
    - æ„å»ºæ‰€æœ‰kå¼ å›¾åƒçš„å¢å¼ºå­å›¾å’Œæè¿°
    - æ”¯æŒæ‰¹é‡ç‰¹å¾æå–å’ŒåŠ æƒç›¸ä¼¼åº¦è®¡ç®—
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = f"./pre_extracted_feat/{args.arch.replace('/', '')}/seed{args.seed}"
    os.makedirs(save_dir, exist_ok=True)

    # æ„å»ºæè¿°æ–‡ä»¶è·¯å¾„
    descriptions_dir = "./descriptions"
    retrieved_desc_file = os.path.join(descriptions_dir, f"{args.test_set}_retrieved_descriptions.json")
    test_desc_file = os.path.join(descriptions_dir, f"{args.test_set}_test_descriptions.json")
    
    print(f"æ£€æŸ¥æè¿°æ–‡ä»¶:")
    print(f"  æ£€ç´¢æè¿°: {retrieved_desc_file}")
    print(f"  æµ‹è¯•æè¿°: {test_desc_file}")
    
    # æ£€æŸ¥æè¿°æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(retrieved_desc_file):
        print(f"âŒ æ£€ç´¢æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {retrieved_desc_file}")
        return False
    if not os.path.exists(test_desc_file):
        print(f"âŒ æµ‹è¯•æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {test_desc_file}")
        return False
    
    # åŠ è½½æè¿°æ–‡ä»¶
    try:
        with open(retrieved_desc_file, 'r', encoding='utf-8') as f:
            retrieved_descriptions = json.load(f)
        with open(test_desc_file, 'r', encoding='utf-8') as f:
            test_descriptions = json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½æè¿°æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    print(f"âœ… æˆåŠŸåŠ è½½æè¿°æ–‡ä»¶:")
    print(f"  æ£€ç´¢æè¿°: {len(retrieved_descriptions)} æ¡")
    print(f"  æµ‹è¯•æè¿°: {len(test_descriptions)} æ¡")

    # å­˜å‚¨æ‰€æœ‰æå–çš„å¤šæ¨¡æ€ç‰¹å¾
    all_retrieved_data = []  # æ£€ç´¢åˆ°çš„[å›¾-æ–‡]ç‰¹å¾
    all_test_data = []       # å¾…æµ‹è¯•çš„[å›¾-æ–‡]ç‰¹å¾
    
    # å¤„ç†æ£€ç´¢å›¾åƒåŠå…¶æè¿° - æ”¯æŒæ¯ä¸ªç±»åˆ«kå¼ å›¾åƒ
    print("ğŸ”„ å¤„ç†æ£€ç´¢å›¾åƒ-æ–‡æœ¬å¯¹ï¼ˆæ”¯æŒæ¯ç±»åˆ«kå¼ å›¾åƒï¼‰...")
    try:
        # æŒ‰ç±»åˆ«åˆ†ç»„å¤„ç†æ£€ç´¢æ•°æ®
        category_features = {}  # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„æ‰€æœ‰å›¾åƒç‰¹å¾
        
        for i, (images, target) in enumerate(tqdm(retrieved_loader, desc="Processing retrieved")):
            # å®‰å…¨å¤„ç†å›¾åƒåˆ—è¡¨
            if isinstance(images, list):
                # å°†æ‰€æœ‰è§†å›¾ç§»åˆ°GPU
                for k in range(len(images)):
                    images[k] = images[k].cuda(non_blocking=True)
                # æ‹¼æ¥æ‰€æœ‰è§†å›¾
                images = torch.cat(images, dim=0)
            else:
                # å•å¼ å›¾åƒ
                images = images.cuda(non_blocking=True)
            
            # å°†æ ‡ç­¾ç§»åˆ°GPU
            target = target.cuda(non_blocking=True)
            target_item = target.item()

            # ä½¿ç”¨æ··åˆç²¾åº¦æå–å¤šæ¨¡æ€ç‰¹å¾
            with torch.cuda.amp.autocast():
                # ä½¿ç”¨CLIPç¼–ç å›¾åƒ
                image_features = clip_model.encode_image(images)
                # L2å½’ä¸€åŒ–ç‰¹å¾
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # è·å–å¯¹åº”çš„æ–‡æœ¬æè¿°
                desc_key = str(i) if str(i) in retrieved_descriptions else list(retrieved_descriptions.keys())[i % len(retrieved_descriptions)]
                text_description = retrieved_descriptions[desc_key]
                
                # ç¼–ç æ–‡æœ¬æè¿°
                text_tokens = clip.tokenize([text_description], truncate=True).cuda()
                text_features = clip_model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # æ‹¼æ¥å›¾æ–‡ç‰¹å¾ [Ti, Ii]
                if len(image_features.shape) == 1:
                    image_features = image_features.unsqueeze(0)
                text_features_expanded = text_features.expand(image_features.size(0), -1)
                multimodal_features = torch.cat([text_features_expanded, image_features], dim=-1)

            # æŒ‰ç±»åˆ«å­˜å‚¨ç‰¹å¾ï¼ˆæ”¯æŒæ¯ä¸ªç±»åˆ«å¤šå¼ å›¾åƒï¼‰
            if target_item not in category_features:
                category_features[target_item] = []
            category_features[target_item].append((multimodal_features, target))
            
            # æ¯100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"  å·²å¤„ç†æ£€ç´¢æ ·æœ¬: {i + 1}")
        
        # å°†æŒ‰ç±»åˆ«åˆ†ç»„çš„ç‰¹å¾è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
        # æ¯ä¸ªç±»åˆ«çš„æ‰€æœ‰å›¾åƒä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡å¤„ç†
        for category, features_list in category_features.items():
            if len(features_list) > 1:
                # å¤šå¼ å›¾åƒï¼šæ‹¼æ¥æ‰€æœ‰å›¾åƒçš„ç‰¹å¾
                all_features = []
                targets = []
                for feat, tgt in features_list:
                    all_features.append(feat)
                    targets.append(tgt)
                # æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„ç‰¹å¾å¼ é‡ shape: (k*n_views, feature_dim)
                combined_features = torch.cat(all_features, dim=0)
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡ä½œä¸ºä»£è¡¨ï¼ˆæ‰€æœ‰å›¾åƒéƒ½æ˜¯åŒä¸€ç±»åˆ«ï¼‰
                representative_target = targets[0]
                all_retrieved_data.append((combined_features, representative_target))
            else:
                # å•å¼ å›¾åƒï¼šç›´æ¥ä½¿ç”¨
                all_retrieved_data.append(features_list[0])
                
    except Exception as e:
        print(f"âŒ å¤„ç†æ£€ç´¢å›¾åƒå¤±è´¥: {e}")
        return False

    # å¤„ç†æµ‹è¯•å›¾åƒåŠå…¶æè¿°
    print("ğŸ”„ å¤„ç†æµ‹è¯•å›¾åƒ-æ–‡æœ¬å¯¹...")
    try:
        for i, (images, target) in enumerate(tqdm(test_loader, desc="Processing test")):
            # å®‰å…¨å¤„ç†å›¾åƒåˆ—è¡¨
            if isinstance(images, list):
                # å°†æ‰€æœ‰è§†å›¾ç§»åˆ°GPU
                for k in range(len(images)):
                    images[k] = images[k].cuda(non_blocking=True)
                # æ‹¼æ¥æ‰€æœ‰è§†å›¾
                images = torch.cat(images, dim=0)
            else:
                # å•å¼ å›¾åƒ
                images = images.cuda(non_blocking=True)
            
            # å°†æ ‡ç­¾ç§»åˆ°GPU
            target = target.cuda(non_blocking=True)

            # ä½¿ç”¨æ··åˆç²¾åº¦æå–å¤šæ¨¡æ€ç‰¹å¾
            with torch.cuda.amp.autocast():
                # ä½¿ç”¨CLIPç¼–ç å›¾åƒ
                image_features = clip_model.encode_image(images)
                # L2å½’ä¸€åŒ–ç‰¹å¾
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # è·å–å¯¹åº”çš„æ–‡æœ¬æè¿°
                desc_key = str(i) if str(i) in test_descriptions else list(test_descriptions.keys())[i % len(test_descriptions)]
                text_description = test_descriptions[desc_key]
                
                # ç¼–ç æ–‡æœ¬æè¿°
                text_tokens = clip.tokenize([text_description], truncate=True).cuda()
                text_features = clip_model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # æ‹¼æ¥å›¾æ–‡ç‰¹å¾ [T'j, I'j]
                if len(image_features.shape) == 1:
                    image_features = image_features.unsqueeze(0)
                text_features_expanded = text_features.expand(image_features.size(0), -1)
                multimodal_features = torch.cat([text_features_expanded, image_features], dim=-1)

            # ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
            all_test_data.append((multimodal_features, target))
            
            # æ¯100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"  å·²å¤„ç†æµ‹è¯•æ ·æœ¬: {i + 1}")
                
    except Exception as e:
        print(f"âŒ å¤„ç†æµ‹è¯•å›¾åƒå¤±è´¥: {e}")
        return False

    # ä¿å­˜åˆ°æ–‡ä»¶
    try:
        retrieved_save_path = os.path.join(save_dir, f"{args.test_set}_retrieved.pth")
        test_save_path = os.path.join(save_dir, f"{args.test_set}_test.pth")
        
        torch.save(all_retrieved_data, retrieved_save_path)
        torch.save(all_test_data, test_save_path)
        
        print(f"âœ… æˆåŠŸä¿å­˜æ£€ç´¢ç‰¹å¾åˆ°: {retrieved_save_path}")
        print(f"âœ… æˆåŠŸä¿å­˜æµ‹è¯•ç‰¹å¾åˆ°: {test_save_path}")
        print(f"ğŸ“Š æ£€ç´¢æ ·æœ¬: {len(all_retrieved_data)} ä¸ª")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬: {len(all_test_data)} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ç‰¹å¾æ–‡ä»¶å¤±è´¥: {e}")
        return False


# åŠ è½½JSONæ–‡ä»¶çš„è¾…åŠ©å‡½æ•°
def load_json(file_path):
    import json
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


# ç®€åŒ–çš„ä¸»å·¥ä½œå‡½æ•°ï¼Œä¸“ä¸ºdiscovering.pyé›†æˆè®¾è®¡
def main_worker(args):
    """
    ç®€åŒ–çš„ä¸»å·¥ä½œå‡½æ•°
    ä¸“ä¸ºä¸discovering.pyå¿«æ…¢æ€è€ƒç³»ç»Ÿé›†æˆè€Œè®¾è®¡
    """
    print(f"ğŸš€ å¼€å§‹MECç‰¹å¾é¢„æå–: {args.test_set}")
    print(f"ğŸ“ æ¨¡å‹æ¶æ„: {args.arch}")
    print(f"ğŸ² éšæœºç§å­: {args.seed}")
    
    try:
        # åŠ è½½CLIPæ¨¡å‹
        print("ğŸ”„ åŠ è½½CLIPæ¨¡å‹...")
        clip_model = load_clip_to_cpu(args.arch)
        clip_model = clip_model.cuda()
        clip_model.float()
        clip_model.eval()

        # å†»ç»“æ‰€æœ‰å‚æ•°
        for _, param in clip_model.named_parameters():
            param.requires_grad_(False)
        
        print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")

        # CLIPå½’ä¸€åŒ–å‚æ•°
        normalize = transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711]
        )

        # ç®€åŒ–çš„å›¾åƒå˜æ¢ï¼ˆä¸ä½¿ç”¨è¿‡å¤šå¢å¼ºï¼Œæé«˜ç¨³å®šæ€§ï¼‰
        base_transform = transforms.Compose([
            transforms.Resize(args.resolution, interpolation=BICUBIC),
            transforms.CenterCrop(args.resolution)
        ])
        
        preprocess = transforms.Compose([
            transforms.ToTensor(), 
            normalize
        ])
        
        # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
        data_transform = Augmenter(base_transform, preprocess, n_views=args.batch_size)

        print(f"ğŸ”„ æ„å»ºæ•°æ®é›†: {args.test_set}")
        
        # æ„å»ºæ•°æ®é›† - å¢åŠ é”™è¯¯å¤„ç†
        try:
            retrieved_dataset = build_dataset(f"{args.test_set}_retrieved", data_transform, args.data, mode='test')
            test_dataset = build_dataset(f"{args.test_set}_test", data_transform, args.data, mode='test')
        except Exception as e:
            print(f"âŒ æ„å»ºæ•°æ®é›†å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„å’Œæè¿°æ–‡ä»¶æ˜¯å¦æ­£ç¡®")
            return False
        
        print(f"ğŸ“Š æ£€ç´¢æ ·æœ¬æ•°é‡: {len(retrieved_dataset)}")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_dataset)}")
        
        if len(retrieved_dataset) == 0 or len(test_dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return False
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°‘å¹¶å‘é¿å…é—®é¢˜
        print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        retrieved_loader = torch.utils.data.DataLoader(
            retrieved_dataset,
            batch_size=1, 
            shuffle=False,  
            num_workers=min(args.workers, 2),  # å‡å°‘workeræ•°é‡
            pin_memory=True,
            timeout=60  # å¢åŠ è¶…æ—¶
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=1, 
            shuffle=False,  
            num_workers=min(args.workers, 2),  # å‡å°‘workeræ•°é‡
            pin_memory=True,
            timeout=60  # å¢åŠ è¶…æ—¶
        )
        
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # å¼€å§‹æå–å¤šæ¨¡æ€ç‰¹å¾
        print("ğŸš€ å¼€å§‹æå–å¤šæ¨¡æ€ç‰¹å¾...")
        success = pre_extract_multimodal_feature(retrieved_loader, test_loader, clip_model, args)
        
        if success:
            print("ğŸ‰ MECç‰¹å¾é¢„æå–å®Œæˆ!")
            return True
        else:
            print("âŒ MECç‰¹å¾é¢„æå–å¤±è´¥!")
            return False
            
    except Exception as e:
        print(f"âŒ MECç‰¹å¾é¢„æå–å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


# ä¸»ç¨‹åºå…¥å£
if __name__ == '__main__':
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='Pre-extracting image features')
    parser.add_argument('data', metavar='DIR', help='path to dataset root')  # æ•°æ®é›†æ ¹ç›®å½•
    parser.add_argument('--test_set', type=str, help='dataset name')  # æ•°æ®é›†åç§°
    parser.add_argument('-a', '--arch', metavar='ARCH', default='ViT-B/16')  # æ¨¡å‹æ¶æ„
    parser.add_argument('--resolution', default=224, type=int, help='CLIP image resolution')  # å›¾åƒåˆ†è¾¨ç‡
    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',
                        help='number of data loading workers (default: 4)')  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    parser.add_argument('-b', '--batch-size', default=50, type=int, metavar='N')  # å¢å¼ºè§†å›¾æ•°é‡
    parser.add_argument('--seed', type=int, default=0)  # éšæœºç§å­

    args = parser.parse_args()
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    set_random_seed(args.seed)
    # å¯åŠ¨ä¸»å·¥ä½œå‡½æ•°
    main_worker(args)