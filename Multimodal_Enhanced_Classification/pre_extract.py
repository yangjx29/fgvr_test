# å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
import argparse
# å¯¼å…¥PILå›¾åƒå¤„ç†åº“
from PIL import Image
# å¯¼å…¥è¿›åº¦æ¡æ˜¾ç¤ºåº“
from tqdm import tqdm
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—
import os
import json
import sys
# å¯¼å…¥YAMLé…ç½®æ–‡ä»¶å¤„ç†æ¨¡å—
import yaml

# å¯¼å…¥æ–‡æœ¬æè¿°ç”Ÿæˆå™¨
from text_description_generator import TextDescriptionGenerator

# å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch
import torch.nn.parallel
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
# å¯¼å…¥PyTorchçš„å›¾åƒå˜æ¢æ¨¡å—
import torchvision.transforms as transforms

# å°è¯•ä»torchvisionå¯¼å…¥æ’å€¼æ¨¡å¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
try:
    from torchvision.transforms import InterpolationMode
    BICUBIC = InterpolationMode.BICUBIC
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼ˆæ—§ç‰ˆæœ¬ï¼‰ï¼Œä½¿ç”¨PILçš„åŒä¸‰æ¬¡æ’å€¼
    BICUBIC = Image.BICUBIC

# å¯¼å…¥æ•°æ®å¢å¼ºå™¨å’Œæ•°æ®é›†æ„å»ºå‡½æ•°
from data.datautils import Augmenter, build_dataset
# å¯¼å…¥éšæœºç§å­è®¾ç½®å·¥å…·
from utils.tools import set_random_seed

# å¯¼å…¥CLIPæ¨¡å‹
from clip import clip

# è¯»å–é…ç½®æ–‡ä»¶
def load_config(config_path="./config.yaml"):
    """
    è¯»å–YAMLé…ç½®æ–‡ä»¶
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        print(f"âš ï¸  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "k_shot_image_processing": "average",
            "similarity_processing": "image_text_pair"
        }
# åŠ è½½CLIPæ¨¡å‹åˆ°CPU
# å‚æ•°:
#   arch: æ¨¡å‹æ¶æ„åç§°ï¼ˆå¦‚'ViT-B/16'ï¼‰
# è¿”å›:
#   model: åŠ è½½å¥½çš„CLIPæ¨¡å‹
def load_clip_to_cpu(arch):
    # è·å–æ¨¡å‹çš„ä¸‹è½½URL
    url = clip._MODELS[arch]
    # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    model_path = clip._download(url)
    try:
        # å°è¯•åŠ è½½JITç¼–è¯‘çš„æ¨¡å‹
        model = torch.jit.load(model_path, map_location="cpu").eval()
        state_dict = None
    except RuntimeError:
        # å¦‚æœJITåŠ è½½å¤±è´¥ï¼Œåˆ™åŠ è½½çŠ¶æ€å­—å…¸
        state_dict = torch.load(model_path, map_location="cpu")
    # æ„å»ºCLIPæ¨¡å‹
    model = clip.build_model(state_dict or model.state_dict())
    return model

# ç®€åŒ–çš„ç‰¹å¾æå–å‡½æ•°ï¼Œä¸“ä¸ºdiscovering.pyé›†æˆè®¾è®¡
@torch.no_grad()
def pre_extract_multimodal_feature(retrieved_loader, test_loader, clip_model, args, text_generator=None):
    """
    ç®€åŒ–çš„å¤šæ¨¡æ€ç‰¹å¾æå–å‡½æ•°
    ä¸“ä¸ºä¸discovering.pyå¿«æ…¢æ€è€ƒç³»ç»Ÿé›†æˆè€Œè®¾è®¡
    
    Args:
        text_generator: æ–‡æœ¬æè¿°ç”Ÿæˆå™¨ï¼Œç”¨äºä¸ºå­è§†å›¾ç”Ÿæˆæè¿°
    
    ä¿®æ”¹æ”¯æŒï¼š
    - æ¯ä¸ªç±»åˆ«å¤„ç†kå¼ å›¾åƒï¼ˆä»category_image_paths.jsonè·å–ï¼‰
    - æ„å»ºæ‰€æœ‰kå¼ å›¾åƒçš„å¢å¼ºå­å›¾å’Œæè¿°
    - æ”¯æŒæ‰¹é‡ç‰¹å¾æå–å’ŒåŠ æƒç›¸ä¼¼åº¦è®¡ç®—
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = f"./pre_extracted_feat/{args.arch.replace('/', '')}/seed{args.seed}"
    os.makedirs(save_dir, exist_ok=True)

    # æ„å»ºæè¿°æ–‡ä»¶è·¯å¾„
    descriptions_dir = "./descriptions"
    retrieved_desc_file = os.path.join(descriptions_dir, f"{args.test_set}_retrieved_descriptions.json")
    test_desc_file = os.path.join(descriptions_dir, f"{args.test_set}_test_descriptions.json")
    
    print(f"æ£€æŸ¥æè¿°æ–‡ä»¶:")
    print(f"  æ£€ç´¢æè¿°: {retrieved_desc_file}")
    print(f"  æµ‹è¯•æè¿°: {test_desc_file}")
    
    # æ£€æŸ¥æè¿°æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(retrieved_desc_file):
        print(f"âŒ æ£€ç´¢æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {retrieved_desc_file}")
        return False
    if not os.path.exists(test_desc_file):
        print(f"âŒ æµ‹è¯•æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {test_desc_file}")
        return False
    
    # åŠ è½½æè¿°æ–‡ä»¶
    try:
        with open(retrieved_desc_file, 'r', encoding='utf-8') as f:
            retrieved_descriptions_raw = json.load(f)
        with open(test_desc_file, 'r', encoding='utf-8') as f:
            test_descriptions_raw = json.load(f)
        
        # å¤„ç†å¯èƒ½çš„åˆ—è¡¨æ ¼å¼ï¼ˆç”±dump_jsonäº§ç”Ÿï¼‰
        if isinstance(retrieved_descriptions_raw, list) and len(retrieved_descriptions_raw) > 0:
            retrieved_descriptions = retrieved_descriptions_raw[0]
        else:
            retrieved_descriptions = retrieved_descriptions_raw
            
        if isinstance(test_descriptions_raw, list) and len(test_descriptions_raw) > 0:
            test_descriptions = test_descriptions_raw[0]
        else:
            test_descriptions = test_descriptions_raw
            
    except Exception as e:
        print(f"âŒ åŠ è½½æè¿°æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    print(f"âœ… æˆåŠŸåŠ è½½æè¿°æ–‡ä»¶:")
    print(f"  æ£€ç´¢æè¿°: {len(retrieved_descriptions)} æ¡")
    print(f"  æµ‹è¯•æè¿°: {len(test_descriptions)} æ¡")

    # ç”Ÿæˆå­è§†å›¾æ–‡æœ¬æè¿°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    generated_descriptions = {}
    if text_generator and text_generator.is_generate:
        print("ğŸ”„ ä¸ºå­è§†å›¾ç”Ÿæˆæ–‡æœ¬æè¿°...")
        try:
            # ä¸ºå½“å‰æ•°æ®é›†ç”Ÿæˆæè¿°
            generated_descriptions = text_generator.generate_dataset_descriptions(
                data_dir=args.data,
                dataset_name=args.test_set,
                cache_dir="./description_cache"
            )
            print(f"âœ… ç”Ÿæˆäº† {len(generated_descriptions)} ä¸ªå›¾åƒæè¿°")
        except Exception as e:
            print(f"âš ï¸  æ–‡æœ¬æè¿°ç”Ÿæˆå¤±è´¥: {e}")
            generated_descriptions = {}
    else:
        print("âš ï¸  æ–‡æœ¬æè¿°ç”ŸæˆåŠŸèƒ½å·²ç¦ç”¨")

    # å­˜å‚¨æ‰€æœ‰æå–çš„å¤šæ¨¡æ€ç‰¹å¾
    all_retrieved_data = []  # æ£€ç´¢åˆ°çš„[å›¾-æ–‡]ç‰¹å¾
    all_test_data = []       # å¾…æµ‹è¯•çš„[å›¾-æ–‡]ç‰¹å¾
    
    # å¤„ç†æ£€ç´¢å›¾åƒåŠå…¶æè¿° - æ”¯æŒæ¯ä¸ªç±»åˆ«kå¼ å›¾åƒ
    print("ğŸ”„ å¤„ç†æ£€ç´¢å›¾åƒ-æ–‡æœ¬å¯¹ï¼ˆæ”¯æŒæ¯ç±»åˆ«kå¼ å›¾åƒï¼‰...")
    try:
        # æŒ‰ç±»åˆ«åˆ†ç»„å¤„ç†æ£€ç´¢æ•°æ®
        category_features = {}  # å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„æ‰€æœ‰å›¾åƒç‰¹å¾
        
        for i, batch_data in enumerate(tqdm(retrieved_loader, desc="Processing retrieved")):
            try:
                # å®‰å…¨è§£åŒ…æ‰¹æ¬¡æ•°æ®
                if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 2:
                    images, target = batch_data[0], batch_data[1]
                else:
                    print(f"âŒ æ‰¹æ¬¡æ•°æ®æ ¼å¼é”™è¯¯: {type(batch_data)}, è·³è¿‡")
                    continue
                
                print(f"Debug: batch {i}, images type={type(images)}, target type={type(target)}")
                
                # å®‰å…¨å¤„ç†å›¾åƒæ•°æ®
                if isinstance(images, list):
                    # å¤„ç†å›¾åƒåˆ—è¡¨
                    valid_images = []
                    for k, img in enumerate(images):
                        if hasattr(img, 'cuda'):
                            valid_images.append(img.cuda(non_blocking=True))
                        else:
                            print(f"âŒ å›¾åƒ {k} ä¸æ˜¯tensor: {type(img)}")
                    
                    if valid_images:
                        images = torch.cat(valid_images, dim=0)
                    else:
                        print(f"âŒ æ²¡æœ‰æœ‰æ•ˆå›¾åƒï¼Œè·³è¿‡æ‰¹æ¬¡ {i}")
                        continue
                        
                elif hasattr(images, 'cuda'):
                    # å•å¼ å›¾åƒtensor
                    images = images.cuda(non_blocking=True)
                else:
                    print(f"âŒ å›¾åƒæ•°æ®ä¸æ˜¯tensor: {type(images)}, è·³è¿‡")
                    continue
                
                # å®‰å…¨å¤„ç†æ ‡ç­¾
                if hasattr(target, 'cuda'):
                    target = target.cuda(non_blocking=True)
                    target_item = target.item()
                else:
                    print(f"âŒ æ ‡ç­¾ä¸æ˜¯tensor: {type(target)}, è·³è¿‡")
                    continue
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                continue

            # ä½¿ç”¨æ··åˆç²¾åº¦æå–å¤šæ¨¡æ€ç‰¹å¾
            with torch.cuda.amp.autocast():
                # ä½¿ç”¨CLIPç¼–ç å›¾åƒ
                image_features = clip_model.encode_image(images)
                # L2å½’ä¸€åŒ–ç‰¹å¾
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # è·å–å¯¹åº”çš„æ–‡æœ¬æè¿°
                try:
                    # é¦–å…ˆå°è¯•ä»ç”Ÿæˆçš„æè¿°ä¸­è·å–
                    text_description = None
                    
                    # å°è¯•æ ¹æ®å›¾åƒè·¯å¾„è·å–ç”Ÿæˆçš„æè¿°
                    # è¿™é‡Œéœ€è¦è·å–å½“å‰å›¾åƒçš„å®é™…è·¯å¾„
                    # ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥è·å–è·¯å¾„ï¼Œå…ˆä½¿ç”¨åŸæœ‰é€»è¾‘ï¼Œåç»­å¯ä»¥æ”¹è¿›
                    
                    desc_key = str(i) if str(i) in retrieved_descriptions else list(retrieved_descriptions.keys())[i % len(retrieved_descriptions)]
                    original_description = retrieved_descriptions[desc_key]
                    
                    # å¦‚æœæœ‰ç”Ÿæˆçš„æè¿°ä¸”åŸæè¿°æ˜¯ç‰¹å¾å‘é‡ï¼Œåˆ™ä½¿ç”¨ç”Ÿæˆçš„æè¿°
                    if generated_descriptions and isinstance(original_description, list) and len(original_description) > 0 and isinstance(original_description[0], (int, float)):
                        # åŸæè¿°æ˜¯ç‰¹å¾å‘é‡ï¼Œå°è¯•ä½¿ç”¨ç”Ÿæˆçš„æè¿°
                        # è¿™é‡Œéœ€è¦æ›´å¥½çš„æ˜ å°„æœºåˆ¶ï¼Œæš‚æ—¶ä½¿ç”¨é»˜è®¤æè¿°
                        text_description = "a detailed photo"
                        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤æè¿°æ›¿ä»£ç‰¹å¾å‘é‡")
                    else:
                        text_description = original_description
                    
                    print(f"Debug: desc_key={desc_key}, text_description type={type(text_description)}")
                    
                    # ç¡®ä¿text_descriptionæ˜¯å­—ç¬¦ä¸²
                    if isinstance(text_description, list):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å¾å‘é‡ï¼ˆæ•°å€¼åˆ—è¡¨ï¼‰
                        if len(text_description) > 0 and isinstance(text_description[0], (int, float)):
                            # è¿™æ˜¯ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨é»˜è®¤æè¿°
                            text_description = "a photo"
                            print(f"âš ï¸  æ£€æµ‹åˆ°ç‰¹å¾å‘é‡è€Œéæ–‡æœ¬æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°")
                        else:
                            # è¿™æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                            text_description = text_description[0] if len(text_description) > 0 else "a photo"
                    elif isinstance(text_description, (int, float)):
                        text_description = "a photo"
                        print(f"âš ï¸  æ£€æµ‹åˆ°æ•°å€¼è€Œéæ–‡æœ¬æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°")
                    elif text_description is None:
                        text_description = "a photo"
                    elif not isinstance(text_description, str):
                        text_description = str(text_description)
                        
                    print(f"Debug: final text_description={text_description}")
                except Exception as e:
                    print(f"Debug: Error in text description processing: {e}")
                    text_description = "a photo"
                
                # ç¼–ç æ–‡æœ¬æè¿°
                text_tokens = clip.tokenize([text_description], truncate=True).cuda()
                text_features = clip_model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # æ‹¼æ¥å›¾æ–‡ç‰¹å¾ [Ti, Ii]
                if len(image_features.shape) == 1:
                    image_features = image_features.unsqueeze(0)
                text_features_expanded = text_features.expand(image_features.size(0), -1)
                multimodal_features = torch.cat([text_features_expanded, image_features], dim=-1)

            # æŒ‰ç±»åˆ«å­˜å‚¨ç‰¹å¾ï¼ˆæ”¯æŒæ¯ä¸ªç±»åˆ«å¤šå¼ å›¾åƒï¼‰
            if target_item not in category_features:
                category_features[target_item] = []
            category_features[target_item].append((multimodal_features, target))
            
            # æ¯100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"  å·²å¤„ç†æ£€ç´¢æ ·æœ¬: {i + 1}")
        
        # å°†æŒ‰ç±»åˆ«åˆ†ç»„çš„ç‰¹å¾è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
        # æ¯ä¸ªç±»åˆ«çš„æ‰€æœ‰å›¾åƒä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡å¤„ç†
        for category, features_list in category_features.items():
            if len(features_list) > 1:
                # å¤šå¼ å›¾åƒï¼šæ‹¼æ¥æ‰€æœ‰å›¾åƒçš„ç‰¹å¾
                all_features = []
                targets = []
                for feat, tgt in features_list:
                    all_features.append(feat)
                    targets.append(tgt)
                # æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„ç‰¹å¾å¼ é‡ shape: (k*n_views, feature_dim)
                combined_features = torch.cat(all_features, dim=0)
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡ä½œä¸ºä»£è¡¨ï¼ˆæ‰€æœ‰å›¾åƒéƒ½æ˜¯åŒä¸€ç±»åˆ«ï¼‰
                representative_target = targets[0]
                all_retrieved_data.append((combined_features, representative_target))
            else:
                # å•å¼ å›¾åƒï¼šç›´æ¥ä½¿ç”¨
                all_retrieved_data.append(features_list[0])
                
    except Exception as e:
        print(f"âŒ å¤„ç†æ£€ç´¢å›¾åƒå¤±è´¥: {e}")
        return False

    # å¤„ç†æµ‹è¯•å›¾åƒåŠå…¶æè¿°
    print("ğŸ”„ å¤„ç†æµ‹è¯•å›¾åƒ-æ–‡æœ¬å¯¹...")
    try:
        for i, batch_data in enumerate(tqdm(test_loader, desc="Processing test")):
            try:
                # å®‰å…¨è§£åŒ…æ‰¹æ¬¡æ•°æ®
                if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 2:
                    images, target = batch_data[0], batch_data[1]
                else:
                    print(f"âŒ æµ‹è¯•æ‰¹æ¬¡æ•°æ®æ ¼å¼é”™è¯¯: {type(batch_data)}, è·³è¿‡")
                    continue
                
                # å®‰å…¨å¤„ç†å›¾åƒæ•°æ®
                if isinstance(images, list):
                    # å¤„ç†å›¾åƒåˆ—è¡¨
                    valid_images = []
                    for k, img in enumerate(images):
                        if hasattr(img, 'cuda'):
                            valid_images.append(img.cuda(non_blocking=True))
                        else:
                            print(f"âŒ æµ‹è¯•å›¾åƒ {k} ä¸æ˜¯tensor: {type(img)}")
                    
                    if valid_images:
                        images = torch.cat(valid_images, dim=0)
                    else:
                        print(f"âŒ æ²¡æœ‰æœ‰æ•ˆæµ‹è¯•å›¾åƒï¼Œè·³è¿‡æ‰¹æ¬¡ {i}")
                        continue
                        
                elif hasattr(images, 'cuda'):
                    # å•å¼ å›¾åƒtensor
                    images = images.cuda(non_blocking=True)
                else:
                    print(f"âŒ æµ‹è¯•å›¾åƒæ•°æ®ä¸æ˜¯tensor: {type(images)}, è·³è¿‡")
                    continue
                
                # å®‰å…¨å¤„ç†æ ‡ç­¾
                if hasattr(target, 'cuda'):
                    target = target.cuda(non_blocking=True)
                    target_item = target.item()
                else:
                    print(f"âŒ æµ‹è¯•æ ‡ç­¾ä¸æ˜¯tensor: {type(target)}, è·³è¿‡")
                    continue
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æµ‹è¯•æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
                continue

            # ä½¿ç”¨æ··åˆç²¾åº¦æå–å¤šæ¨¡æ€ç‰¹å¾
            with torch.cuda.amp.autocast():
                # ä½¿ç”¨CLIPç¼–ç å›¾åƒ
                image_features = clip_model.encode_image(images)
                # L2å½’ä¸€åŒ–ç‰¹å¾
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # è·å–å¯¹åº”çš„æ–‡æœ¬æè¿°
                desc_key = str(i) if str(i) in test_descriptions else list(test_descriptions.keys())[i % len(test_descriptions)]
                text_description = test_descriptions[desc_key]
                
                # ç¡®ä¿text_descriptionæ˜¯å­—ç¬¦ä¸²
                if isinstance(text_description, list):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å¾å‘é‡ï¼ˆæ•°å€¼åˆ—è¡¨ï¼‰
                    if len(text_description) > 0 and isinstance(text_description[0], (int, float)):
                        # è¿™æ˜¯ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨é»˜è®¤æè¿°
                        text_description = "a photo"
                        print(f"âš ï¸  æµ‹è¯•æ•°æ®æ£€æµ‹åˆ°ç‰¹å¾å‘é‡è€Œéæ–‡æœ¬æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°")
                    else:
                        # è¿™æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                        text_description = text_description[0] if len(text_description) > 0 else "a photo"
                elif isinstance(text_description, (int, float)):
                    text_description = "a photo"
                    print(f"âš ï¸  æµ‹è¯•æ•°æ®æ£€æµ‹åˆ°æ•°å€¼è€Œéæ–‡æœ¬æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°")
                elif text_description is None:
                    text_description = "a photo"
                elif not isinstance(text_description, str):
                    text_description = str(text_description)
                
                # ç¼–ç æ–‡æœ¬æè¿°
                text_tokens = clip.tokenize([text_description], truncate=True).cuda()
                text_features = clip_model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # æ‹¼æ¥å›¾æ–‡ç‰¹å¾ [T'j, I'j]
                if len(image_features.shape) == 1:
                    image_features = image_features.unsqueeze(0)
                text_features_expanded = text_features.expand(image_features.size(0), -1)
                multimodal_features = torch.cat([text_features_expanded, image_features], dim=-1)

            # ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
            all_test_data.append((multimodal_features, target))
            
            # æ¯100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"  å·²å¤„ç†æµ‹è¯•æ ·æœ¬: {i + 1}")
                
    except Exception as e:
        print(f"âŒ å¤„ç†æµ‹è¯•å›¾åƒå¤±è´¥: {e}")
        return False

    # ä¿å­˜åˆ°æ–‡ä»¶
    try:
        retrieved_save_path = os.path.join(save_dir, f"{args.test_set}_retrieved.pth")
        test_save_path = os.path.join(save_dir, f"{args.test_set}_test.pth")
        
        torch.save(all_retrieved_data, retrieved_save_path)
        torch.save(all_test_data, test_save_path)
        
        print(f"âœ… æˆåŠŸä¿å­˜æ£€ç´¢ç‰¹å¾åˆ°: {retrieved_save_path}")
        print(f"âœ… æˆåŠŸä¿å­˜æµ‹è¯•ç‰¹å¾åˆ°: {test_save_path}")
        print(f"ğŸ“Š æ£€ç´¢æ ·æœ¬: {len(all_retrieved_data)} ä¸ª")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬: {len(all_test_data)} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ç‰¹å¾æ–‡ä»¶å¤±è´¥: {e}")
        return False


# åŠ è½½JSONæ–‡ä»¶çš„è¾…åŠ©å‡½æ•°
def load_json(file_path):
    import json
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


# ç®€åŒ–çš„ä¸»å·¥ä½œå‡½æ•°ï¼Œä¸“ä¸ºdiscovering.pyé›†æˆè®¾è®¡
def main_worker(args):
    """
    ç®€åŒ–çš„ä¸»å·¥ä½œå‡½æ•°
    ä¸“ä¸ºä¸discovering.pyå¿«æ…¢æ€è€ƒç³»ç»Ÿé›†æˆè€Œè®¾è®¡
    """
    print(f"ğŸš€ å¼€å§‹MECç‰¹å¾é¢„æå–: {args.test_set}")
    print(f"ğŸ“ æ¨¡å‹æ¶æ„: {args.arch}")
    print(f"ğŸ² éšæœºç§å­: {args.seed}")
    
    # åˆå§‹åŒ–æ–‡æœ¬æè¿°ç”Ÿæˆå™¨
    print("ğŸ”„ åˆå§‹åŒ–æ–‡æœ¬æè¿°ç”Ÿæˆå™¨...")
    try:
        text_generator = TextDescriptionGenerator(device='cuda', device_id=0)
        print("âœ… æ–‡æœ¬æè¿°ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸  æ–‡æœ¬æè¿°ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        text_generator = None
    
    try:
        # åŠ è½½CLIPæ¨¡å‹
        print("ğŸ”„ åŠ è½½CLIPæ¨¡å‹...")
        clip_model = load_clip_to_cpu(args.arch)
        clip_model = clip_model.cuda()
        clip_model.float()
        clip_model.eval()

        # å†»ç»“æ‰€æœ‰å‚æ•°
        for _, param in clip_model.named_parameters():
            param.requires_grad_(False)
        
        print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")

        # CLIPå½’ä¸€åŒ–å‚æ•°
        normalize = transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711]
        )

        # ç®€åŒ–çš„å›¾åƒå˜æ¢ï¼ˆä¸ä½¿ç”¨è¿‡å¤šå¢å¼ºï¼Œæé«˜ç¨³å®šæ€§ï¼‰
        base_transform = transforms.Compose([
            transforms.Resize(args.resolution, interpolation=BICUBIC),
            transforms.CenterCrop(args.resolution)
        ])
        
        preprocess = transforms.Compose([
            transforms.ToTensor(), 
            normalize
        ])
        
        # åˆ›å»ºæ•°æ®å¢å¼ºå™¨
        data_transform = Augmenter(base_transform, preprocess, n_views=args.batch_size)

        print(f"ğŸ”„ æ„å»ºæ•°æ®é›†: {args.test_set}")
        
        # æ„å»ºæ•°æ®é›† - å¢åŠ é”™è¯¯å¤„ç†
        try:
            retrieved_dataset = build_dataset(f"{args.test_set}_retrieved", data_transform, args.data, mode='test')
            test_dataset = build_dataset(f"{args.test_set}_test", data_transform, args.data, mode='test')
        except Exception as e:
            print(f"âŒ æ„å»ºæ•°æ®é›†å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„å’Œæè¿°æ–‡ä»¶æ˜¯å¦æ­£ç¡®")
            return False
        
        print(f"ğŸ“Š æ£€ç´¢æ ·æœ¬æ•°é‡: {len(retrieved_dataset)}")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_dataset)}")
        
        if len(retrieved_dataset) == 0 or len(test_dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return False
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°‘å¹¶å‘é¿å…é—®é¢˜
        print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        retrieved_loader = torch.utils.data.DataLoader(
            retrieved_dataset,
            batch_size=1, 
            shuffle=False,  
            num_workers=min(args.workers, 2),  # å‡å°‘workeræ•°é‡
            pin_memory=True,
            timeout=60  # å¢åŠ è¶…æ—¶
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=1, 
            shuffle=False,  
            num_workers=min(args.workers, 2),  # å‡å°‘workeræ•°é‡
            pin_memory=True,
            timeout=60  # å¢åŠ è¶…æ—¶
        )
        
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # å¼€å§‹æå–å¤šæ¨¡æ€ç‰¹å¾
        print("ğŸš€ å¼€å§‹æå–å¤šæ¨¡æ€ç‰¹å¾...")
        success = pre_extract_multimodal_feature(retrieved_loader, test_loader, clip_model, args, text_generator)
        
        if success:
            print("ğŸ‰ MECç‰¹å¾é¢„æå–å®Œæˆ!")
            return True
        else:
            print("âŒ MECç‰¹å¾é¢„æå–å¤±è´¥!")
            return False
            
    except Exception as e:
        print(f"âŒ MECç‰¹å¾é¢„æå–å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


# ä¸»ç¨‹åºå…¥å£
if __name__ == '__main__':
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='Pre-extracting image features')
    parser.add_argument('data', metavar='DIR', help='path to dataset root')  # æ•°æ®é›†æ ¹ç›®å½•
    parser.add_argument('--test_set', type=str, help='dataset name')  # æ•°æ®é›†åç§°
    parser.add_argument('-a', '--arch', metavar='ARCH', default='ViT-B/16')  # æ¨¡å‹æ¶æ„
    parser.add_argument('--resolution', default=224, type=int, help='CLIP image resolution')  # å›¾åƒåˆ†è¾¨ç‡
    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',
                        help='number of data loading workers (default: 4)')  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    parser.add_argument('-b', '--batch-size', default=50, type=int, metavar='N')  # å¢å¼ºè§†å›¾æ•°é‡
    parser.add_argument('--seed', type=int, default=0)  # éšæœºç§å­

    args = parser.parse_args()
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    set_random_seed(args.seed)
    # å¯åŠ¨ä¸»å·¥ä½œå‡½æ•°
    success = main_worker(args)
    
    # æ ¹æ®æ‰§è¡Œç»“æœè®¾ç½®é€€å‡ºç 
    if success:
        print("âœ… ç‰¹å¾é¢„æå–æˆåŠŸå®Œæˆ")
        exit(0)
    else:
        print("âŒ ç‰¹å¾é¢„æå–å¤±è´¥")
        exit(1)