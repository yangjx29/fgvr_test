"""
çŸ¥è¯†åº“æ„å»ºæ¨¡å—
ç”¨äºæ„å»ºå¿«æ…¢æ€è€ƒç»†ç²’åº¦å›¾åƒè¯†åˆ«çš„çŸ¥è¯†æ£€ç´¢åº“

åŒ…å«åŠŸèƒ½ï¼š
1. æ„å»ºå›¾åƒçŸ¥è¯†åº“ï¼šåŒ…å«åŸå§‹å›¾åƒã€å¢å¼ºå›¾åƒç­‰
2. æ„å»ºæ–‡æœ¬çŸ¥è¯†åº“ï¼šåŒ…å«ç±»åˆ«æè¿°ã€å±æ€§æè¿°ç­‰
3. æ”¯æŒå¤šç§æ•°æ®å¢å¼ºç­–ç•¥
4. æä¾›æ£€ç´¢æ¥å£
"""

import os
import json
import numpy as np
import torch
from PIL import Image, ImageEnhance, ImageFilter
import random
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import requests
from bs4 import BeautifulSoup
import re
from collections import defaultdict

from agents.mllm_bot import MLLMBot
from retrieval.multimodal_retrieval import MultimodalRetrieval
from utils.fileios import dump_json, load_json, dump_json_override


class KnowledgeBaseBuilder:
    """çŸ¥è¯†åº“æ„å»ºå™¨"""
    
    def __init__(self, 
                 image_encoder_name="./models/Clip/clip-vit-base-patch32",
                 text_encoder_name="./models/Clip/clip-vit-base-patch32", 
                 device="cuda" if torch.cuda.is_available() else "cpu",
                 cfg=None,
                 dataset_info=None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“æ„å»ºå™¨
        
        Args:
            image_encoder_name: å›¾åƒç¼–ç å™¨åç§°
            text_encoder_name: æ–‡æœ¬ç¼–ç å™¨åç§°  
            device: è®¾å¤‡
            cfg: é…ç½®å‚æ•°
        """

        from datetime import datetime
        init_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print("\n================= çŸ¥è¯†åº“æ„å»ºå™¨åˆå§‹åŒ– =================")
        print(f"ğŸ•’ åˆå§‹åŒ–æ—¶é—´: {init_time}")
        print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")
        print(f"ğŸ–¼ï¸ å›¾åƒç¼–ç å™¨: {image_encoder_name}")
        print(f"ğŸ“ æ–‡æœ¬ç¼–ç å™¨: {text_encoder_name}")

        self.device = device
        self.cfg = cfg
        self.dataset_info = dataset_info or {}

        if self.dataset_info:
            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯ keys: {list(self.dataset_info.keys())}")
        else:
            print("ğŸ“Š æ•°æ®é›†ä¿¡æ¯: ç©º")
        
        # åˆå§‹åŒ–æ£€ç´¢æ¨¡å—
        print("ğŸ” åˆå§‹åŒ– MultimodalRetrieval æ¨¡å—...")
        self.retrieval = MultimodalRetrieval(
            image_encoder_name=image_encoder_name,
            text_encoder_name=text_encoder_name,
            fusion_method='weighted',
            device=device
        )
        print("âœ“ æ£€ç´¢æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        
        # çŸ¥è¯†åº“å­˜å‚¨
        self.image_knowledge_base = {}  # {category: [image_features]}
        self.text_knowledge_base = {}   # {category: [text_features]}
        self.category_descriptions = {} # {category: description}
        print("ğŸ“š çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
        print("====================================================\n")
        
    def augment_image(self, image_path: str, augmentation_type: str = "all") -> List[str]:
        """
        å¯¹å›¾åƒè¿›è¡Œæ•°æ®å¢å¼º
        
        Args:
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            augmentation_type: å¢å¼ºç±»å‹ ("all", "rotation", "brightness", "contrast", "blur", "flip")
            
        Returns:
            List[str]: å¢å¼ºåå›¾åƒè·¯å¾„åˆ—è¡¨
        """
        image = Image.open(image_path).convert("RGB")
        augmented_paths = []
        
        # ä¿å­˜åŸå§‹å›¾åƒ
        base_name = os.path.splitext(os.path.basename(image_path))[0]
        base_dir = os.path.dirname(image_path)
        
        if augmentation_type in ["all", "rotation"]:
            # æ—‹è½¬å¢å¼º
            for angle in [90, 180, 270]:
                rotated = image.rotate(angle, expand=True)
                rotated_path = os.path.join(base_dir, f"{base_name}_rot_{angle}.jpg")
                rotated.save(rotated_path)
                augmented_paths.append(rotated_path)
        
        if augmentation_type in ["all", "brightness"]:
            # äº®åº¦å¢å¼º
            for factor in [0.7, 1.3]:
                enhancer = ImageEnhance.Brightness(image)
                bright = enhancer.enhance(factor)
                bright_path = os.path.join(base_dir, f"{base_name}_bright_{factor}.jpg")
                bright.save(bright_path)
                augmented_paths.append(bright_path)
        
        if augmentation_type in ["all", "contrast"]:
            # å¯¹æ¯”åº¦å¢å¼º
            for factor in [0.8, 1.2]:
                enhancer = ImageEnhance.Contrast(image)
                contrast = enhancer.enhance(factor)
                contrast_path = os.path.join(base_dir, f"{base_name}_contrast_{factor}.jpg")
                contrast.save(contrast_path)
                augmented_paths.append(contrast_path)
        
        if augmentation_type in ["all", "blur"]:
            # æ¨¡ç³Šå¢å¼º
            for radius in [1, 2]:
                blurred = image.filter(ImageFilter.GaussianBlur(radius=radius))
                blur_path = os.path.join(base_dir, f"{base_name}_blur_{radius}.jpg")
                blurred.save(blur_path)
                augmented_paths.append(blur_path)
        
        if augmentation_type in ["all", "flip"]:
            # ç¿»è½¬å¢å¼º
            flipped_h = image.transpose(Image.FLIP_LEFT_RIGHT)
            flip_h_path = os.path.join(base_dir, f"{base_name}_flip_h.jpg")
            flipped_h.save(flip_h_path)
            augmented_paths.append(flip_h_path)
            
            flipped_v = image.transpose(Image.FLIP_TOP_BOTTOM)
            flip_v_path = os.path.join(base_dir, f"{base_name}_flip_v.jpg")
            flipped_v.save(flip_v_path)
            augmented_paths.append(flip_v_path)
        
        return augmented_paths
    
    def get_wiki_description(self, category_name: str) -> str:
        """
        ä»Wikipediaè·å–ç±»åˆ«æè¿°
        
        Args:
            category_name: ç±»åˆ«åç§°
            
        Returns:
            str: Wikipediaæè¿°
        """
        try:
            # æ„å»ºWikipedia URL
            url = f"https://en.wikipedia.org/wiki/{category_name.replace('_', ' ').replace('-', ' ')}"
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–ç¬¬ä¸€æ®µæè¿°
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 100 and category_name.lower() in text.lower():
                    return text
            
            return f"Information about {category_name} from Wikipedia."
        except:
            return f"Description for {category_name} category."
    
    def generate_category_description(self, mllm_bot: MLLMBot, category_name: str, 
                                   sample_images: List[str] = None) -> str:
        """
        ä½¿ç”¨MLLMç”Ÿæˆç±»åˆ«æè¿°
        
        Args:
            mllm_bot: MLLMæ¨¡å‹
            category_name: ç±»åˆ«åç§°
            sample_images: æ ·æœ¬å›¾åƒè·¯å¾„åˆ—è¡¨
            
        Returns:
            str: ç”Ÿæˆçš„ç±»åˆ«æè¿°
        """
        # é¦–å…ˆå°è¯•ä»Wikipediaè·å–
        wiki_desc = self.get_wiki_description(category_name)
        print(f'wiki desc: {wiki_desc}')
        if sample_images and len(sample_images) > 0:
            # ä½¿ç”¨æ ·æœ¬å›¾åƒç”Ÿæˆæ›´å‡†ç¡®çš„æè¿°
            prompt = f"""Based on the provided images of {category_name}, generate a comprehensive and discriminative description that captures the key visual characteristics that distinguish this category from other similar categories. 
            
            Focus on:
            1. Distinctive physical features
            2. Color patterns and markings
            3. Size and proportions
            4. Behavioral characteristics (if applicable)
            5. Unique identifying traits
            
            The description should be concise but informative, suitable for fine-grained visual recognition.
            """
            
            # åŠ è½½å›¾åƒ
            images = [Image.open(img_path).convert("RGB") for img_path in sample_images[:3]]  # æœ€å¤šä½¿ç”¨3å¼ å›¾åƒ
            
            try:
                reply, description = mllm_bot.describe_attribute(images, prompt)
                if isinstance(description, list):
                    description = " ".join(description)
                print(f'MLLM description: {description}')
                
                # æ¸…ç†å›¾åƒä»¥é‡Šæ”¾å†…å­˜
                del images
                import gc
                gc.collect()
                
                return description
            except Exception as e:
                print(f"MLLMæè¿°ç”Ÿæˆå¤±è´¥: {e}")
                # æ¸…ç†å›¾åƒ
                del images
                import gc
                gc.collect()
                return wiki_desc
        else:
            return wiki_desc
    
    def build_image_knowledge_base(self, train_samples: Dict[str, List[str]], 
                                 augmentation: bool = True) -> Dict[str, np.ndarray]:
        """
        æ„å»ºå›¾åƒçŸ¥è¯†åº“
        
        Args:
            train_samples: {category: [image_paths]} è®­ç»ƒæ ·æœ¬
            augmentation: æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
            
        Returns:
            Dict[str, np.ndarray]: {category: image_features}
        """
        print("æ„å»ºå›¾åƒçŸ¥è¯†åº“...")
        image_kb = {}
        
        for category, image_paths in tqdm(train_samples.items()):
            print(f"å¤„ç†ç±»åˆ«: {category}")
            category_features = []
            
            for img_path in image_paths:
                # æå–åŸå§‹å›¾åƒç‰¹å¾
                try:
                    feat = self.retrieval.extract_image_feat(img_path)
                    category_features.append(feat)
                except Exception as e:
                    print(f"æå–å›¾åƒç‰¹å¾å¤±è´¥ {img_path}: {e}")
                    continue
                
                # æ•°æ®å¢å¼º
                # if augmentation:
                #     try:
                #         augmented_paths = self.augment_image(img_path, "all")
                #         for aug_path in augmented_paths:
                #             try:
                #                 aug_feat = self.retrieval.extract_image_feat(aug_path)
                #                 category_features.append(aug_feat)
                #             except Exception as e:
                #                 print(f"æå–å¢å¼ºå›¾åƒç‰¹å¾å¤±è´¥ {aug_path}: {e}")
                #                 continue
                #             # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                #             if os.path.exists(aug_path):
                #                 os.remove(aug_path)
                #     except Exception as e:
                #         print(f"å›¾åƒå¢å¼ºå¤±è´¥ {img_path}: {e}")
                #         continue
            
            if category_features:
                # è®¡ç®—ç±»åˆ«å¹³å‡ç‰¹å¾
                category_features = np.array(category_features)
                avg_feature = np.mean(category_features, axis=0)
                image_kb[category] = avg_feature
                print(f"ç±»åˆ« {category} å›¾åƒç‰¹å¾ç»´åº¦: {avg_feature.shape}")
            else:
                print(f"è­¦å‘Š: ç±»åˆ« {category} æ²¡æœ‰æœ‰æ•ˆçš„å›¾åƒç‰¹å¾")
        
        self.image_knowledge_base = image_kb
        return image_kb
    
    def build_text_knowledge_base(self, mllm_bot: MLLMBot, train_samples: Dict[str, List[str]]) -> Dict[str, np.ndarray]:
        """
        æ„å»ºæ–‡æœ¬çŸ¥è¯†åº“
        
        Args:
            mllm_bot: MLLMæ¨¡å‹
            train_samples: {category: [image_paths]} è®­ç»ƒæ ·æœ¬
            
        Returns:
            Dict[str, np.ndarray]: {category: text_features}
        """
        print("æ„å»ºæ–‡æœ¬çŸ¥è¯†åº“...")
        text_kb = {}
        
        for category, image_paths in tqdm(train_samples.items()):
            print(f"å¤„ç†ç±»åˆ«: {category}")
            
            # ç”Ÿæˆç±»åˆ«æè¿°
            description = self.generate_category_description(mllm_bot, category, image_paths)
            self.category_descriptions[category] = description
            
            # æå–æ–‡æœ¬ç‰¹å¾
            try:
                text_feat = self.retrieval.extract_text_feat(description)
                text_kb[category] = text_feat
                print(f"ç±»åˆ« {category} æ–‡æœ¬ç‰¹å¾ç»´åº¦: {text_feat.shape}")
                print(f"æè¿°: {description[:100]}...")
            except Exception as e:
                print(f"æå–æ–‡æœ¬ç‰¹å¾å¤±è´¥ {category}: {e}")
                continue
        
        self.text_knowledge_base = text_kb
        return text_kb
    
    def build_knowledge_base(self, mllm_bot: MLLMBot, train_samples: Dict[str, List[str]], 
                           augmentation: bool = True) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
        """
        æ„å»ºå®Œæ•´çŸ¥è¯†åº“
        
        Args:
            mllm_bot: MLLMæ¨¡å‹
            train_samples: è®­ç»ƒæ ·æœ¬
            augmentation: æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
            
        Returns:
            Tuple: (image_kb, text_kb)
        """
        print("å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
        
        # æ„å»ºå›¾åƒçŸ¥è¯†åº“
        image_kb = self.build_image_knowledge_base(train_samples, augmentation)
        
        # æ„å»ºæ–‡æœ¬çŸ¥è¯†åº“
        text_kb = self.build_text_knowledge_base(mllm_bot, train_samples)
        
        print("çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
        print(f"å›¾åƒçŸ¥è¯†åº“åŒ…å« {len(image_kb)} ä¸ªç±»åˆ«")
        print(f"æ–‡æœ¬çŸ¥è¯†åº“åŒ…å« {len(text_kb)} ä¸ªç±»åˆ«")
        
        return image_kb, text_kb
    
    def initialize_lcb_stats_with_labels(self, train_samples: Dict[str, List[str]], 
                                       fast_thinking_module, top_k: int = 5) -> Dict:
        """
        ä½¿ç”¨æœ‰æ ‡ç­¾çš„è®­ç»ƒé›†åˆå§‹åŒ–LCBç»Ÿè®¡å‚æ•°
        
        Args:
            train_samples: {category: [image_paths]} è®­ç»ƒæ ·æœ¬
            fast_thinking_module
            top_k: æ£€ç´¢top-kç»“æœ
            
        Returns:
            Dict: åˆå§‹åŒ–ç»Ÿè®¡ç»“æœ
        """
        stats_summary = {
            "total_samples": 0,
            "correct_predictions": 0,
            "category_stats": {},
            "initialization_accuracy": 0.0
        }
        
        # é‡ç½®ç»Ÿè®¡é‡
        fast_thinking_module.category_stats = defaultdict(lambda: {"n": 0, "m": 0})
        fast_thinking_module.total_predictions = 0
        
        correct_count = 0
        total_count = 0
        
        print(f'test train_samples:{train_samples}')
        for category, image_paths in train_samples.items():
            # print(f"å¤„ç†ç±»åˆ«: {category} ({len(image_paths)} å¼ å›¾åƒ)")
            category_correct = 0
            category_total = 0
            # print(f'image_paths:{image_paths}')
            for img_path in image_paths:
                # æ‰§è¡Œå¿«æ€è€ƒæµç¨‹
                # print(f'path test: {img_path}')
                result = fast_thinking_module.fast_thinking_pipeline(img_path, top_k)
                
                predicted_category = result.get("fused_top1", "unknown")
                
                # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
                from utils.util import is_similar
                is_correct = is_similar(predicted_category, category, threshold=0.5)
                
                # æ›´æ–°ç»Ÿè®¡é‡
                fast_thinking_module.update_stats(category, is_correct)
                
                if is_correct:
                    correct_count += 1
                    category_correct += 1
                
                total_count += 1
                category_total += 1
            
            # è®°å½•ç±»åˆ«ç»Ÿè®¡
            stats_summary["category_stats"][category] = {
                "total_samples": category_total,
                "correct_predictions": category_correct,
                "accuracy": category_correct / category_total if category_total > 0 else 0.0
            }
            
            print(f"ç±»åˆ« {category}: {category_correct}/{category_total} æ­£ç¡®")
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        stats_summary["total_samples"] = total_count
        stats_summary["correct_predictions"] = correct_count
        stats_summary["initialization_accuracy"] = correct_count / total_count if total_count > 0 else 0.0
        
        # ä¿å­˜ç»Ÿè®¡é‡
        fast_thinking_module.save_stats()
        
        print(f"LCBç»Ÿè®¡å‚æ•°åˆå§‹åŒ–å®Œæˆ!")
        print(f"æ€»æ ·æœ¬æ•°: {total_count}")
        print(f"æ­£ç¡®é¢„æµ‹: {correct_count}")
        print(f"åˆå§‹åŒ–å‡†ç¡®ç‡: {stats_summary['initialization_accuracy']:.4f}")
        print(f"æ€»é¢„æµ‹æ¬¡æ•°: {fast_thinking_module.total_predictions}")
        
        return stats_summary
    
    def save_knowledge_base(self, save_dir: str):
        """
        ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜å›¾åƒçŸ¥è¯†åº“
        image_kb_path = os.path.join(save_dir, "image_knowledge_base.json")
        image_kb_to_save = {cat: feat.tolist() for cat, feat in self.image_knowledge_base.items()}
        dump_json(image_kb_path, image_kb_to_save)
        
        # ä¿å­˜æ–‡æœ¬çŸ¥è¯†åº“
        text_kb_path = os.path.join(save_dir, "text_knowledge_base.json")
        text_kb_to_save = {cat: feat.tolist() for cat, feat in self.text_knowledge_base.items()}
        dump_json(text_kb_path, text_kb_to_save)
        
        # ä¿å­˜ç±»åˆ«æè¿°
        desc_path = os.path.join(save_dir, "category_descriptions.json")
        dump_json(desc_path, self.category_descriptions)
        
        print(f"çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {save_dir}")
    
    def update_knowledge_base(self, save_dir: str):
        """
        ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜å›¾åƒçŸ¥è¯†åº“
        image_kb_path = os.path.join(save_dir, "image_knowledge_base.json")
        image_kb_to_save = {cat: feat.tolist() for cat, feat in self.image_knowledge_base.items()}
        dump_json_override(image_kb_path, image_kb_to_save)
        
        # ä¿å­˜æ–‡æœ¬çŸ¥è¯†åº“
        text_kb_path = os.path.join(save_dir, "text_knowledge_base.json")
        text_kb_to_save = {cat: feat.tolist() for cat, feat in self.text_knowledge_base.items()}
        dump_json_override(text_kb_path, text_kb_to_save)
        
        # ä¿å­˜ç±»åˆ«æè¿°
        desc_path = os.path.join(save_dir, "category_descriptions.json")
        dump_json_override(desc_path, self.category_descriptions)

        print(f"çŸ¥è¯†åº“å·²æ›´æ–°: {save_dir}")
    
    def load_knowledge_base(self, load_dir: str):
        """
        ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“
        
        Args:
            load_dir: åŠ è½½ç›®å½•
        """
        # åŠ è½½å›¾åƒçŸ¥è¯†åº“
        image_kb_path = os.path.join(load_dir, "image_knowledge_base.json")
        if os.path.exists(image_kb_path):
            image_kb_data = load_json(image_kb_path)
            # å¤„ç†ä¸¤ç§æ ¼å¼ï¼šå­—å…¸æ ¼å¼æˆ–åˆ—è¡¨æ ¼å¼ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
            if isinstance(image_kb_data, dict):
                self.image_knowledge_base = {cat: np.array(feat) for cat, feat in image_kb_data.items()}
            elif isinstance(image_kb_data, list) and len(image_kb_data) > 0:
                # æ—§æ ¼å¼ï¼šåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å­—å…¸
                if isinstance(image_kb_data[0], dict):
                    self.image_knowledge_base = {cat: np.array(feat) for cat, feat in image_kb_data[0].items()}
                else:
                    print(f"è­¦å‘Š: å›¾åƒçŸ¥è¯†åº“æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡åŠ è½½")
                    self.image_knowledge_base = {}
            else:
                print(f"è­¦å‘Š: å›¾åƒçŸ¥è¯†åº“ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
                self.image_knowledge_base = {}
        
        # åŠ è½½æ–‡æœ¬çŸ¥è¯†åº“
        text_kb_path = os.path.join(load_dir, "text_knowledge_base.json")
        if os.path.exists(text_kb_path):
            text_kb_data = load_json(text_kb_path)
            # å¤„ç†ä¸¤ç§æ ¼å¼ï¼šå­—å…¸æ ¼å¼æˆ–åˆ—è¡¨æ ¼å¼ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
            if isinstance(text_kb_data, dict):
                self.text_knowledge_base = {cat: np.array(feat) for cat, feat in text_kb_data.items()}
            elif isinstance(text_kb_data, list) and len(text_kb_data) > 0:
                # æ—§æ ¼å¼ï¼šåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å­—å…¸
                if isinstance(text_kb_data[0], dict):
                    self.text_knowledge_base = {cat: np.array(feat) for cat, feat in text_kb_data[0].items()}
                else:
                    print(f"è­¦å‘Š: æ–‡æœ¬çŸ¥è¯†åº“æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡åŠ è½½")
                    self.text_knowledge_base = {}
            else:
                print(f"è­¦å‘Š: æ–‡æœ¬çŸ¥è¯†åº“ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
                self.text_knowledge_base = {}
        
        # åŠ è½½ç±»åˆ«æè¿°
        desc_path = os.path.join(load_dir, "category_descriptions.json")
        if os.path.exists(desc_path):
            desc_data = load_json(desc_path)
            # å¤„ç†ä¸¤ç§æ ¼å¼ï¼šå­—å…¸æ ¼å¼æˆ–åˆ—è¡¨æ ¼å¼
            if isinstance(desc_data, dict):
                self.category_descriptions = desc_data
            elif isinstance(desc_data, list) and len(desc_data) > 0:
                # åˆ—è¡¨æ ¼å¼ï¼šå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆåº”è¯¥æ˜¯å­—å…¸ï¼‰
                if isinstance(desc_data[0], dict):
                    self.category_descriptions = desc_data[0]
                else:
                    print(f"è­¦å‘Š: ç±»åˆ«æè¿°æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡åŠ è½½")
                    self.category_descriptions = {}
            else:
                print(f"è­¦å‘Š: ç±»åˆ«æè¿°ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
                self.category_descriptions = {}
        
        # åŠ è½½self-belief
        belief_path = os.path.join(load_dir, "self_belief.txt")
        if os.path.exists(belief_path):
            with open(belief_path, 'r', encoding='utf-8') as f:
                self.self_belief = f.read()
            print(f"Self-Beliefå·²ä» {belief_path} åŠ è½½")
        print(f"çŸ¥è¯†åº“å·²ä» {load_dir} åŠ è½½")
    
    def image_retrieval(self, query_image_path: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        å›¾åƒæ£€ç´¢
        
        Args:
            query_image_path: æŸ¥è¯¢å›¾åƒè·¯å¾„
            top_k: è¿”å›top-kç»“æœ
            
        Returns:
            List[Tuple[str, float]]: [(category, similarity_score), ...]
        """
        if not self.image_knowledge_base:
            raise ValueError("å›¾åƒçŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†åº“")
        
        # æå–æŸ¥è¯¢å›¾åƒç‰¹å¾
        query_feat = self.retrieval.extract_image_feat(query_image_path)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for category, feat in self.image_knowledge_base.items():
            sim = np.dot(query_feat, feat)  # ä½™å¼¦ç›¸ä¼¼åº¦
            similarities.append((category, sim))
        
        # æ’åºå¹¶è¿”å›top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def text_retrieval(self, query_text: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ–‡æœ¬æ£€ç´¢
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›top-kç»“æœ
            
        Returns:
            List[Tuple[str, float]]: [(category, similarity_score), ...]
        """
        if not self.text_knowledge_base:
            raise ValueError("æ–‡æœ¬çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†åº“")
        
        # æå–æŸ¥è¯¢æ–‡æœ¬ç‰¹å¾
        query_feat = self.retrieval.extract_text_feat(query_text)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for category, feat in self.text_knowledge_base.items():
            sim = np.dot(query_feat, feat)  # ä½™å¼¦ç›¸ä¼¼åº¦
            similarities.append((category, sim))
        
        # æ’åºå¹¶è¿”å›top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def incremental_update(self, category: str, image_path: str, structured_description: str,key_regions: List[Dict], confidence: float, mllm_bot, save_dir: str,
                         signals: Dict = None) -> bool:
        """
        å¢é‡æ›´æ–°çŸ¥è¯†åº“ï¼šä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ + å¹³æ»‘æ›´æ–°æœºåˆ¶
        
        Args:
            category: é¢„æµ‹ç±»åˆ«
            image_path: å›¾åƒè·¯å¾„
            structured_description: ç»“æ„åŒ–æè¿°
            key_regions: å…³é”®åŒºåŸŸä¿¡æ¯
            confidence: é¢„æµ‹ç½®ä¿¡åº¦
            mllm_bot: MLLMæ¨¡å‹
            save_dir: ä¿å­˜ç›®å½•
            signals: è¾…åŠ©ä¿¡å·ï¼ˆLCBã€ä¸€è‡´æ€§ç­‰ï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        signals = signals or {}
        
        # === ä¸¥æ ¼çš„è´¨é‡åˆ¤æ–­æœºåˆ¶ ===
        quality_score = self._evaluate_update_quality(category, confidence, signals)
        
        if quality_score < 0.7:  # è´¨é‡é˜ˆå€¼
            print(f"è·³è¿‡æ›´æ–°ï¼šè´¨é‡åˆ†æ•°è¿‡ä½ {quality_score:.3f}")
            return False
        
        print(f"è´¨é‡åˆ†æ•°: {quality_score:.3f}ï¼Œå¼€å§‹å¢é‡æ›´æ–°")
        # === å›¾åƒçŸ¥è¯†åº“æ›´æ–° ===
        self._update_image_knowledge_base(category, image_path, quality_score)
        
        # === æ–‡æœ¬çŸ¥è¯†åº“æ›´æ–° ===
        # self._update_text_knowledge_base(category, structured_description, mllm_bot, quality_score)
        
        # === ä¿å­˜æ›´æ–°åçš„çŸ¥è¯†åº“ ===
        self.update_knowledge_base(save_dir)
        
        print(f"çŸ¥è¯†åº“å¢é‡æ›´æ–°å®Œæˆ: {category}")
        return True

    
    def _evaluate_update_quality(self, category: str, confidence: float, signals: Dict) -> float:
        """
        è¯„ä¼°æ›´æ–°è´¨é‡ï¼šç»¼åˆå¤šä¸ªæŒ‡æ ‡
        
        Args:
            category: é¢„æµ‹ç±»åˆ«
            confidence: é¢„æµ‹ç½®ä¿¡åº¦
            signals: è¾…åŠ©ä¿¡å·
            
        Returns:
            float: è´¨é‡åˆ†æ•° [0, 1]
        """
        quality_factors = []
        
        # 1. ç½®ä¿¡åº¦å› å­ (æƒé‡: 0.3)
        conf_factor = min(1.0, confidence / 0.8)  # 0.8ä¸ºé«˜ç½®ä¿¡åº¦åŸºå‡†
        quality_factors.append(("confidence", conf_factor, 0.3))
        
        # 2. LCBå› å­ (æƒé‡: 0.25)
        lcb_value = signals.get('lcb', 0.5)
        lcb_factor = min(1.0, lcb_value / 0.7)  # 0.7ä¸ºLCBé«˜å€¼åŸºå‡†
        quality_factors.append(("lcb", lcb_factor, 0.25))
        
        # 3. å¿«æ…¢æ€è€ƒä¸€è‡´æ€§å› å­ (æƒé‡: 0.2)
        fast_slow_consistent = signals.get('fast_slow_consistent', False)
        consistency_factor = 1.0 if fast_slow_consistent else 0.3
        quality_factors.append(("consistency", consistency_factor, 0.2))
        
        # 4. èåˆTop-1æ¦‚ç‡å› å­ (æƒé‡: 0.15)
        fused_prob = signals.get('fused_top1_prob', 0.5)
        fused_factor = min(1.0, fused_prob / 0.7)
        quality_factors.append(("fused_prob", fused_factor, 0.15))
        
        # 5. æ’åå› å­ (æƒé‡: 0.1)
        rank_fast = signals.get('rank_in_fast_topk', 10)  # é»˜è®¤æ’åé å
        rank_enh = signals.get('rank_in_enhanced_topk', 10)
        rank_factor = max(0.0, 1.0 - (min(rank_fast, rank_enh) / 5.0))  # æ’åè¶Šé å‰è¶Šå¥½
        quality_factors.append(("ranking", rank_factor, 0.1))
        
        # è®¡ç®—åŠ æƒè´¨é‡åˆ†æ•°
        total_score = sum(factor * weight for _, factor, weight in quality_factors)
        
        print(f"è´¨é‡è¯„ä¼°è¯¦æƒ…: {[(name, f'{factor:.3f}', f'{weight:.2f}') for name, factor, weight in quality_factors]}")
        
        return total_score
    
    def _update_image_knowledge_base(self, category: str, image_path: str, quality_score: float):
        """
        å¹³æ»‘æ›´æ–°å›¾åƒçŸ¥è¯†åº“
        
        Args:
            category: ç±»åˆ«
            image_path: å›¾åƒè·¯å¾„
            quality_score: è´¨é‡åˆ†æ•°
        """
        # æå–æ–°å›¾åƒç‰¹å¾
        new_feat = self.retrieval.extract_image_feat(image_path)
        
        if category in self.image_knowledge_base:
            # å¹³æ»‘æ›´æ–°ï¼šæ–°ç‰¹å¾æƒé‡åŸºäºè´¨é‡åˆ†æ•°
            old_feat = self.image_knowledge_base[category]
            
            # åŠ¨æ€æƒé‡ï¼šè´¨é‡è¶Šé«˜ï¼Œæ–°ç‰¹å¾æƒé‡è¶Šå¤§ï¼Œä½†ä¸è¶…è¿‡0.1
            new_weight = min(0.1, quality_score * 0.04)
            old_weight = 1.0 - new_weight
            
            # åŠ æƒå¹³å‡æ›´æ–°
            updated_feat = old_weight * old_feat + new_weight * new_feat
            self.image_knowledge_base[category] = updated_feat
            
            print(f"å›¾åƒçŸ¥è¯†åº“å¹³æ»‘æ›´æ–°: {category}, æ–°æƒé‡={new_weight:.3f}")
        else:
            # æ–°ç±»åˆ«ï¼šç›´æ¥æ·»åŠ 
            # self.image_knowledge_base[category] = new_feat
            # print(f"å›¾åƒçŸ¥è¯†åº“æ–°å¢ç±»åˆ«: {category}")
            print(f'å›¾åƒçŸ¥è¯†åº“è·³è¿‡æ–°å¢ç±»åˆ«: {category}')
    
    def _update_text_knowledge_base(self, category: str, structured_description: str, 
                                  mllm_bot, quality_score: float):
        """
        å¹³æ»‘æ›´æ–°æ–‡æœ¬çŸ¥è¯†åº“
        
        Args:
            category: ç±»åˆ«
            structured_description: ç»“æ„åŒ–æè¿°
            mllm_bot: MLLMæ¨¡å‹
            quality_score: è´¨é‡åˆ†æ•°
        """
        try:
            # ç”Ÿæˆæ–°çš„æ–‡æœ¬æè¿°ï¼ˆåŸºäºç»“æ„åŒ–æè¿°ï¼‰
            if structured_description and len(structured_description.strip()) > 10:
                # ä½¿ç”¨ç»“æ„åŒ–æè¿°ç”Ÿæˆæ›´ç²¾ç¡®çš„ç±»åˆ«æè¿°
                prompt = f"""Based on the detailed structured description of this {category} image, generate a concise but discriminative text description that captures the key visual characteristics for fine-grained recognition.

                Structured description: {structured_description}

                Focus on distinctive features that help distinguish this category from similar ones."""
                
                try:
                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„MLLMæ¥å£è°ƒæ•´
                    new_description = structured_description  # æš‚æ—¶ä½¿ç”¨ç»“æ„åŒ–æè¿°
                except:
                    new_description = f"Updated description for {category}: {structured_description[:100]}..."
            else:
                new_description = f"Updated description for {category} category."
            
            # æå–æ–°æ–‡æœ¬ç‰¹å¾
            new_text_feat = self.retrieval.extract_text_feat(new_description)
            
            if category in self.text_knowledge_base:
                # å¹³æ»‘æ›´æ–°æ–‡æœ¬ç‰¹å¾
                old_text_feat = self.text_knowledge_base[category]
                
                # æ–‡æœ¬ç‰¹å¾æ›´æ–°æƒé‡æ›´ä¿å®ˆ
                new_weight = min(0.2, quality_score * 0.3)
                old_weight = 1.0 - new_weight
                
                updated_text_feat = old_weight * old_text_feat + new_weight * new_text_feat
                self.text_knowledge_base[category] = updated_text_feat
                
                # æ›´æ–°æè¿°
                if category in self.category_descriptions:
                    old_desc = self.category_descriptions[category]
                    self.category_descriptions[category] = f"{old_desc}\n\nUpdated: {new_description}"
                else:
                    self.category_descriptions[category] = new_description
                
                print(f"æ–‡æœ¬çŸ¥è¯†åº“å¹³æ»‘æ›´æ–°: {category}, æ–°æƒé‡={new_weight:.3f}")
            else:
                # æ–°ç±»åˆ«
                self.text_knowledge_base[category] = new_text_feat
                self.category_descriptions[category] = new_description
                print(f"æ–‡æœ¬çŸ¥è¯†åº“æ–°å¢ç±»åˆ«: {category}")
                
        except Exception as e:
            print(f"æ–‡æœ¬çŸ¥è¯†åº“æ›´æ–°å¤±è´¥: {e}")
            raise


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆå§‹åŒ–
    builder = KnowledgeBaseBuilder()
    mllm_bot = MLLMBot(model_tag="Qwen2.5-VL-7B", model_name="Qwen2.5-VL-7B", device="cuda")
    
    # ç¤ºä¾‹è®­ç»ƒæ ·æœ¬
    train_samples = {
        "Chihuahua": ["path/to/chihuahua1.jpg", "path/to/chihuahua2.jpg"],
        "Shiba Inu": ["path/to/shiba1.jpg", "path/to/shiba2.jpg"]
    }
    
    # æ„å»ºçŸ¥è¯†åº“
    image_kb, text_kb = builder.build_knowledge_base(mllm_bot, train_samples)
    
    # ä¿å­˜çŸ¥è¯†åº“
    builder.save_knowledge_base("./knowledge_base")
    
    print("çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
