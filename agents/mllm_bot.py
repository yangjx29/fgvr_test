import sys
import os

# ç¡®ä¿å¯¼å…¥æ­£ç¡®çš„utilsæ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from utils.util import encode_base64, prepare_qwen2_5_input, get_important_image_tokens, create_attention_mask

import torch
from os import path
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from agents.CFG import CFGLogits 
from agents.attention import qwen_modify, qwen_modify_with_importance
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from skimage.measure import block_reduce

QWEN = {
    'Qwen2.5-VL-7B': 'Qwen/Qwen2.5-VL-7B-Instruct'
}

ANSWER_INSTRUCTION = 'Answer given questions. If you are not sure about the answer, say you don\'t ' \
                     'know honestly. Don\'t imagine any contents that are not in the image.'

SUB_ANSWER_INSTRUCTION = 'Answer: '  # template following qwen2_5 huggingface demo


def get_chat_log(questions, answers, last_n=-1):
    n_addition_q = len(questions) - len(answers)
    assert (n_addition_q) in [0, 1]
    template = 'Question: {} \nAnswer: {} \n'
    chat_log = ''
    if last_n > 0:
        answers = answers[-last_n:]
        questions = questions[-(last_n + n_addition_q):]
    elif last_n == 0:
        answers = []
        questions = questions[-1:] if n_addition_q else []

    for i in range(len(answers)):
        chat_log = chat_log + template.format(questions[i], answers[i])
    if n_addition_q:
        chat_log = chat_log + 'Question: {}'.format(questions[-1])
    else:
        chat_log = chat_log[:-2]
    return chat_log


def trim_answer(answer):
    if isinstance(answer, list):
        return answer
    answer = answer.split('Question:')[0].replace('\n', ' ').strip()
    return answer


class MLLMBot:
    def __init__(self, model_tag, model_name, pai_enable_attn=False, device='cpu', device_id=0, bit8=False, max_answer_tokens=-1):
        self.model_tag = model_tag
        self.model_name = model_name
        self.max_answer_tokens = max_answer_tokens

        local_model_path_abs = "./models/Qwen"
        local_model_path = path.join(local_model_path_abs, QWEN[self.model_tag].split('/')[-1])

        # åŠ è½½å¤„ç†å™¨
        self.qwen2_5_processor = AutoProcessor.from_pretrained(local_model_path)

        print("\n================= æ¨¡å‹åˆå§‹åŒ–ï¼ˆMLLMBotï¼‰ =================")
        print(f"ğŸ“Œ æ¨¡å‹æ ‡è¯†ï¼ˆmodel_tagï¼‰: {model_tag}")
        print(f"ğŸ“Œ æ¨¡å‹åç§°ï¼ˆmodel_nameï¼‰: {model_name}")
        print(f"ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")

        # ========== CPU ==========
        if device == 'cpu':
            self.device = 'cpu'
            self.qwen2_5 = Qwen2_5_VLForConditionalGeneration.from_pretrained(local_model_path)
            dtype_used = "float32ï¼ˆCPU é»˜è®¤ï¼‰"
            print(f"ğŸ–¥ï¸ è®¾å¤‡: CPU")

        # ========== GPU ==========
        else:
            self.device = f'cuda:{device_id}'
            self.bit8 = bit8

            print(f"ğŸ–¥ï¸ è®¾å¤‡: GPU - {self.device}")
            print(f"ğŸ¤– ä½¿ç”¨ 8bit æ¨ç†: {'æ˜¯' if self.bit8 else 'å¦'}")

            # æŒ‰ä½ çš„åŸå§‹é€»è¾‘ï¼š8bit æˆ– float32
            if self.bit8:
                dtype_config = {"load_in_8bit": True}
                dtype_used = "int8ï¼ˆ8bit é‡åŒ–æ¨ç†ï¼‰"
            else:
                dtype_config = {"torch_dtype": torch.float32}
                dtype_used = "float32ï¼ˆFP32ï¼‰"

            print(f"ğŸ” ä½¿ç”¨æ•°æ®ç±»å‹: {dtype_used}")

            self.qwen2_5 = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                local_model_path,
                device_map="auto",
                **dtype_config
            ).eval()

            # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
            if hasattr(self.qwen2_5, 'gradient_checkpointing_enable'):
                self.qwen2_5.gradient_checkpointing_enable()
                print("âœ“ å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜")

        print(f"ğŸ“ local_model_path: {local_model_path}")
        print(f"ğŸ”¢ å½“å‰ä½¿ç”¨çš„ç²¾åº¦ dtype: {dtype_used}")
        print(f"ğŸ”§ æœ€å¤§ç”Ÿæˆé•¿åº¦ max_answer_tokens: {self.max_answer_tokens}")
        print("ğŸš€ æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print("========================================================\n")
        
        # TODOè¶…å‚æ•°
        self.pai_enable_attn = pai_enable_attn   # é˜¶æ®µä¸€ï¼šæ˜¯å¦å¢å¼ºå›¾åƒæ³¨æ„åŠ›
        self.pai_alpha = 0.5           # é˜¶æ®µä¸€ï¼šå¢å¼ºç³»æ•° Î±
        self.pai_layers = (10, 28)     # é˜¶æ®µä¸€ï¼šå±‚å…ˆéªŒï¼ˆæ·±å±‚æ›´æœ‰æ•ˆï¼‰
        self.pai_enable_cfg = False    # é˜¶æ®µäºŒï¼šæ˜¯å¦å¼€å¯CFG logitsç²¾ç‚¼
        self.pai_gamma = 1.1           # é˜¶æ®µäºŒï¼šÎ³ æŒ‡å¯¼å¼ºåº¦
        self.num_map = 0
        
    def __del__(self):
        """ææ„å‡½æ•°ï¼šæ¸…ç†GPUå†…å­˜"""
        try:
            if hasattr(self, 'qwen2_5'):
                del self.qwen2_5
            if hasattr(self, 'qwen2_5_processor'):
                del self.qwen2_5_processor
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception as e:
            print(f"æ¸…ç†MLLMBotå†…å­˜æ—¶å‡ºé”™: {e}")
    
    def cleanup(self):
        """æ‰‹åŠ¨æ¸…ç†å†…å­˜"""
        try:
            if hasattr(self, 'qwen2_5'):
                del self.qwen2_5
            if hasattr(self, 'qwen2_5_processor'):
                del self.qwen2_5_processor
            torch.cuda.empty_cache()
            print("MLLMBotå†…å­˜å·²æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†MLLMBotå†…å­˜æ—¶å‡ºé”™: {e}")
        
    def _get_model_device(self):
        try:
            return self.qwen2_5.model.embed_tokens.weight.device
        except Exception:
            # é€€åŒ–æ–¹æ¡ˆï¼šå–ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨è®¾å¤‡æˆ– self.device
            try:
                return next(self.qwen2_5.parameters()).device
            except Exception:
                return torch.device(self.device)

    # # TODO è¿™é‡Œåº”è¯¥éœ€è¦è€ƒè™‘chunkåˆ‡åˆ†
    # def _resolve_img_token_span(self, messages, inputs):
    #     """è¿”å›(img_start_idx, img_end_idx)ã€‚
    #     å¯å‘å¼ï¼šç¼ºå°‘æ˜¾å¼ image special token æ—¶ï¼Œè¿‘ä¼¼æŠŠæœ«å°¾ 256 ä¸ª token å½“ä½œå›¾åƒåŒºåŸŸã€‚
    #     è‹¥åºåˆ—è¿‡çŸ­æˆ–æ— æ³•è§£æï¼Œåˆ™è¿”å› (None, None) è·³è¿‡æ³¨å…¥ã€‚
    #     """
    #     try:
    #         input_ids = inputs.input_ids
    #         if input_ids is None:
    #             print(f'input_ids is None')
    #             return None, None
    #         seq_len = input_ids.shape[1]
    #         img_tokens = 256
    #         print(f'input_ids:{input_ids.shape}\nseq_len:{seq_len}')
    #         if seq_len <= img_tokens:
    #             print(f'seq_len <= img_tokens')
    #             return None, None
    #         img_start = seq_len - img_tokens
    #         img_end = seq_len
    #         print(f'img_start:{img_start}, img_end:{img_end}')
    #         return img_start, img_end
    #     except Exception as e:
    #         print(f"error return None None:{e}")
    #         return None, None


    def _resolve_img_token_span(self, messages, inputs):
        try:
            input_ids = inputs.input_ids
            if input_ids is None:
                print(f'input_ids is None')
                return None, None
            seq_len = input_ids.shape[1]
            # tokenizer é‡Œæœ‰ special token çš„æ˜ å°„
            tokenizer = self.qwen2_5_processor.tokenizer
            # img_start_token_id = tokenizer.convert_tokens_to_ids("<img_start>")
            # img_end_token_id   = tokenizer.convert_tokens_to_ids("<img_end>")
            vision_start_id = tokenizer.convert_tokens_to_ids('<|vision_start|>')
            image_pad_id = tokenizer.convert_tokens_to_ids('<|image_pad|>')
            vision_end_id = tokenizer.convert_tokens_to_ids('<|vision_end|>')
            print(f'input_ids:{input_ids.shape}\nseq_len:{seq_len}')
            input_ids_list = input_ids[0].tolist()
            if vision_start_id in input_ids_list and vision_end_id in input_ids_list:
                img_start = input_ids_list.index(vision_start_id)
                img_end   = input_ids_list.index(vision_end_id) + 1  # åŒ…å« img_end
                print(f"æ‰¾åˆ° image token span: img_start={img_start}, img_end={img_end}")
                return img_start, img_end
            else:
                print("æœªæ‰¾åˆ° image token span")
                return None, None
        except Exception as e:
            print(f"error return None None:{e}")
            return None, None

    def _inject_qwen_pai_attention(self, img_start_idx, img_end_idx):
        if img_start_idx is None or img_end_idx is None:
            print('[ATTN] skip injection for Qwen (img span unresolved).')
            return
        print(f'[ATTN] inject Qwen attention layers {self.pai_layers} alpha={self.pai_alpha} span=({img_start_idx},{img_end_idx})')
        qwen_modify(self.qwen2_5, self.pai_layers[0], self.pai_layers[1], True, self.pai_alpha, False, img_start_idx, img_end_idx)

    def _inject_qwen_pai_attention_with_importance(self, img_start_idx, img_end_idx, important_tokens_info):
        if img_start_idx is None or img_end_idx is None:
            print('[ATTN] skip injection for Qwen (img span unresolved).')
            return
        
        print(f'[ATTN] inject Qwen attention layers with importance weights {self.pai_layers} alpha={self.pai_alpha} span=({img_start_idx},{img_end_idx})')
        
        # æå–é‡è¦æ€§æƒé‡ä¿¡æ¯
        importance_weights = important_tokens_info['weights']  # æ‰€æœ‰å›¾åƒtokençš„æƒé‡
        important_indices = important_tokens_info['important_indices']  # é‡è¦tokençš„ç´¢å¼•
        
        # è°ƒç”¨ä¿®æ”¹å‡½æ•°ï¼Œä¼ é€’é‡è¦æ€§ä¿¡æ¯
        qwen_modify_with_importance(self.qwen2_5, self.pai_layers[0], self.pai_layers[1], True, self.pai_alpha, False, img_start_idx, img_end_idx, importance_weights, important_indices)

    def get_name(self):
        return self.model_name
    
    def _resize_image_if_needed(self, image: Image.Image, max_size: int = 1750) -> Image.Image:
        """
        å¦‚æœå›¾åƒå°ºå¯¸è¶…è¿‡max_sizeï¼ŒæŒ‰æ¯”ä¾‹ç¼©å°ä»¥é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸
        
        Args:
            image: PILå›¾åƒ
            max_size: æœ€å¤§è¾¹é•¿ï¼ˆé»˜è®¤1536ï¼Œè¶³å¤Ÿä¿ç•™ç»†èŠ‚ï¼‰
            
        Returns:
            è°ƒæ•´åçš„PILå›¾åƒ
        """
        width, height = image.size
        max_dim = max(width, height)
        
        if max_dim > max_size:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale = max_size / max_dim
            new_width = int(width * scale)
            new_height = int(height * scale)
            
            print(f"å›¾åƒè¿‡å¤§ ({width}x{height})ï¼Œç¼©å°åˆ° ({new_width}x{new_height}) ä»¥èŠ‚çœæ˜¾å­˜")
            return image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return image

    def __call_qwen2_5(self, raw_image, prompt, max_new_tokens=256):
        # print(f"MLLMBot prompt: {prompt}")

        if isinstance(raw_image, Image.Image):
            raw_image = [raw_image]

        content = []
        # å…ˆæ·»åŠ æ‰€æœ‰å›¾åƒ
        # for img in raw_image:
        #     content.append({"type": "image", "image": img})
        # content.append({"type": "text", "text": prompt})
        for img in raw_image:
            # é™åˆ¶å›¾åƒæœ€å¤§å°ºå¯¸ï¼Œé˜²æ­¢è¶…å¤§å›¾ç‰‡å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸
            img = self._resize_image_if_needed(img, max_size=1536)
            image_str = encode_base64(img)
            content.append({"type": "image", "image": f'data:image;base64,{image_str}'})
        content.append({"type": "text", "text": prompt})
        # 1. æ„é€  messages
        messages = [
            {
                "role": "user",
                "content": content
            }
        ]
        # # 2. æ„é€ è¾“å…¥æ–‡æœ¬
        # text = self.qwen2_5_processor.apply_chat_template(
        #     messages, tokenize=False, add_generation_prompt=True
        # )
        # # 3. æå–è§†è§‰è¾“å…¥
        # image_inputs, video_inputs = process_vision_info(messages)

        # if self.device == 'cpu':
        #     inputs = self.qwen2_5_processor(
        #         text=[text],    
        #         images=image_inputs,  
        #         videos=video_inputs,           
        #         padding=True,
        #         return_tensors="pt"
        #     )
        # else:
        #     inputs = self.qwen2_5_processor(
        #         text=[text],    
        #         images=image_inputs,  
        #         videos=video_inputs,           
        #         padding=True,
        #         return_tensors="pt"
        #     ).to(self.device,torch.float32)
        # generated_ids = self.qwen2_5.generate(**inputs, max_new_tokens=256)
        model_device = self._get_model_device()
        inputs = prepare_qwen2_5_input(messages, self.qwen2_5_processor).to(model_device, torch.float32)

        # TODOé˜¶æ®µä¸€ æ³¨æ„åŠ›å¢å¼º
        if self.pai_enable_attn:
            # 1.è®¡ç®—rel-attentionè®¡ç®—æ³¨æ„åŠ›çŸ©é˜µ
            from agents.qwen2_5_methods import rel_attention_qwen2_5
            general_question = 'Write a general description of the image.'
            # ç¡®ä¿ä¸¤ä¸ªpromptæ ¼å¼ä¸€è‡´
            formatted_prompt = f"<image>\nUSER: {prompt} Answer the question with a concise phrase.\nASSISTANT:"
            general_prompt = f"<image>\nUSER: {general_question} Answer the question with a concise phrase.\nASSISTANT:"
            att_map = rel_attention_qwen2_5(raw_image[0], formatted_prompt, general_prompt, self.qwen2_5, self.qwen2_5_processor)
            import matplotlib.pyplot as plt
            path='./maps/'
            os.makedirs(path, exist_ok=True)
            plt.imshow(att_map, interpolation='none')
            plt.axis('off')
            plt.savefig(
                fname=f"{path}/attention_map_{self.num_map}.png",  # ä¿å­˜ä¸º PNG æ ¼å¼ï¼ˆæ¨èï¼Œæ— æŸï¼‰
                dpi=300,                    
                bbox_inches='tight',        # è‡ªåŠ¨å»é™¤å¤šä½™ç©ºç™½
                pad_inches=0                # æ— é¢å¤–è¾¹è·
            )
            self.num_map +=1

            # 2.å°†att_mapä¸­çš„é‡è¦tokenå’ŒåŸå§‹è¾“å…¥tokenä½ç½®å¯¹åº”
            # å°†æ³¨æ„åŠ›æƒé‡æ˜ å°„åˆ°åŸå§‹è¾“å…¥ä¸­çš„å›¾åƒtoken

            important_tokens_info = get_important_image_tokens(att_map, inputs, self.qwen2_5_processor, threshold=1)
            print(f"å›¾åƒtokenä½ç½®èŒƒå›´: {important_tokens_info['positions']}")
            print(f"é‡è¦stokenæ•°é‡: {len(important_tokens_info['important_indices'])}")
            if len(important_tokens_info['important_indices']) > 0:
                print(f"æœ€é«˜æ³¨æ„åŠ›æƒé‡: {important_tokens_info['important_weights'].max().item():.4f}")
                print(f"å¹³å‡æ³¨æ„åŠ›æƒé‡: {important_tokens_info['important_weights'].mean().item():.4f}")
            # åˆ›å»ºæ³¨æ„åŠ›mask
            # attention_mask = create_attention_mask(att_map, inputs, important_tokens_info, self.qwen2_5_processor, threshold=1)
            img_start_idx, img_end_idx = self._resolve_img_token_span(messages, inputs)
            
            # å°†æ³¨æ„åŠ›é‡è¦æ€§ä¿¡æ¯ä¼ é€’ç»™æ³¨æ„åŠ›å±‚
            # self._inject_qwen_pai_attention(img_start_idx, img_end_idx)
            self._inject_qwen_pai_attention_with_importance(img_start_idx, img_end_idx, important_tokens_info)

        # æ¸…ç†æ˜¾å­˜ç¼“å­˜
        torch.cuda.empty_cache()
        
        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ - åŸºäºå‰©ä½™æ˜¾å­˜çš„æ¸…ç†ç­–ç•¥
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            memory_free = memory_total - memory_allocated
            print(f"æ¨ç†å‰æ˜¾å­˜: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB, å‰©ä½™={memory_free:.2f}GB")
            
            # å¦‚æœå‰©ä½™æ˜¾å­˜ < 12GBï¼Œè§¦å‘æ¸…ç†ï¼ˆé€‚é…A6000 48GBå’ŒA800 80GBï¼‰
            if memory_free < 12:  
                print(f"è­¦å‘Š: å‰©ä½™æ˜¾å­˜ä¸è¶³ ({memory_free:.2f}GB < 12GB)ï¼Œå¼ºåˆ¶æ¸…ç†...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                # å†æ¬¡æ£€æŸ¥
                memory_after_clear = torch.cuda.memory_allocated() / 1024**3
                memory_free_after = memory_total - memory_after_clear
                print(f"æ¸…ç†åå‰©ä½™æ˜¾å­˜: {memory_free_after:.2f}GB")
                if memory_free_after < 10:
                    torch.cuda.reset_peak_memory_stats()
                    print(f"å·²é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡")
        
        with torch.no_grad():
            # ä½¿ç”¨å¹³è¡¡é€Ÿåº¦å’Œæ˜¾å­˜çš„ç”Ÿæˆå‚æ•°
            generated_ids = self.qwen2_5.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,                     # ç¦ç”¨é‡‡æ ·ä»¥æé«˜é€Ÿåº¦
                use_cache=True,                      # å¯ç”¨KVç¼“å­˜åŠ é€Ÿæ¨ç†
                num_beams=1,                         # ç¦ç”¨beam searchèŠ‚çœæ˜¾å­˜
                pad_token_id=self.qwen2_5_processor.tokenizer.eos_token_id,
                # logits_processor=logits_processors
            )
            
        # åœ¨åˆ é™¤inputsä¹‹å‰ï¼Œå…ˆä¿å­˜input_ids
        input_ids = inputs.input_ids
        
        # ä½¿ç”¨ä¿å­˜çš„input_ids
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(input_ids, generated_ids)
        ]
        
        # 8. è§£ç 
        reply = self.qwen2_5_processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        
        # æ¨ç†å®Œæˆåç«‹å³æ¸…ç†ä¸´æ—¶å˜é‡
        del inputs, input_ids, generated_ids, generated_ids_trimmed
        
        # åŸºäºå‰©ä½™æ˜¾å­˜å†³å®šæ˜¯å¦æ¸…ç†
        if torch.cuda.is_available():
            memory_after = torch.cuda.memory_allocated() / 1024**3
            memory_reserved = torch.cuda.memory_reserved() / 1024**3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            memory_free = memory_total - memory_after
            print(f"æ¨ç†åæ˜¾å­˜: å·²åˆ†é…={memory_after:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB, å‰©ä½™={memory_free:.2f}GB")
            
            # åªåœ¨å‰©ä½™æ˜¾å­˜ < 10GB æ—¶æ‰æ¸…ç†ï¼Œå¦åˆ™ä¿ç•™ç¼“å­˜æå‡æ€§èƒ½
            if memory_free < 10:
                torch.cuda.empty_cache()
                print(f"å‰©ä½™æ˜¾å­˜ä¸è¶³10GBï¼Œå·²æ¸…ç†ç¼“å­˜")
                
                # å¦‚æœä¿ç•™çš„æ˜¾å­˜è¿‡å¤šä¸”å‰©ä½™ä¸è¶³10GBï¼Œé‡ç½®å³°å€¼ç»Ÿè®¡
                if memory_reserved > memory_free and memory_free < 10:
                    torch.cuda.reset_peak_memory_stats()
                    print(f"å·²é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡")
        
        # print(f"test MLLM answer after decode: {reply}")
        return reply

    def answer_chat_log(self, raw_image, chat_log, n_qwen2_5_context=-1):
        # prepare the context for qwen2_5
        qwen2_5_prompt = '\n'.join([ANSWER_INSTRUCTION,
                                  get_chat_log(chat_log['questions'],chat_log['answers'],
                                               last_n=n_qwen2_5_context), SUB_ANSWER_INSTRUCTION]
                                 )

        reply = self.__call_qwen2_5(raw_image, qwen2_5_prompt)
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply

    def tell_me_the_obj(self, raw_image, super_class, super_unit):
        std_prompt = f"Questions: What is the {super_unit} of the {super_class} in this photo? Answer:"
        # std_prompt = f"Questions: What is the name of the main object in this photo? Answer:"
        reply = self.__call_qwen2_5(raw_image, std_prompt)
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply

    def describe_attribute(self, raw_image, attr_prompt, max_new_tokens=256):
        # raw_imageæ˜¯Image.openä¹‹åçš„æ ¼å¼   
        reply = self.__call_qwen2_5(raw_image, attr_prompt, max_new_tokens)
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply
    
    def compare_attention_enhancement(self, raw_image, attr_prompt, save_dir="./experiments/attention_comparison"):
        """
        å¯¹æ¯”æ³¨æ„åŠ›å¢å¼ºå‰åçš„æ•ˆæœ
        """
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        print("=" * 60)
        print("ATTENTION ENHANCEMENT COMPARISON")
        print("=" * 60)
        
        # 1. è¿è¡Œæœªå¢å¼ºç‰ˆæœ¬
        print("\n[1] Running WITHOUT attention enhancement...")
        original_attn = self.pai_enable_attn
        self.pai_enable_attn = False
        
        reply_no_enhance, _ = self.describe_attribute(raw_image, attr_prompt)
        print(f"Without enhancement: {reply_no_enhance}")
        
        # 2. è¿è¡Œå¢å¼ºç‰ˆæœ¬
        print("\n[2] Running WITH attention enhancement...")
        self.pai_enable_attn = True
        
        reply_with_enhance, _ = self.describe_attribute(raw_image, attr_prompt)
        print(f"With enhancement: {reply_with_enhance}")
        
        # 3. æ¢å¤åŸå§‹è®¾ç½®
        self.pai_enable_attn = original_attn
        
        # 4. ä¿å­˜å¯¹æ¯”ç»“æœ
        with open(os.path.join(save_dir, "comparison_results.txt"), "w", encoding="utf-8") as f:
            f.write("ATTENTION ENHANCEMENT COMPARISON\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Prompt: {attr_prompt}\n\n")
            f.write(f"Without enhancement: {reply_no_enhance}\n\n")
            f.write(f"With enhancement: {reply_with_enhance}\n\n")
            f.write(f"Enhancement layers: {self.pai_layers}\n")
            f.write(f"Alpha value: {self.pai_alpha}\n")
        
        print(f"\n[3] Comparison results saved to {save_dir}")
        print("=" * 60)
        
        return reply_no_enhance, reply_with_enhance

    def caption(self, raw_image):
        # starndard way to caption an image in the qwen2_5 paper
        std_prompt = 'a photo of'
        reply = self.__call_qwen2_5(raw_image, std_prompt)
        reply = reply.replace('\n', ' ').strip()  # trim caption
        return reply

    def call_llm(self, prompts):
        prompts_temp = self.qwen2_5_processor(None, prompts, return_tensors="pt")
        model_device = self._get_model_device()
        input_ids = prompts_temp['input_ids'].to(model_device)
        attention_mask = prompts_temp['attention_mask'].to(model_device, torch.float32)

        prompts_embeds = self.qwen2_5.language_model.get_input_embeddings()(input_ids)

        with torch.no_grad():
            outputs = self.qwen2_5.language_model.generate(
                inputs_embeds=prompts_embeds,
                attention_mask=attention_mask)

        outputs = self.qwen2_5_processor.decode(outputs[0], skip_special_tokens=True)
        return outputs
