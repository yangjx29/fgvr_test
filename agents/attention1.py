"""
æŽ¨ç†å¹²é¢„(inference intervention)
æ–¹å¼ï¼šåŠ¨æ€æ›¿æ¢ Transformers ä¸­çš„æ³¨æ„åŠ›å‰å‘ï¼Œæ”¾å¤§å›¾åƒ token çš„æ³¨æ„åŠ›

æ•´ä¸ªè°ƒç”¨é“¾æ˜¯ model.forward() â†’ LlamaModel.forward() â†’ LlamaDecoderLayer.forward() â†’ self_attn.forward()
"""
import math
import types
from typing import Optional, Tuple, Union, Callable, Any
import warnings

import torch
import torch.nn as nn
import torch.nn.functional as F

# å°è¯•å¯¼å…¥Qwenç›¸å…³æ¨¡å—
try:
    from transformers.models.qwen2.modeling_qwen2 import apply_rotary_pos_emb as qwen_apply_rotary_pos_emb
except ImportError:
    qwen_apply_rotary_pos_emb = None
    warnings.warn("Qwen2 rotary position embedding not available")

try:
    from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (
        apply_multimodal_rotary_pos_emb,
        eager_attention_forward 
    )
except ImportError:
    qwen25_apply_mrope = None
    qwen25_eager_attention_forward = None
    warnings.warn("Qwen2.5-VL modules not available")

try:
    from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS
except ImportError:
    QWEN25_ALL_ATTN_FUNCS = {}
    warnings.warn("Attention functions not available")


# def detect_image_tokens(
#     input_ids: Optional[torch.Tensor] = None,
#     position_ids: Optional[torch.Tensor] = None,
#     image_token_mask: Optional[torch.Tensor] = None,
#     img_start_idx: Optional[int] = None,
#     img_end_idx: Optional[int] = None,
#     seq_len: int = 0
# ) -> Tuple[Optional[int], Optional[int]]:
#     """
#     æ£€æµ‹å›¾åƒtokençš„ä½ç½®èŒƒå›´
    
#     Args:
#         input_ids: è¾“å…¥token ids
#         position_ids: ä½ç½®ids
#         image_token_mask: å›¾åƒtokenæŽ©ç  (å¦‚æžœæœ‰çš„è¯)
#         img_start_idx: æ‰‹åŠ¨æŒ‡å®šçš„å›¾åƒå¼€å§‹ä½ç½®
#         img_end_idx: æ‰‹åŠ¨æŒ‡å®šçš„å›¾åƒç»“æŸä½ç½®
#         seq_len: åºåˆ—é•¿åº¦
    
#     Returns:
#         (start_idx, end_idx): å›¾åƒtokençš„èµ·å§‹å’Œç»“æŸä½ç½®
#     """
#     # æ–¹æ³•1: ä½¿ç”¨æä¾›çš„æŽ©ç 
#     if image_token_mask is not None:
#         # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå’Œæœ€åŽä¸€ä¸ªTrueçš„ä½ç½®
#         true_indices = torch.where(image_token_mask)[0]
#         if len(true_indices) > 0:
#             return int(true_indices[0].item()), int(true_indices[-1].item() + 1)
    
#     # æ–¹æ³•2: ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„èŒƒå›´
#     if img_start_idx is not None and img_end_idx is not None:
#         if 0 <= img_start_idx < img_end_idx <= seq_len:
#             return img_start_idx, img_end_idx
    
#     # æ–¹æ³•3: é€šè¿‡ç‰¹æ®Štokenæ£€æµ‹ (éœ€è¦æ ¹æ®å…·ä½“æ¨¡åž‹è°ƒæ•´)
#     if input_ids is not None:
#         # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„å›¾åƒtoken IDæ¥æ£€æµ‹
#         # ä¾‹å¦‚: <image> tokençš„ID
#         image_token_ids = [151644, 151645]  # è¿™äº›æ˜¯ç¤ºä¾‹IDï¼Œéœ€è¦æ ¹æ®å®žé™…æ¨¡åž‹è°ƒæ•´
#         image_positions = []
#         for token_id in image_token_ids:
#             positions = torch.where(input_ids == token_id)[1]  # å‡è®¾input_idsæ˜¯[batch, seq]
#             image_positions.extend(positions.tolist())
        
#         if image_positions:
#             start_pos = min(image_positions)
#             end_pos = max(image_positions) + 1
#             return start_pos, end_pos
    
#     return None, None


# def apply_attention_enhancement(
#     attn_weights: torch.Tensor,
#     img_start_idx: Optional[int],
#     img_end_idx: Optional[int],
#     alpha: float = 1.0,
#     enhancement_type: str = "multiply"
# ) -> torch.Tensor:
#     """
#     å¯¹å›¾åƒtokenåŒºåŸŸçš„æ³¨æ„åŠ›æƒé‡è¿›è¡Œå¢žå¼º
    
#     Args:
#         attn_weights: æ³¨æ„åŠ›æƒé‡ [batch, heads, q_len, kv_len]
#         img_start_idx: å›¾åƒtokenå¼€å§‹ä½ç½®
#         img_end_idx: å›¾åƒtokenç»“æŸä½ç½®
#         alpha: å¢žå¼ºç³»æ•°
#         enhancement_type: å¢žå¼ºç±»åž‹ ("multiply", "add", "replace")
    
#     Returns:
#         å¢žå¼ºåŽçš„æ³¨æ„åŠ›æƒé‡
#     """
#     if img_start_idx is None or img_end_idx is None:
#         return attn_weights
    
#     # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
#     kv_len = attn_weights.size(-1)
#     img_start_idx = max(0, min(img_start_idx, kv_len))
#     img_end_idx = max(img_start_idx, min(img_end_idx, kv_len))
    
#     if img_start_idx >= img_end_idx:
#         return attn_weights
    
#     # åˆ›å»ºå¢žå¼ºåŽçš„æ³¨æ„åŠ›æƒé‡
#     enhanced_weights = attn_weights.clone()
#     image_region = enhanced_weights[:, :, :, img_start_idx:img_end_idx]
    
#     if enhancement_type == "multiply":
#         # ä¹˜æ³•å¢žå¼º
#         enhanced_weights[:, :, -1, img_start_idx:img_end_idx] = image_region * alpha
#     elif enhancement_type == "add":
#         # åŠ æ³•å¢žå¼º
#         enhanced_weights[:, :, -1, img_start_idx:img_end_idx] = image_region + (image_region.abs() * alpha)
#     elif enhancement_type == "replace":
#         # æ›¿æ¢å¢žå¼º
#         enhanced_weights[:, :, -1, img_start_idx:img_end_idx] = image_region.abs() * alpha
#     else:
#         warnings.warn(f"Unknown enhancement type: {enhancement_type}, using multiply")
#         enhanced_weights[:, :, :, img_start_idx:img_end_idx] = image_region * alpha
    
#     return enhanced_weights


"""
è¿™é‡Œæ˜¯qwençš„æºç 
def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_values is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            position_ids=position_ids,  # pass positions for FA2
            **kwargs,
        )

        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights
"""


def qwen_modify_attention(
    model: Any,
    start_layer: int = 0,
    end_layer: int = -1,
    use_attn: bool = True,
    alpha: float = 1.5,
    use_cfg: bool = False,
    img_start_idx: Optional[int] = None,
    img_end_idx: Optional[int] = None,
    enhancement_type: str = "multiply",
    image_token_ids: Optional[list] = None,
    verbose: bool = True
) -> bool:
    """
    ä¸º Qwen2 ç³»æ¨¡åž‹æ³¨å…¥æ³¨æ„åŠ›æ”¾å¤§å¼€å…³ä¸Žå‚æ•°ï¼ˆæŽ¨ç†æœŸï¼‰ã€‚
    
    Args:
        model: è¦ä¿®æ”¹çš„æ¨¡åž‹
        start_layer: å¼€å§‹ä¿®æ”¹çš„å±‚ç´¢å¼•
        end_layer: ç»“æŸä¿®æ”¹çš„å±‚ç´¢å¼• (-1è¡¨ç¤ºåˆ°æœ€åŽä¸€å±‚)
        use_attn: æ˜¯å¦å¯ç”¨æ³¨æ„åŠ›å¢žå¼º
        alpha: æ³¨æ„åŠ›å¢žå¼ºç³»æ•°
        use_cfg: æ˜¯å¦åœ¨CFGæ¨¡å¼ä¸‹ä½¿ç”¨
        img_start_idx: å›¾åƒtokenå¼€å§‹ä½ç½®
        img_end_idx: å›¾åƒtokenç»“æŸä½ç½®
        enhancement_type: å¢žå¼ºç±»åž‹ ("multiply", "add", "replace")
        image_token_ids: å›¾åƒtokençš„IDåˆ—è¡¨ï¼Œç”¨äºŽè‡ªåŠ¨æ£€æµ‹
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸä¿®æ”¹
    """
    if not use_attn:
        if verbose:
            print("wrong æ³¨æ„åŠ›å¢žå¼ºå·²ç¦ç”¨")
        return True
    
    # æ£€æŸ¥æ¨¡åž‹ç»“æž„
    if not hasattr(model, 'model') or not hasattr(model.model, 'language_model'):
        if verbose:
            print("wrong æ¨¡åž‹ç»“æž„ä¸æ”¯æŒï¼Œè·³è¿‡ä¿®æ”¹")
        return False
    
    # ç¡®å®šå±‚èŒƒå›´
    total_layers = len(model.model.language_model.layers)
    if end_layer == -1:
        end_layer = total_layers
    end_layer = min(end_layer, total_layers)
    
    if start_layer >= end_layer:
        if verbose:
            print(f"wrong æ— æ•ˆçš„å±‚èŒƒå›´: {start_layer}-{end_layer}")
        return False
    
    if verbose:
        print(f"ä¿®æ”¹å±‚èŒƒå›´: {start_layer}-{end_layer}, å¢žå¼ºç³»æ•°: {alpha}")
    
    # å°è¯•å¯¼å…¥Qwen2Attention
    try:
        from transformers.models.qwen2.modeling_qwen2 import Qwen2Attention
    except ImportError:
        if verbose:
            print("wrong transformers æœªåŒ…å« Qwen2Attentionï¼Œå°è¯•å…¶ä»–æ–¹å¼")
        # å°è¯•å…¶ä»–å¯èƒ½çš„å¯¼å…¥è·¯å¾„
        try:
            from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2VLAttention as Qwen2Attention
        except ImportError:
            if verbose:
                print("wrong æ— æ³•æ‰¾åˆ°Qwenæ³¨æ„åŠ›æ¨¡å—ï¼Œè·³è¿‡æ³¨å…¥")
            return False

    # def qwen_new_forward(self,
    #                      hidden_states: torch.Tensor,
    #                      attention_mask: Optional[torch.Tensor] = None,
    #                      position_ids: Optional[torch.Tensor] = None,
    #                      past_key_values: Optional[Any] = None,
    #                      output_attentions: bool = False,
    #                      use_cache: bool = False,
    #                      cache_position: Optional[torch.Tensor] = None,
    #                      position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    #                      **kwargs) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    #     """
    #     å¢žå¼ºçš„æ³¨æ„åŠ›å‰å‘ä¼ æ’­å‡½æ•°ï¼Œæ”¯æŒå›¾åƒtokenæ³¨æ„åŠ›å¢žå¼º
    #     """
    #     bsz, q_len, _ = hidden_states.size()
        
    #     # èŽ·å–é…ç½®å‚æ•°
    #     use_attn_flag = getattr(self, 'use_attn', False)
    #     use_cfg_flag = getattr(self, 'use_cfg', False)
    #     alpha_v = getattr(self, 'alpha', 1.0)
    #     enhancement_type = getattr(self, 'enhancement_type', 'multiply')
    #     image_token_ids = getattr(self, 'image_token_ids', None)
    #     verbose = getattr(self, 'verbose', False)
        
    #     if verbose:
    #         print(f'[ATTN] Layer {getattr(self, "layer_idx", "unknown")}: bsz={bsz}, q_len={q_len}')
        
    #     # Step 1: çº¿æ€§å˜æ¢å¾—åˆ° QKV
    #     query_states = self.q_proj(hidden_states)
    #     key_states = self.k_proj(hidden_states)
    #     value_states = self.v_proj(hidden_states)

    #     # é‡å¡‘ä¸º multi-head å½¢å¼
    #     query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
    #     key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
    #     value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

    #     # Step 2: åº”ç”¨ RoPE
    #     if position_embeddings is None:
    #         # å°è¯•ä»ŽkwargsèŽ·å–
    #         position_embeddings = kwargs.get('position_embeddings', None)
        
    #     if position_embeddings is None:
    #         if verbose:
    #             print("[ATTN] è­¦å‘Š: ç¼ºå°‘ position_embeddingsï¼Œè·³è¿‡ RoPE")
    #     else:
    #         cos, sin = position_embeddings
    #         # åº”ç”¨ RoPE
    #         if qwen25_apply_mrope is not None:
    #             try:
    #                 query_states, key_states = qwen25_apply_mrope(
    #                     query_states, key_states, cos, sin, 
    #                     self.rope_scaling.get("mrope_section", None), 
    #                     unsqueeze_dim=1
    #                 )
    #             except Exception as e:
    #                 if verbose:
    #                     print(f"wrong RoPE åº”ç”¨å¤±è´¥: {e}")
    #         elif qwen_apply_rotary_pos_emb is not None:
    #             try:
    #                 query_states, key_states = qwen_apply_rotary_pos_emb(
    #                     query_states, key_states, cos, sin
    #                 )
    #             except Exception as e:
    #                 if verbose:
    #                     print(f"wrong ä¼ ç»Ÿ RoPE åº”ç”¨å¤±è´¥: {e}")

    #     # Step 3: æ›´æ–° KV Cache
    #     if past_key_values is not None:
    #         try:
    #             cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
    #             key_states, value_states = past_key_values.update(
    #                 key_states, value_states, getattr(self, 'layer_idx', 0), cache_kwargs
    #             )
    #         except Exception as e:
    #             if verbose:
    #                 print(f"wrong KV Cache æ›´æ–°å¤±è´¥: {e}")

    #     # Step 4: è°ƒç”¨æ³¨æ„åŠ›æ ¸å¿ƒå‡½æ•°ï¼ˆæ”¯æŒ eager / flash_attnï¼‰
    #     attention_interface = qwen25_eager_attention_forward
    #     if qwen25_eager_attention_forward is None:
    #         # å›žé€€åˆ°æ ‡å‡†æ³¨æ„åŠ›
    #         from transformers.modeling_utils import eager_attention_forward
    #         attention_interface = eager_attention_forward
        
    #     if getattr(self.config, '_attn_implementation', 'eager') != 'eager':
    #         if self.config._attn_implementation in QWEN25_ALL_ATTN_FUNCS:
    #             attention_interface = QWEN25_ALL_ATTN_FUNCS[self.config._attn_implementation]
        
    #     try:
    #         attn_output, attn_weights = attention_interface(
    #             self,
    #             query_states,
    #             key_states,
    #             value_states,
    #             attention_mask,
    #             dropout=0.0 if not self.training else getattr(self, 'attention_dropout', 0.0),
    #             scaling=getattr(self, 'scaling', 1.0),
    #             sliding_window=getattr(self, 'sliding_window', None),
    #             position_ids=position_ids,
    #             **kwargs,
    #         )
    #     except Exception as e:
    #         if verbose:
    #             print(f"wrong æ³¨æ„åŠ›è®¡ç®—å¤±è´¥: {e}")
    #         # å›žé€€åˆ°ç®€å•å®žçŽ°
    #         attn_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)
    #         if attention_mask is not None:
    #             attn_weights = attn_weights + attention_mask
    #         attn_weights = F.softmax(attn_weights, dim=-1)
    #         attn_output = torch.matmul(attn_weights, value_states)

    #     # Step 5: å›¾åƒtokenæ³¨æ„åŠ›å¢žå¼º
    #     if (use_attn_flag and not use_cfg_flag and 
    #         attn_weights is not None and q_len > 0):
            
    #         # æ£€æµ‹å›¾åƒtokenä½ç½®
    #         img_s, img_e = detect_image_tokens(
    #             input_ids=kwargs.get('input_ids'),
    #             position_ids=position_ids,
    #             image_token_mask=kwargs.get('image_token_mask'),
    #             img_start_idx=getattr(self, 'img_start_idx', None),
    #             img_end_idx=getattr(self, 'img_end_idx', None),
    #             seq_len=key_states.size(2)
    #         )
            
    #         if img_s is not None and img_e is not None:
    #             if verbose:
    #                 print(f'[ATTN] å¢žå¼ºå›¾åƒtokenæ³¨æ„åŠ›: {img_s}-{img_e}, alpha={alpha_v}')
                
    #             # åº”ç”¨æ³¨æ„åŠ›å¢žå¼º
    #             attn_weights = apply_attention_enhancement(
    #                 attn_weights, img_s, img_e, alpha_v, enhancement_type
    #             )
                
    #             # é‡æ–°è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
    #             attn_output = torch.matmul(attn_weights, value_states)
    #         else:
    #             print(f'img_s is None or img_e is None, imgs:{img_s}, imge:{img_e}')
    #     # Step 6: è¾“å‡ºæŠ•å½±
    #     attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
    #     attn_output = self.o_proj(attn_output)
        
    #     if not output_attentions:
    #         attn_weights = None
            
    #     return attn_output, attn_weights
    def qwen_new_forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        # Step 1: QKV æŠ•å½±
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Reshape to multi-head format
        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        # Step 2: å¤šæ¨¡æ€ RoPE (MRoPE)
        if position_embeddings is None:
            raise ValueError("position_embeddings must be provided for Qwen2.5-VL.")
        cos, sin = position_embeddings
        rope_section = self.rope_scaling.get("mrope_section", None)
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, position_ids, rope_section
        )

        # Step 3: KV Cache æ›´æ–°
        if past_key_values is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # Step 4: ä½¿ç”¨ eager æˆ– Flash Attention å®žçŽ°è®¡ç®— attn_output å’Œ attn_weights
        # TODOè¿™é‡Œä¸èƒ½ç›´æŽ¥ç”¨eagerï¼Œå› ä¸ºeageræ˜¯å·²ç» softmax è¿‡äº†
        attention_interface = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementatioÂ·n]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            position_ids=position_ids,
            **kwargs,
        )
        # æ³¨æ„ï¼šattn_weights æ˜¯ softmax å‰çš„ logitsï¼è¿™æ˜¯å…³é”®ï¼

        # --- ðŸ”¥ PAI's modification: æ”¾å¤§æœ€åŽä¸€ä¸ª token å¯¹ image tokens çš„æ³¨æ„åŠ› ---
        # è¯»å–æ³¨å…¥çš„æŽ§åˆ¶å‚æ•°ï¼ˆæ¥è‡ª llama_modifyï¼‰
        use_attn = getattr(self, "use_attn", False)
        use_cfg = getattr(self, "use_cfg", False)
        img_start_idx = getattr(self, "img_start_idx", None)
        img_end_idx = getattr(self, "img_end_idx", None)
        alpha = getattr(self, "alpha", 0.0)

        if use_attn and (not use_cfg) and attn_weights is not None and q_len > 0:
            if img_start_idx is not None and img_end_idx is not None:
                kv_seq_len = attn_weights.size(-1)
                if img_start_idx < kv_seq_len and img_end_idx <= kv_seq_len and img_start_idx < img_end_idx:
                    # åªä¿®æ”¹æœ€åŽä¸€ä¸ª query token çš„ scores
                    device = attn_weights.device
                    dtype = attn_weights.dtype
                    slice_ = attn_weights[:, :, -1, img_start_idx:img_end_idx]  # [B, H, LEN_IMG]

                    # æ ¸å¿ƒå…¬å¼ï¼š|w| * alpha + w
                    boosted_slice = slice_.abs() * alpha + slice_
                    attn_weights[:, :, -1, img_start_idx:img_end_idx] = boosted_slice.to(dtype)

        # --- ðŸ”¥ END OF PAI's modification ---

        # Step 5: Softmax å½’ä¸€åŒ–ï¼ˆç”± attention_interface å†…éƒ¨å®Œæˆï¼Ÿè§†å®žçŽ°è€Œå®šï¼‰
        # âš ï¸ æ³¨æ„ï¼šeager_attention_forward è¿”å›žçš„æ˜¯ softmax åŽçš„ç»“æžœå—ï¼Ÿ
        # æ ¹æ® HF å®žçŽ°ï¼Œé€šå¸¸è¿”å›žçš„æ˜¯ softmax åŽçš„æ¦‚çŽ‡åˆ†å¸ƒã€‚
        # æ‰€ä»¥æˆ‘ä»¬ä¸Šé¢çš„æ“ä½œæ˜¯åœ¨ softmax å‰åšçš„å—ï¼ŸâŒ ä¸ä¸€å®šï¼

        # â—å› æ­¤æˆ‘ä»¬å¿…é¡»ç¡®è®¤ï¼šattn_weights æ˜¯ logits è¿˜æ˜¯ probs
        # å¦‚æžœæ˜¯ probsï¼Œåˆ™ä¸èƒ½è¿™æ ·æ”¹ï¼å¿…é¡» hook åˆ°æ›´åº•å±‚ï¼

        # ðŸ› ï¸ å®‰å…¨èµ·è§ï¼šå‡è®¾å®ƒæ˜¯ logitsï¼ˆå¦‚æŸäº›ç‰ˆæœ¬ï¼‰ï¼Œå¦åˆ™æ­¤ä¿®æ”¹æ— æ•ˆã€‚
        # æ›´æŽ¨èåšæ³•ï¼šä½¿ç”¨è‡ªå®šä¹‰ attention_interfaceï¼Œä½†å¤ªå¤æ‚ã€‚

        # å½“å‰ç­–ç•¥ï¼šä¿¡ä»» attn_weights æ˜¯ softmax å‰çš„ logitsï¼ˆéƒ¨åˆ† HF å®žçŽ°å¦‚æ­¤ï¼‰

        # Step 6: reshape è¾“å‡º
        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)

        return attn_output, attn_weights

    # ä¿®æ”¹æŒ‡å®šå±‚çš„å‚æ•°
    success_count = 0
    for i in range(start_layer, end_layer):
        try:
            attn = model.model.language_model.layers[i].self_attn
            if verbose:
                print(f'ä¿®æ”¹ç¬¬ {i} å±‚æ³¨æ„åŠ›æ¨¡å—\natten:{attn}')
            
            # è®¾ç½®å¢žå¼ºå‚æ•°
            attn.use_attn = use_attn
            attn.alpha = alpha
            attn.use_cfg = use_cfg
            attn.img_start_idx = img_start_idx
            attn.img_end_idx = img_end_idx
            attn.enhancement_type = enhancement_type
            attn.image_token_ids = image_token_ids
            attn.verbose = verbose
            attn.layer_idx = i
            
             # åŠ¨æ€æ›¿æ¢æ¨¡åž‹ä¸­ç¬¬iå±‚self-attentionçš„forwardæ–¹æ³•ã€‚ ç»‘å®šåŽï¼Œè°ƒç”¨æ—¶ self ä¼šè‡ªåŠ¨æŒ‡å‘ model.model.layers[i].self_attn
            attn.forward = types.MethodType(qwen_new_forward, attn)
            success_count += 1
            
        except Exception as e:
            if verbose:
                print(f'wrongç¬¬ {i} å±‚ä¿®æ”¹å¤±è´¥: {e}')
            continue
    
    if verbose:
        print(f'[ATTN] æˆåŠŸä¿®æ”¹ {success_count}/{end_layer - start_layer} å±‚')
    
    return success_count > 0


