"""
ç»éªŒåº“æ„å»ºæ¨¡å— - æ¨¡å‹è‡ªåæ€ä¸ä¼˜åŒ–
åŸºäºSelf-Beliefå’ŒWorld-Beliefçš„ç»†ç²’åº¦è§†è§‰è¯†åˆ«ç­–ç•¥ä¼˜åŒ–

æ ¸å¿ƒæ€æƒ³ï¼š
- Self-Beliefï¼šå¯¹è‡ªèº«å½“å‰æ¨ç†èƒ½åŠ›ä¸ç­–ç•¥çš„å…ƒè®¤çŸ¥
- World-Beliefï¼šå¯¹æ¯ä¸ªç»†ç²’åº¦ç±»åˆ«çš„é€šç”¨è§†è§‰-è¯­ä¹‰æè¿°ï¼ˆç”±knowledge_base_builderæä¾›ï¼‰
- é€šè¿‡å¤±è´¥ç»éªŒ â†’ åæ€ä¿¡å¿µåå·® â†’ æç‚¼ç­–ç•¥è§„åˆ™ â†’ ä¼˜åŒ–è¡Œä¸ºæŒ‡ä»¤ â†’ å®ç°ç­–ç•¥è¿›åŒ–
"""

import os
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
from PIL import Image
import re
from collections import defaultdict

from agents.mllm_bot import MLLMBot
from knowledge_base_builder import KnowledgeBaseBuilder
from utils.fileios import dump_json, load_json
from utils.util import is_similar


class ExperienceBaseBuilder:
    """ç»éªŒåº“æ„å»ºå™¨ - åŸºäºæ¨¡å‹è‡ªåæ€ä¸ä¼˜åŒ–"""
    
    # åˆå§‹Self-Beliefç­–ç•¥
    INITIAL_SELF_BELIEF = """You are an expert in fine-grained visual recognition. Please follow these steps:
    1. Observe: First, look at the overall object and identify its coarse category (e.g., bird, dog, car).
    2. Localize: Then, identify the most discriminative local parts.
    3. Compare: Recall the visual characteristics of the candidate subcategories from your knowledge.
    4. Decide: Based on the match between observed details and subcategory descriptions, choose the most likely class.
    Answer only with the final class name."""
    
    def __init__(self,
                 mllm_bot: MLLMBot,
                 knowledge_base_builder: KnowledgeBaseBuilder,
                 fast_thinking_module,
                 slow_thinking_module,
                 device: str = "cuda",
                 dataset_info: dict = None):
        """
        åˆå§‹åŒ–ç»éªŒåº“æ„å»ºå™¨
        
        Args:
            mllm_bot: MLLMæ¨¡å‹
            knowledge_base_builder: çŸ¥è¯†åº“æ„å»ºå™¨ï¼ˆæä¾›World-Beliefï¼‰
            fast_thinking_module: å¿«æ€è€ƒæ¨¡å—
            slow_thinking_module: æ…¢æ€è€ƒæ¨¡å—
            device: è®¾å¤‡
        """
        self.mllm_bot = mllm_bot
        self.kb_builder = knowledge_base_builder
        self.fast_thinking = fast_thinking_module
        self.slow_thinking = slow_thinking_module
        self.device = device
        self.dataset_info = dataset_info or {}
        
        # Self-Beliefï¼šå½“å‰æ¨ç†ç­–ç•¥
        self.max_strategy_rules = 8    ### todo ç»éªŒæ¡æ•°
        self.strategy_rules = []
        self.next_rule_id = 1
        self.self_belief_core = self.INITIAL_SELF_BELIEF
        self.self_belief = self._compose_self_belief_prompt()
        
        # ä¼ªè½¨è¿¹å­˜å‚¨ï¼šè®°å½•æ¨ç†è¿‡ç¨‹
        self.pseudo_trajectories = []  # List[Dict]: {image_path, cot, prediction, label, top_k_candidates}
        
        # åæ€å†å²ï¼šè®°å½•æ¯æ¬¡åæ€çš„ç­–ç•¥æ”¹è¿›
        self.reflection_history = []  # List[Dict]: {rule_id, rule, failure_pattern, added_at}
        
        # ç»éªŒåº“å­˜å‚¨è·¯å¾„
        self.save_dir = None

        # è¾“å‡ºåˆå§‹åŒ–å‚æ•°ä¿¡æ¯
        print("\n================= ç»éªŒåº“æ„å»ºå™¨åˆå§‹åŒ– =================")
        print(f"ğŸ–¥ï¸ è®¾å¤‡: {self.device}")
        print(f"ğŸ¤– ä½¿ç”¨çš„ MLLM æ¨¡å‹: {type(self.mllm_bot).__name__}")
        print(f"ğŸ“š çŸ¥è¯†åº“æ„å»ºå™¨: {type(self.kb_builder).__name__}")
        print(f"âš¡ å¿«æ€è€ƒæ¨¡å—: {self.fast_thinking}")
        print(f"ğŸ¢ æ…¢æ€è€ƒæ¨¡å—: {self.slow_thinking}")
        print(f"ğŸ“¦ æ•°æ®é›†ä¿¡æ¯: {self.dataset_info}")
        print("====================================================\n")
    
    def initialize_self_belief(self, custom_belief: Optional[str] = None):
        """
        åˆå§‹åŒ–Self-Belief
        
        Args:
            custom_belief: è‡ªå®šä¹‰çš„åˆå§‹ä¿¡å¿µï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        self.strategy_rules = []
        self.next_rule_id = 1
        if custom_belief:
            self.self_belief_core = custom_belief
        else:
            self.self_belief_core = self.INITIAL_SELF_BELIEF
        self.self_belief = self._compose_self_belief_prompt()
        
        print("Self-Beliefå·²åˆå§‹åŒ–")
        print(f"åˆå§‹ç­–ç•¥:\n{self.self_belief}")
    
    def _compose_self_belief_prompt(self) -> str:
        """
        æ„é€ åŒ…å«ç­–ç•¥è§„åˆ™çš„Self-Beliefæç¤ºè¯
        """
        if not self.strategy_rules:
            return self.self_belief_core
        
        rules_text = "\n\nPolicy Memory (learned strategy rules):\n"
        for idx, rule in enumerate(self.strategy_rules, start=1):
            condition = rule.get("applicability_signals", "applicable to challenging cases")
            rules_text += f"{idx}. {rule['rule']} (Trigger: {condition})\n"
        return f"{self.self_belief_core}{rules_text}"
    
    def _refresh_self_belief_prompt(self):
        """
        æ ¹æ®å½“å‰ç­–ç•¥è§„åˆ™åˆ·æ–°Self-Belief
        """
        self.self_belief = self._compose_self_belief_prompt()
    
    def generate_cot_with_self_belief(self, 
                                      image_path: str,
                                      top_k_candidates: List[Tuple[str, float]],
                                      world_belief_context: Optional[str] = None) -> Tuple[str, str]:
        """
        ä½¿ç”¨å½“å‰Self-Beliefç”ŸæˆCoTæ¨ç†é“¾å’Œæœ€ç»ˆé¢„æµ‹
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            top_k_candidates: Top-Kå€™é€‰ç±»åˆ«åˆ—è¡¨ [(category, score), ...]
            world_belief_context: World-Beliefä¸Šä¸‹æ–‡ï¼ˆç±»åˆ«æè¿°ç­‰ï¼‰
            
        Returns:
            Tuple[str, str]: (CoTæ¨ç†é“¾, æœ€ç»ˆé¢„æµ‹ç±»åˆ«)
        """
        image = Image.open(image_path).convert("RGB")
        
        # æ„å»ºå€™é€‰ç±»åˆ«æ–‡æœ¬
        candidate_text = ""
        for i, (cat, score) in enumerate(top_k_candidates, start=1):
            candidate_text += f"{i}. {cat} (similarity: {score:.4f})\n"
        
        # æ„å»ºå®Œæ•´æç¤ºè¯ï¼šSelf-Belief + World-Belief + å€™é€‰ç±»åˆ«
        prompt = f"""{self.self_belief}
        Candidate classes (highly likely to contain the correct option):
        {candidate_text}"""     
        # æ·»åŠ World-Beliefä¸Šä¸‹æ–‡ï¼ˆç±»åˆ«æè¿°ï¼‰
        if world_belief_context:
            prompt += f"\n\nCategory descriptions:\n{world_belief_context}\n"
        
        prompt += """\nPlease analyze the image step by step and provide:
            1. Your reasoning chain (CoT) following the steps above
            2. Your final prediction (only the category name)
            Format your response as:
            Reasoning: [your step-by-step reasoning]
            Prediction: [category name]"""
                
        # è°ƒç”¨MLLMç”Ÿæˆæ¨ç†é“¾
        reply, response = self.mllm_bot.describe_attribute(image, prompt)
        if isinstance(response, list):
            response = " ".join(response)
        
        # è§£æCoTå’Œé¢„æµ‹
        cot = ""
        prediction = "unknown"
        
        # æå–æ¨ç†é“¾
        reasoning_match = re.search(r'Reasoning[:\s]+(.*?)(?=Prediction|$)', response, re.DOTALL | re.IGNORECASE)
        if reasoning_match:
            cot = reasoning_match.group(1).strip()
        else:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®æ ‡è®°ï¼Œå°è¯•æå–æ•´ä¸ªå“åº”ä½œä¸ºæ¨ç†é“¾
            cot = response
        
        # æå–é¢„æµ‹
        prediction_match = re.search(r'Prediction[:\s]+([^\n]+)', response, re.IGNORECASE)
        if prediction_match:
            # print(f"è§£ææˆåŠŸ! é¢„æµ‹: {prediction_match.group(1).strip()}")
            prediction = prediction_match.group(1).strip()
        else:
            # å°è¯•ä»å“åº”æœ«å°¾æå–ç±»åˆ«åç§°
            lines = response.strip().split('\n')
            if lines:
                last_line = lines[-1].strip()
                # æ£€æŸ¥æ˜¯å¦æ˜¯å€™é€‰ç±»åˆ«ä¹‹ä¸€
                for cat, _ in top_k_candidates:
                    if cat.lower() in last_line.lower() or last_line.lower() in cat.lower():
                        prediction = cat
                        break
                if prediction == "unknown":
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå€™é€‰ä½œä¸ºfallback
                    prediction = top_k_candidates[0][0] if top_k_candidates else "unknown"
            
        print(f"æ¨ç†é“¾: {cot}")
        print(f"é¢„æµ‹: {prediction}")
        return cot, prediction
    
    def build_pseudo_trajectories(self,
                                  validation_samples: Dict[str, List[str]],
                                  top_k: int = 5,
                                  max_samples_per_category: Optional[int] = None) -> List[Dict]:
        """
        æ„é€ ä¼ªè½¨è¿¹ï¼šä½¿ç”¨å½“å‰Self-Belief + World-Beliefè¿›è¡Œæ¨ç†
        
        Args:
            validation_samples: {true_category: [image_paths]} éªŒè¯æ ·æœ¬
            top_k: æ£€ç´¢top-kç»“æœ
            max_samples_per_category: æ¯ä¸ªç±»åˆ«æœ€å¤šå¤„ç†çš„æ ·æœ¬æ•°
            
        Returns:
            List[Dict]: ä¼ªè½¨è¿¹åˆ—è¡¨ï¼Œæ¯ä¸ªè½¨è¿¹åŒ…å« {image_path, cot, prediction, label, top_k_candidates}
        """
        print("å¼€å§‹æ„é€ ä¼ªè½¨è¿¹...")
        self.pseudo_trajectories = []
        
        total_samples = sum(len(paths) for paths in validation_samples.values())
        processed = 0
        
        for true_category, image_paths in validation_samples.items():
            print(f"å¤„ç†ç±»åˆ«: {true_category} ({len(image_paths)} å¼ å›¾åƒ)")
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
            if max_samples_per_category:
                image_paths = image_paths[:max_samples_per_category]
            
            # è·å–World-Beliefï¼šè¯¥ç±»åˆ«çš„æè¿°
            world_belief_context = self._get_world_belief_context(true_category)
            
            for img_path in tqdm(image_paths, desc=f"å¤„ç† {true_category}"):
                # 1. å¿«æ€è€ƒè·å–Top-Kå€™é€‰
                fast_result = self.fast_thinking.fast_thinking_pipeline(img_path, top_k)
                top_k_candidates = fast_result.get("fused_results", [])[:top_k]
                
                if not top_k_candidates:
                    print(f"è­¦å‘Š: {img_path} æ²¡æœ‰æ£€ç´¢åˆ°å€™é€‰ç±»åˆ«")
                    continue
                
                # 2. ä½¿ç”¨Self-Beliefç”ŸæˆCoTå’Œé¢„æµ‹
                cot, prediction = self.generate_cot_with_self_belief(
                    img_path, top_k_candidates, world_belief_context
                )
                
                # 3. è®°å½•ä¼ªè½¨è¿¹
                trajectory = {
                    "image_path": img_path,
                    "cot": cot,
                    "prediction": prediction,
                    "label": true_category,
                    "top_k_candidates": top_k_candidates,
                    "is_correct": is_similar(prediction, true_category, threshold=0.3)
                }
                
                self.pseudo_trajectories.append(trajectory)
                processed += 1
        
        print(f"ä¼ªè½¨è¿¹æ„é€ å®Œæˆ! å…± {len(self.pseudo_trajectories)} æ¡è½¨è¿¹")
        print(f"æ­£ç¡®é¢„æµ‹: {sum(1 for t in self.pseudo_trajectories if t['is_correct'])}")
        print(f"é”™è¯¯é¢„æµ‹: {sum(1 for t in self.pseudo_trajectories if not t['is_correct'])}")
        
        return self.pseudo_trajectories
    
    def _get_world_belief_context(self, category: str) -> str:
        """
        è·å–World-Beliefä¸Šä¸‹æ–‡ï¼ˆç±»åˆ«æè¿°ï¼‰
        
        Args:
            category: ç±»åˆ«åç§°
            
        Returns:
            str: ç±»åˆ«æè¿°
        """
        if hasattr(self.kb_builder, 'category_descriptions'):
            # category_descriptions åº”ä¸º {ç±»åˆ«å: æè¿°} çš„å­—å…¸
            description = self.kb_builder.category_descriptions.get(category, "")
            if description:
                return f"{category}: {description}"
        return f"{category}: No description available"
    
    def _normalize_rule_text(self, text: str) -> str:
        """
        è§„èŒƒåŒ–ç­–ç•¥æ–‡æœ¬ç”¨äºå»é‡
        """
        cleaned = text.lower().strip()
        cleaned = re.sub(r'[^a-z0-9\s]', ' ', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned.strip()
    
    def _aggregate_rules(self, candidates: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶é‡å¤æˆ–ç›¸ä¼¼çš„ç­–ç•¥è§„åˆ™
        """
        aggregated = {}
        for candidate in candidates:
            rule_text = candidate.get("rule", "").strip()
            if not rule_text:
                continue
            normalized = self._normalize_rule_text(rule_text)
            if not normalized:
                continue
            if normalized in aggregated:
                aggregated_rule = aggregated[normalized]
                aggregated_rule["support"] += candidate.get("support", 1)
                aggregated_rule["source_images"].extend(candidate.get("source_images", []))
                aggregated_rule["labels"].extend(candidate.get("labels", []))
                aggregated_rule["failure_patterns"].append(candidate.get("failure_pattern", ""))
            else:
                candidate["normalized"] = normalized
                candidate.setdefault("support", 1)
                candidate.setdefault("source_images", [])
                candidate.setdefault("labels", [])
                candidate.setdefault("failure_patterns", [])
                aggregated[normalized] = candidate
        return list(aggregated.values())
    
    def _add_strategy_rule(self, rule_entry: Dict) -> bool:
        """
        å°†æ–°çš„ç­–ç•¥è§„åˆ™å†™å…¥ç¼“å­˜ï¼Œå¹¶æ ¹æ®å®¹é‡æ§åˆ¶ç­–ç•¥
        """
        rule_text = rule_entry.get("rule", "").strip()
        if not rule_text:
            return False
        
        normalized = self._normalize_rule_text(rule_text)
        if not normalized:
            return False
        
        added = False
        for existing in self.strategy_rules:
            if existing.get("normalized") == normalized:
                existing["support"] += rule_entry.get("support", 1)
                existing["source_images"].extend(rule_entry.get("source_images", []))
                existing["labels"].extend(rule_entry.get("labels", []))
                existing["failure_patterns"].extend(rule_entry.get("failure_patterns", []))
                existing["priority"] = rule_entry.get("priority", existing.get("priority", "medium"))
                added = True
                break
        
        if not added:
            rule_entry["normalized"] = normalized
            rule_entry["id"] = f"R{self.next_rule_id}"
            self.next_rule_id += 1
            rule_entry.setdefault("support", 1)
            rule_entry.setdefault("source_images", [])
            rule_entry.setdefault("labels", [])
            rule_entry.setdefault("failure_patterns", [])
            rule_entry.setdefault("priority", "medium")
            rule_entry.setdefault("applicability_signals", "challenging scenarios")
            rule_entry.setdefault("effectiveness", 0.0)
            rule_entry.setdefault("applicability", 0.0)
            self.strategy_rules.append(rule_entry)
            self.reflection_history.append({
                "rule_id": rule_entry["id"],
                "rule": rule_entry["rule"],
                "failure_pattern": rule_entry.get("failure_pattern", ""),
                "source_images": rule_entry.get("source_images", []),
                "added_at": len(self.reflection_history) + 1
            })
            added = True
        
        if len(self.strategy_rules) > self.max_strategy_rules:
            self._manage_rule_capacity()
        
        return added
    
    def _manage_rule_capacity(self):
        """
        å½“ç­–ç•¥ç¼“å­˜è¶…è¿‡å®¹é‡æ—¶ï¼Œè°ƒç”¨è¯„ä¼°æœºåˆ¶ä¿ç•™æœ€æœ‰ä»·å€¼çš„è§„åˆ™
        """
        if len(self.strategy_rules) <= self.max_strategy_rules:
            return
        
        ranked = self._rank_rules_for_retention()
        if ranked:
            keep_ids = set(ranked.get("keep_ids", []))
            if keep_ids:
                self.strategy_rules = [rule for rule in self.strategy_rules if rule["id"] in keep_ids]
        if len(self.strategy_rules) > self.max_strategy_rules:
            self.strategy_rules = self.strategy_rules[:self.max_strategy_rules]
    
    def _rank_rules_for_retention(self) -> Dict:
        """
        ä½¿ç”¨MLLMè¯„ä¼°ç­–ç•¥è§„åˆ™çš„ä¿ç•™ä¼˜å…ˆçº§
        """
        summary_lines = []
        for rule in self.strategy_rules:
            summary_lines.append(
                f"{rule['id']}: {rule['rule']} | support={rule.get('support', 1)} | "
                f"priority={rule.get('priority', 'medium')} | effectiveness={rule.get('effectiveness', 0.0)} | "
                f"applicability={rule.get('applicability', 0.0)}"
            )
        
        prompt = f"""You are maintaining a policy rule cache for fine-grained recognition. Capacity is {self.max_strategy_rules}.
Rules:
{chr(10).join(summary_lines)}

Please decide which rules to keep to maximize generalization coverage. Return ONLY a JSON object:
{{
    "keep_ids": ["R1", "R2", ...],  // exactly {self.max_strategy_rules} IDs if possible
    "drop_ids": ["R3", ...]
}}"""
        
        dummy_image = Image.new('RGB', (224, 224), color='white')
        reply, response = self.mllm_bot.describe_attribute(dummy_image, prompt)
        if isinstance(response, list):
            response = " ".join(response)
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            decision = json.loads(json_str)
            return decision
        return {}
    
    def reflect_on_failed_samples(self,
                                  failed_trajectories: List[Dict],
                                  max_reflections: int = 5) -> List[Dict]:
        """
        å¯¹å¤±è´¥çš„æ ·æœ¬è¿›è¡Œåæ€ï¼Œèšç„¦äºæ¨ç†ç­–ç•¥ç¼ºé™·
        
        Args:
            failed_trajectories: å¤±è´¥çš„è½¨è¿¹åˆ—è¡¨
            max_reflections: æœ€å¤šåæ€çš„æ ·æœ¬æ•°
            
        Returns:
            List[Dict]: æ–°ç”Ÿæˆçš„ç­–ç•¥è§„åˆ™
        """
        if not failed_trajectories:
            return []
        
        print(f"å¼€å§‹åæ€ {len(failed_trajectories)} ä¸ªå¤±è´¥æ ·æœ¬...")
        
        # é€‰æ‹©æœ€å¤šmax_reflectionsä¸ªæ ·æœ¬è¿›è¡Œåæ€
        selected_samples = failed_trajectories[:max_reflections]
        
        rule_candidates = []
        
        for i, traj in enumerate(selected_samples, start=1):
            print(f"åæ€æ ·æœ¬ {i}/{len(selected_samples)}: {traj['image_path']}")
            
            # è·å–World-Beliefä¸Šä¸‹æ–‡
            world_belief_context = self._get_world_belief_context(traj['label'])
            
            # æ„å»ºå€™é€‰ç±»åˆ«æ–‡æœ¬
            candidate_text = ""
            for j, (cat, score) in enumerate(traj['top_k_candidates'], start=1):
                candidate_text += f"{j}. {cat} (similarity: {score:.4f})\n"
            
            # æ„å»ºåæ€æç¤ºè¯
            reflection_prompt = f"""You are reviewing a failed fine-grained recognition case.
Candidate classes (highly likely to contain the correct option):
{candidate_text}
Your reasoning: "{traj['cot']}"
Prediction: {traj['prediction']}
Ground truth: {traj['label']}

Please reflect at the strategy level:
1. Correctness: Did your reasoning correctly interpret visual evidence?
2. Consistency: Was your step-by-step logic consistent with known class characteristics?
3. Rationality: Did you focus on the right discriminative parts? Did you ignore key details?
4. Generalization: What general lesson can be learned to avoid similar mistakes on other images?
Then, propose a domain-general rule of thumb that applies to future FGVR cases.

Return ONLY a JSON object:
{{
    "failure_pattern": "concise description of the challenging scenario",
    "root_cause": "brief diagnosis of the reasoning flaw",
    "improved_rule": "Actionable instruction starting with a verb",
    "applicability_signals": "When should this rule be recalled?",
    "priority": "high|medium|low"
}}"""
            
            # è°ƒç”¨MLLMè¿›è¡Œåæ€
            image = Image.open(traj['image_path']).convert("RGB")
            reply, response = self.mllm_bot.describe_attribute(image, reflection_prompt)
            if isinstance(response, list):
                response = " ".join(response)
            
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed = json.loads(json_str)
            else:
                parsed = {}
            
            improved_rule = parsed.get("improved_rule", "").strip()
            failure_pattern = parsed.get("failure_pattern", "").strip()
            applicability_signals = parsed.get("applicability_signals", "").strip()
            priority = parsed.get("priority", "medium").strip()
            
            candidate = {
                "rule": improved_rule,
                "failure_pattern": failure_pattern,
                "applicability_signals": applicability_signals or "similar ambiguous cases",
                "priority": priority or "medium",
                "support": 1,
                "source_images": [traj['image_path']],
                "labels": [traj['label']],
                "root_cause": parsed.get("root_cause", "").strip(),
                "source_reasoning": traj['cot']
            }
            rule_candidates.append(candidate)
        
        refined_rules = self._aggregate_rules(rule_candidates)
        return refined_rules
    
    def update_self_belief(self, refined_rules: List[Dict]) -> bool:
        """
        å°†æç‚¼çš„ç­–ç•¥æ”¹è¿›åˆå¹¶åˆ°Self-Beliefä¸­
        
        Args:
            refined_rules: æç‚¼åçš„ç­–ç•¥è§„åˆ™åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        if not refined_rules:
            print("è­¦å‘Š: ç­–ç•¥æ”¹è¿›ä¸ºç©ºï¼Œè·³è¿‡æ›´æ–°")
            return False
        
        updated = False
        for rule_entry in refined_rules:
            added = self._add_strategy_rule(rule_entry)
            updated = updated or added
            if added:
                print(f"æ–°å¢ç­–ç•¥è§„åˆ™: {rule_entry.get('rule', '').strip()}")
        
        if updated:
            self._refresh_self_belief_prompt()
            print("Self-Beliefå·²æ›´æ–°å¹¶åŒ…å«æœ€æ–°ç­–ç•¥è§„åˆ™")
        else:
            print("æœªæ·»åŠ æ–°ç­–ç•¥è§„åˆ™ï¼ŒSelf-Beliefä¿æŒä¸å˜")
        
        return updated
    
    def evaluate_with_current_belief(self,
                                     validation_samples: Dict[str, List[str]],
                                     top_k: int = 5) -> Dict:
        """
        ä½¿ç”¨å½“å‰Self-Beliefåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ€§èƒ½
        
        Args:
            validation_samples: {true_category: [image_paths]} éªŒè¯æ ·æœ¬
            top_k: æ£€ç´¢top-kç»“æœ
            
        Returns:
            Dict: è¯„ä¼°ç»“æœ {accuracy, correct_count, total_count, ...}
        """
        print("ä½¿ç”¨å½“å‰Self-Beliefè¯„ä¼°æ€§èƒ½...")
        
        correct_count = 0
        total_count = 0
        category_stats = defaultdict(lambda: {"correct": 0, "total": 0})
        
        for true_category, image_paths in validation_samples.items():
            world_belief_context = self._get_world_belief_context(true_category)
            
            for img_path in image_paths:
                # å¿«æ€è€ƒè·å–Top-K
                fast_result = self.fast_thinking.fast_thinking_pipeline(img_path, top_k)
                top_k_candidates = fast_result.get("fused_results", [])[:top_k]

                
                # ä½¿ç”¨å½“å‰Self-Beliefç”Ÿæˆé¢„æµ‹
                cot, prediction = self.generate_cot_with_self_belief(
                    img_path, top_k_candidates, world_belief_context
                )
                
                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
                is_correct = is_similar(prediction, true_category, threshold=0.4)
                print(f"evaluate_with_current_belief, é¢„æµ‹: {prediction}, çœŸå®: {true_category}, æ˜¯å¦æ­£ç¡®: {is_correct}")
                if is_correct:
                    correct_count += 1
                    category_stats[true_category]["correct"] += 1
                
                total_count += 1
                category_stats[true_category]["total"] += 1
        
        accuracy = correct_count / total_count if total_count > 0 else 0.0
        result = {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": total_count,
            "category_stats": dict(category_stats)
        }
        
        print(f"è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡ = {accuracy:.4f} ({correct_count}/{total_count})")
        
        return result
    
    def build_experience_base(self,
                              validation_samples: Dict[str, List[str]],
                              max_iterations: int = 3,
                              top_k: int = 5,
                              max_reflections_per_iter: int = 5,
                              min_improvement: float = 0.01,
                              max_samples_per_category: Optional[int] = None) -> Dict:
        """
        æ„å»ºç»éªŒåº“ï¼šè¿­ä»£ä¼˜åŒ–Self-Belief
        
        Args:
            validation_samples: {true_category: [image_paths]} éªŒè¯æ ·æœ¬
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            top_k: æ£€ç´¢top-kç»“æœ
            max_reflections_per_iter: æ¯æ¬¡è¿­ä»£æœ€å¤šåæ€çš„æ ·æœ¬æ•°
            min_improvement: æœ€å°æ€§èƒ½æå‡é˜ˆå€¼ï¼ˆå¦‚æœæå‡å°äºæ­¤å€¼ï¼Œåœæ­¢è¿­ä»£ï¼‰
            max_samples_per_category: æ¯ä¸ªç±»åˆ«æœ€å¤šå¤„ç†çš„æ ·æœ¬æ•°
            
        Returns:
            Dict: æ„å»ºç»“æœ
        """
        print("=" * 60)
        print("å¼€å§‹æ„å»ºç»éªŒåº“ï¼ˆæ¨¡å‹è‡ªåæ€ä¸ä¼˜åŒ–ï¼‰")
        print("=" * 60)
        
        # åˆå§‹åŒ–Self-Belief
        self.initialize_self_belief()
        
        # è®°å½•åˆå§‹æ€§èƒ½
        initial_performance = self.evaluate_with_current_belief(validation_samples, top_k)
        best_accuracy = initial_performance["accuracy"]
        best_self_belief = self.self_belief
        
        print(f"\nåˆå§‹å‡†ç¡®ç‡: {best_accuracy:.4f}")
        
        iteration_results = []
        
        for iteration in range(1, max_iterations + 1):
            print(f"\n{'=' * 60}")
            print(f"è¿­ä»£ {iteration}/{max_iterations}")
            print(f"{'=' * 60}")
            
            # 1. æ„é€ ä¼ªè½¨è¿¹
            trajectories = self.build_pseudo_trajectories(
                validation_samples, top_k, max_samples_per_category
            )
            
            # 2. è¯†åˆ«å¤±è´¥æ ·æœ¬
            failed_trajectories = [t for t in trajectories if not t['is_correct']]
            
            if not failed_trajectories:
                print("æ‰€æœ‰æ ·æœ¬éƒ½é¢„æµ‹æ­£ç¡®ï¼Œæ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼")
                break
            
            print(f"å¤±è´¥æ ·æœ¬æ•°: {len(failed_trajectories)}")
            
            # 3. å¯¹å¤±è´¥æ ·æœ¬è¿›è¡Œåæ€
            refined_rules = self.reflect_on_failed_samples(
                failed_trajectories, max_reflections_per_iter
            )
            
            if not refined_rules:
                print("æœªèƒ½æç‚¼å‡ºç­–ç•¥æ”¹è¿›ï¼Œè·³è¿‡æœ¬æ¬¡è¿­ä»£")
                continue
            else:
                print(f"æç‚¼å‡º {len(refined_rules)} æ¡ç­–ç•¥æ”¹è¿›")
            # 4. æ›´æ–°Self-Belief
            updated = self.update_self_belief(refined_rules)
            if not updated:
                print("ç­–ç•¥è§„åˆ™æœªå‘ç”Ÿå˜åŒ–ï¼Œç»§ç»­ä¸‹ä¸€è½®è¿­ä»£")
                continue
            
            # 5. åœ¨éªŒè¯é›†ä¸Šé‡æ–°è¯„ä¼°
            new_performance = self.evaluate_with_current_belief(validation_samples, top_k)
            new_accuracy = new_performance["accuracy"]
            
            performance_change = new_accuracy - best_accuracy
            
            print(f"\næ€§èƒ½å˜åŒ–: {best_accuracy:.4f} â†’ {new_accuracy:.4f} (å˜åŒ–: {performance_change:+.4f})")
            
            # 6. åˆ¤æ–­æ˜¯å¦ä¿ç•™æ›´æ–°
            if performance_change >= min_improvement:
                print(f"æ€§èƒ½æå‡ {performance_change:.4f} >= {min_improvement:.4f}ï¼Œä¿ç•™ç­–ç•¥æ›´æ–°")
                best_accuracy = new_accuracy
                best_self_belief = self.self_belief
            else:
                print(f"æ€§èƒ½æå‡ {performance_change:.4f} < {min_improvement:.4f}ï¼Œå›é€€ç­–ç•¥æ›´æ–°")
                # å›é€€Self-Belief
                self.self_belief = best_self_belief
            
            # è®°å½•è¿­ä»£ç»“æœ
            iteration_result = {
                "iteration": iteration,
                "failed_samples_count": len(failed_trajectories),
                "refined_rules": [rule.get("rule", "") for rule in refined_rules],
                "accuracy_before": best_accuracy - performance_change if performance_change >= min_improvement else best_accuracy,
                "accuracy_after": new_accuracy,
                "performance_change": performance_change,
                "strategy_accepted": performance_change >= min_improvement
            }
            iteration_results.append(iteration_result)
            
            # å¦‚æœæ€§èƒ½ä¸å†æå‡ï¼Œæå‰åœæ­¢
            if performance_change < min_improvement and iteration > 1:
                print("æ€§èƒ½ä¸å†æå‡ï¼Œæå‰åœæ­¢è¿­ä»£")
                break
        
        # ä½¿ç”¨æœ€ä½³Self-Belief
        self.self_belief = best_self_belief
        
        # æœ€ç»ˆè¯„ä¼°
        final_performance = self.evaluate_with_current_belief(validation_samples, top_k)
        
        result = {
            "initial_accuracy": initial_performance["accuracy"],
            "final_accuracy": final_performance["accuracy"],
            "improvement": final_performance["accuracy"] - initial_performance["accuracy"],
            "best_self_belief": best_self_belief,
            "iteration_results": iteration_results,
            "total_iterations": len(iteration_results)
        }
        
        print(f"\n{'=' * 60}")
        print("ç»éªŒåº“æ„å»ºå®Œæˆ!")
        print(f"åˆå§‹å‡†ç¡®ç‡: {initial_performance['accuracy']:.4f}")
        print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_performance['accuracy']:.4f}")
        print(f"æ€§èƒ½æå‡: {result['improvement']:+.4f}")
        print(f"{'=' * 60}")
        
        return result
    
    def save_experience_base(self, save_dir: str):
        """
        ä¿å­˜ç»éªŒåº“åˆ°æ–‡ä»¶
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        self.save_dir = save_dir
        
        # ä¿å­˜Self-Belief
        belief_path = os.path.join(save_dir, "self_belief.txt")
        with open(belief_path, 'w', encoding='utf-8') as f:
            f.write(self.self_belief)
        
        # ä¿å­˜ä¼ªè½¨è¿¹
        trajectories_path = os.path.join(save_dir, "pseudo_trajectories.json")
        dump_json(trajectories_path, self.pseudo_trajectories)
        
        # ä¿å­˜åæ€å†å²
        history_path = os.path.join(save_dir, "reflection_history.json")
        dump_json(history_path, self.reflection_history)
        
        # ä¿å­˜ç­–ç•¥è§„åˆ™
        rules_path = os.path.join(save_dir, "strategy_rules.json")
        dump_json(rules_path, self.strategy_rules)
        
        print(f"ç»éªŒåº“å·²ä¿å­˜åˆ°: {save_dir}")
    
    def load_experience_base(self, load_dir: str):
        """
        ä»æ–‡ä»¶åŠ è½½ç»éªŒåº“
        
        Args:
            load_dir: åŠ è½½ç›®å½•
        """
        self.save_dir = load_dir
        
        # åŠ è½½Self-Belief
        belief_path = os.path.join(load_dir, "self_belief.txt")
        if os.path.exists(belief_path):
            with open(belief_path, 'r', encoding='utf-8') as f:
                self.self_belief = f.read()
            # print(f"Self-Beliefå·²ä» {belief_path} åŠ è½½")
            print(f'åŠ è½½self_belief:{self.self_belief}')
        
        # # åŠ è½½ä¼ªè½¨è¿¹
        # trajectories_path = os.path.join(load_dir, "pseudo_trajectories.json")
        # if os.path.exists(trajectories_path):
        #     self.pseudo_trajectories = load_json(trajectories_path)
        #     print(f"ä¼ªè½¨è¿¹å·²ä» {trajectories_path} åŠ è½½ ({len(self.pseudo_trajectories)} æ¡)")
        
        # # åŠ è½½åæ€å†å²
        # history_path = os.path.join(load_dir, "reflection_history.json")
        # if os.path.exists(history_path):
        #     self.reflection_history = load_json(history_path)
        #     print(f"åæ€å†å²å·²ä» {history_path} åŠ è½½ ({len(self.reflection_history)} æ¡è®°å½•)")
        
        # åŠ è½½ç­–ç•¥è§„åˆ™
        # rules_path = os.path.join(load_dir, "strategy_rules.json")
        # if os.path.exists(rules_path):
        #     self.strategy_rules = load_json(rules_path)
        #     print(f"ç­–ç•¥è§„åˆ™å·²ä» {rules_path} åŠ è½½ ({len(self.strategy_rules)} æ¡)")
        #     print(f'åŠ è½½çš„strategy_rules:{self.strategy_rules}')
        # else:
        #     self.strategy_rules = []
        # if self.strategy_rules:
        #     last_numeric_id = max(int(rule["id"].lstrip("R")) for rule in self.strategy_rules if "id" in rule)
        #     self.next_rule_id = last_numeric_id + 1
        # else:
        #     self.next_rule_id = 1
        
        # åˆ·æ–°Self-Belief æš‚æ—¶ä¸ç”¨
        # self.self_belief = self._compose_self_belief_prompt()
        
        print(f"ç»éªŒåº“å·²ä» {load_dir} åŠ è½½å®Œæˆ")
    
    def get_self_belief(self) -> str:
        """è·å–å½“å‰Self-Belief"""
        return self.self_belief
    
    def get_pseudo_trajectories(self) -> List[Dict]:
        """è·å–ä¼ªè½¨è¿¹åˆ—è¡¨"""
        return self.pseudo_trajectories


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    from agents.mllm_bot import MLLMBot
    from knowledge_base_builder import KnowledgeBaseBuilder
    from fast_thinking_optimized import FastThinkingOptimized
    from slow_thinking_optimized import SlowThinkingOptimized
    
    # åˆå§‹åŒ–ç»„ä»¶
    mllm_bot = MLLMBot(model_tag="Qwen2.5-VL-7B", model_name="Qwen2.5-VL-7B", device="cuda")
    kb_builder = KnowledgeBaseBuilder(device="cuda")
    fast_thinking = FastThinkingOptimized(kb_builder, device="cuda")
    slow_thinking = SlowThinkingOptimized(mllm_bot, kb_builder, fast_thinking, device="cuda")
    
    # åˆå§‹åŒ–ç»éªŒåº“æ„å»ºå™¨
    exp_builder = ExperienceBaseBuilder(
        mllm_bot=mllm_bot,
        knowledge_base_builder=kb_builder,
        fast_thinking_module=fast_thinking,
        slow_thinking_module=slow_thinking,
        device="cuda"
    )
    
    # ç¤ºä¾‹éªŒè¯æ ·æœ¬
    validation_samples = {
        "Chihuahua": ["path/to/chihuahua1.jpg", "path/to/chihuahua2.jpg"],
        "Shiba Inu": ["path/to/shiba1.jpg", "path/to/shiba2.jpg"]
    }
    
    # æ„å»ºç»éªŒåº“
    result = exp_builder.build_experience_base(validation_samples, max_iterations=1)
    
    # ä¿å­˜ç»éªŒåº“
    exp_builder.save_experience_base("./experience_base")
    
    print("ç»éªŒåº“æ„å»ºå®Œæˆ!")

