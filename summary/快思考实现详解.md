# å¿«æ€è€ƒå®ç°è¯¦è§£

## æ¦‚è¿°

å¿«æ€è€ƒæ˜¯ç³»ç»Ÿä¸­å¤„ç†ç®€å•æ ·æœ¬çš„é«˜æ•ˆæ¨¡å—ï¼Œä¸»è¦åŸºäº**CLIPæ¨¡å‹**å®ç°åŒæ¨¡æ€æ£€ç´¢ï¼Œ**ä¸æ˜¯ç›´æ¥åˆ†ç±»ï¼Œè€Œæ˜¯é€šè¿‡æ£€ç´¢é¢„æ„å»ºçš„çŸ¥è¯†åº“**æ¥æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç±»åˆ«ï¼Œç„¶åé€šè¿‡æ™ºèƒ½èåˆç­–ç•¥å’Œè§¦å‘æœºåˆ¶å®ç°å¿«é€Ÿå‡†ç¡®çš„åˆ†ç±»ã€‚

## â— é‡è¦æ¾„æ¸…ï¼šæ£€ç´¢ vs ç›´æ¥åˆ†ç±»

**å¿«æ€è€ƒé‡‡ç”¨çš„æ˜¯"æ£€ç´¢å¼"åˆ†ç±»ï¼Œè€Œé"ç›´æ¥åˆ†ç±»"**ï¼š

### ğŸ” æ£€ç´¢å¼åˆ†ç±»æµç¨‹
```
æŸ¥è¯¢å›¾åƒ â†’ CLIPç‰¹å¾æå– â†’ ä¸çŸ¥è¯†åº“æ£€ç´¢åŒ¹é… â†’ è¿”å›æœ€ç›¸ä¼¼ç±»åˆ« â†’ èåˆå†³ç­–
```

### âŒ ä¸æ˜¯ç›´æ¥åˆ†ç±»
```
æŸ¥è¯¢å›¾åƒ â†’ CLIP â†’ ç›´æ¥è¾“å‡ºç±»åˆ«æ¦‚ç‡ âœ—
```

**ä¸ºä»€ä¹ˆä½¿ç”¨æ£€ç´¢ï¼Ÿ**
1. **çµæ´»æ€§**ï¼šå¯ä»¥åŠ¨æ€æ·»åŠ æ–°ç±»åˆ«åˆ°çŸ¥è¯†åº“
2. **å¯è§£é‡Šæ€§**ï¼šå¯ä»¥çœ‹åˆ°å…·ä½“åŒ¹é…çš„å‚è€ƒæ ·æœ¬
3. **Fine-tuningå…è´¹**ï¼šä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹
4. **å°‘æ ·æœ¬å­¦ä¹ **ï¼šæ¯ä¸ªç±»åˆ«åªéœ€è¦å°‘é‡æ ·æœ¬æ„å»ºæ¨¡æ¿

## ğŸ”§ ä½¿ç”¨çš„æ ¸å¿ƒæ¨¡å‹

### 1. ä¸»è¦æ¨¡å‹ï¼šCLIP (Contrastive Language-Image Pre-training)

**æ¨¡å‹é…ç½®**ï¼š
```python
# é»˜è®¤ä½¿ç”¨CLIP ViT-B/32æ¨¡å‹
image_encoder_name = "/home/Dataset/Models/Clip/clip-vit-base-patch32"
text_encoder_name = "/home/Dataset/Models/Clip/clip-vit-base-patch32"

# æ¨¡å‹åˆå§‹åŒ–
self.clip_model = CLIPModel.from_pretrained(image_encoder_name, local_files_only=True)
self.clip_processor = CLIPProcessor.from_pretrained(image_encoder_name, local_files_only=True)
```

**CLIPæ¨¡å‹çš„å…³é”®èƒ½åŠ›**ï¼š
- **å›¾åƒç¼–ç **ï¼šå°†å›¾åƒè½¬æ¢ä¸º512ç»´ç‰¹å¾å‘é‡
- **æ–‡æœ¬ç¼–ç **ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸º512ç»´ç‰¹å¾å‘é‡  
- **è·¨æ¨¡æ€åŒ¹é…**ï¼šå›¾åƒç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾åœ¨åŒä¸€è¯­ä¹‰ç©ºé—´ä¸­ï¼Œå¯ç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦

### 2. è¾…åŠ©æ¨¡å‹ï¼šBLIP (å¯é€‰)

**ç”¨é€”**ï¼šç”¨äºé«˜çº§ç‰¹å¾èåˆï¼ˆcross-attentionæ¨¡å¼ï¼‰
```python
# BLIPæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
blip_load_path = '/home/Dataset/Models/blip/blip-itm-base-coco'
self.blip_model = BlipForImageTextRetrieval.from_pretrained(blip_load_path)
```

## ğŸš€ å¿«æ€è€ƒå®Œæ•´æ£€ç´¢æµç¨‹

### å‰æï¼šé¢„æ„å»ºçŸ¥è¯†åº“

åœ¨å¿«æ€è€ƒå¼€å§‹å‰ï¼Œç³»ç»Ÿå·²ç»é¢„æ„å»ºäº†ä¸¤ä¸ªçŸ¥è¯†åº“ï¼š

```python
# å›¾åƒçŸ¥è¯†åº“ï¼š{ç±»åˆ«å: CLIPå›¾åƒç‰¹å¾å‘é‡}
image_knowledge_base = {
    "Chihuahua": [0.12, 0.34, ...],      # 512ç»´CLIPå›¾åƒç‰¹å¾
    "German Shepherd": [0.67, 0.23, ...], 
    "Golden Retriever": [0.45, 0.78, ...],
    ...
}

# æ–‡æœ¬çŸ¥è¯†åº“ï¼š{ç±»åˆ«å: CLIPæ–‡æœ¬ç‰¹å¾å‘é‡}  
text_knowledge_base = {
    "Chihuahua": [0.23, 0.56, ...],      # 512ç»´CLIPæ–‡æœ¬ç‰¹å¾
    "German Shepherd": [0.89, 0.12, ...],
    "Golden Retriever": [0.34, 0.67, ...], 
    ...
}
```

### é˜¶æ®µ1ï¼šåŒæ¨¡æ€æ£€ç´¢ï¼ˆæ ¸å¿ƒè¿‡ç¨‹ï¼‰

#### 1.1 å›¾åƒ-å›¾åƒæ£€ç´¢ (Image-to-Image Retrieval)

**æ£€ç´¢åŸç†**ï¼šæŸ¥è¯¢å›¾åƒçš„è§†è§‰ç‰¹å¾ vs çŸ¥è¯†åº“ä¸­å„ç±»åˆ«çš„è§†è§‰ç‰¹å¾æ¨¡æ¿

```python
def image_to_image_retrieval(self, query_image_path: str, top_k: int = 5):
    # æ­¥éª¤1: æå–æŸ¥è¯¢å›¾åƒçš„CLIPç‰¹å¾
    query_img_feat = CLIP_image_encode(query_image)  # 512ç»´å‘é‡
    
    # æ­¥éª¤2: åœ¨å›¾åƒçŸ¥è¯†åº“ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„ç±»åˆ«
    similarities = []
    for category, template_img_feat in image_knowledge_base.items():
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå› ä¸ºéƒ½åšäº†L2å½’ä¸€åŒ–ï¼Œç­‰ä»·äºç‚¹ç§¯ï¼‰
        sim = np.dot(query_img_feat, template_img_feat)  
        similarities.append((category, sim))
    
    # æ­¥éª¤3: æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›Top-Kæœ€ç›¸ä¼¼ç±»åˆ«
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]  # ä¾‹å¦‚ï¼š[("Chihuahua", 0.87), ("Poodle", 0.73), ...]
```

**å®é™…ä»£ç å®ç°**ï¼š
```python
# è°ƒç”¨çŸ¥è¯†åº“æ„å»ºå™¨çš„å›¾åƒæ£€ç´¢æ¥å£
img_category, img_confidence, img_results = self.kb_builder.image_retrieval(query_image_path, top_k)
```

#### 1.2 å›¾åƒ-æ–‡æœ¬æ£€ç´¢ (Image-to-Text Retrieval) 

**æ£€ç´¢åŸç†**ï¼šæŸ¥è¯¢å›¾åƒçš„è§†è§‰ç‰¹å¾ vs çŸ¥è¯†åº“ä¸­å„ç±»åˆ«çš„æ–‡æœ¬æè¿°ç‰¹å¾ï¼ˆè·¨æ¨¡æ€æ£€ç´¢ï¼‰

```python
def image_to_text_retrieval(self, query_image_path: str, top_k: int = 5):
    # æ­¥éª¤1: æå–æŸ¥è¯¢å›¾åƒçš„CLIPç‰¹å¾
    query_img_feat = CLIP_image_encode(query_image)  # 512ç»´å‘é‡
    
    # æ­¥éª¤2: åœ¨æ–‡æœ¬çŸ¥è¯†åº“ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„ç±»åˆ«ï¼ˆè·¨æ¨¡æ€åŒ¹é…ï¼‰
    similarities = []  
    for category, template_text_feat in text_knowledge_base.items():
        # CLIPçš„è·¨æ¨¡æ€èƒ½åŠ›ï¼šå›¾åƒç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾ç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦
        sim = np.dot(query_img_feat, template_text_feat)
        similarities.append((category, sim))
    
    # æ­¥éª¤3: æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›Top-Kæœ€ç›¸ä¼¼ç±»åˆ«
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]  # ä¾‹å¦‚ï¼š[("German Shepherd", 0.91), ("Husky", 0.68), ...]
```

**å®é™…ä»£ç å®ç°**ï¼š
```python
# æå–æŸ¥è¯¢å›¾åƒç‰¹å¾
query_img_feat = self.kb_builder.retrieval.extract_image_feat(query_image_path)

# ä¸æ–‡æœ¬çŸ¥è¯†åº“ä¸­æ¯ä¸ªç±»åˆ«è¿›è¡Œè·¨æ¨¡æ€ç›¸ä¼¼åº¦è®¡ç®—
similarities = []
for category, text_feat in self.kb_builder.text_knowledge_base.items():
    sim = np.dot(query_img_feat, text_feat)  # è·¨æ¨¡æ€ç›¸ä¼¼åº¦
    similarities.append((category, sim))

# è¿”å›Top-Kç»“æœ
text_category, text_confidence, text_results = process_results(similarities, top_k)
```

### ğŸ” æ£€ç´¢è¿‡ç¨‹è¯¦è§£

#### æ£€ç´¢ vs åˆ†ç±»çš„åŒºåˆ«

**æ£€ç´¢å¼ï¼ˆå¿«æ€è€ƒé‡‡ç”¨ï¼‰**ï¼š
```python
# 1. é¢„æ„å»ºçŸ¥è¯†åº“æ¨¡æ¿
templates = build_knowledge_base(train_samples)

# 2. æŸ¥è¯¢æ—¶æ£€ç´¢æœ€ç›¸ä¼¼æ¨¡æ¿
query_feat = CLIP_encode(query_image)
similarities = compute_similarity(query_feat, templates)
top_matches = get_topk(similarities)  # [("Chihuahua", 0.87), ...]
```

**ç›´æ¥åˆ†ç±»å¼ï¼ˆå¿«æ€è€ƒæ²¡é‡‡ç”¨ï¼‰**ï¼š
```python  
# ç›´æ¥è¾“å‡ºç±»åˆ«æ¦‚ç‡
probs = CLIP_classifier(query_image)  # [0.1, 0.8, 0.05, ...]
predicted_class = argmax(probs)
```

#### ä¸ºä»€ä¹ˆé€‰æ‹©æ£€ç´¢å¼ï¼Ÿ

1. **åŠ¨æ€æ‰©å±•**ï¼šæ–°å¢ç±»åˆ«åªéœ€æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼Œæ— éœ€é‡è®­ç»ƒ
2. **å°‘æ ·æœ¬å‹å¥½**ï¼šæ¯ç±»åªéœ€å‡ ä¸ªæ ·æœ¬å°±èƒ½æ„å»ºæœ‰æ•ˆæ¨¡æ¿  
3. **å¯è§£é‡Šæ€§**ï¼šèƒ½çœ‹åˆ°åŒ¹é…çš„å…·ä½“å‚è€ƒæ ·æœ¬
4. **ç»†ç²’åº¦é€‚åº”**ï¼šç‰¹åˆ«é€‚åˆç›¸ä¼¼ç±»åˆ«çš„ç²¾ç»†åŒºåˆ†

### é˜¶æ®µ2ï¼šæ™ºèƒ½èåˆç­–ç•¥ - **åˆ†ç±»ä¾æ®æ ¸å¿ƒ**

#### ğŸ¯ å¿«æ€è€ƒçš„åˆ†ç±»ä¾æ®

å¿«æ€è€ƒçš„åˆ†ç±»ä¾æ®æ˜¯é€šè¿‡**å¤šé‡èåˆç­–ç•¥**ç»¼åˆåŒæ¨¡æ€æ£€ç´¢ç»“æœï¼š

```
åˆ†ç±»ä¾æ® = æ¦‚ç‡èåˆåˆ†æ•° + RRFæ’åºåˆ†æ•° + æƒé‡è°ƒèŠ‚
```

#### 2.1 åŒæ¨¡æ€TOP-Kç»“æœå¤„ç†æµç¨‹

**è¾“å…¥ç¤ºä¾‹**ï¼š
```python
# å›¾åƒ-å›¾åƒæ£€ç´¢ç»“æœï¼ˆTOP-5ï¼‰
img_results = [
    ("Chihuahua", 0.87),
    ("Poodle", 0.73), 
    ("Husky", 0.69),
    ("Beagle", 0.65),
    ("Corgi", 0.61)
]

# å›¾åƒ-æ–‡æœ¬æ£€ç´¢ç»“æœï¼ˆTOP-5ï¼‰
text_results = [
    ("German Shepherd", 0.91),
    ("Chihuahua", 0.84),
    ("Husky", 0.68),
    ("Golden Retriever", 0.64),
    ("Labrador", 0.59)
]
```

#### 2.2 æ¦‚ç‡è½¬æ¢å¤„ç†

**æ­¥éª¤1: æ¸©åº¦ç¼©æ”¾Softmaxè½¬æ¢**
```python
def _to_probs(self, results: List[Tuple[str, float]]) -> Dict[str, float]:
    # 1. èšåˆåŒç±»åˆ†æ•°ï¼ˆå¦‚æœåŒä¸€ç±»åˆ«å‡ºç°å¤šæ¬¡ï¼Œå–æœ€å¤§å€¼ï¼‰
    class_to_score = {}
    for cname, score in results:
        class_to_score[cname] = max(class_to_score.get(cname, score), score)
    
    # 2. æ¸©åº¦ç¼©æ”¾Softmaxï¼ˆæ¸©åº¦=0.07ï¼Œå¢å¼ºåŒºåˆ†åº¦ï¼‰
    scores = np.array(list(class_to_score.values()))
    scores = scores / 0.07  # ä½æ¸©åº¦ä½¿åˆ†å¸ƒæ›´å°–é”
    scores = scores - scores.max()  # æ•°å€¼ç¨³å®šæ€§
    exps = np.exp(scores)
    probs = exps / (exps.sum() + 1e-12)
    
    return {category: prob for category, prob in zip(class_to_score.keys(), probs)}

# ç»“æœç¤ºä¾‹ï¼š
img_probs = {
    "Chihuahua": 0.45,  # åŸ0.87 -> ç»è¿‡softmaxå
    "Poodle": 0.23,     # åŸ0.73 -> ç»è¿‡softmaxå  
    "Husky": 0.18,      # åŸ0.69 -> ç»è¿‡softmaxå
    ...
}
```

#### 2.3 RRFæ’åºåˆ†æ•°è®¡ç®—

**æ­¥éª¤2: å€’æ•°æ’åºèåˆ(RRF)**
```python
def _rrf(self, results: List[Tuple[str, float]], k: int = 60) -> Dict[str, float]:
    """
    RRFåˆ†æ•° = 1 / (k + rank)
    æ’åè¶Šé å‰ï¼ŒRRFåˆ†æ•°è¶Šé«˜
    """
    rrf = {}
    for rank, (cname, _) in enumerate(results, start=1):
        rrf[cname] = rrf.get(cname, 0.0) + 1.0 / (60 + rank)
    return rrf

# ç»“æœç¤ºä¾‹ï¼š
img_rrf = {
    "Chihuahua": 1/61 â‰ˆ 0.0164,  # æ’å1
    "Poodle": 1/62 â‰ˆ 0.0161,     # æ’å2
    "Husky": 1/63 â‰ˆ 0.0159,      # æ’å3
    ...
}
```

#### 2.4 æœ€ç»ˆèåˆåˆ†æ•°è®¡ç®—

**æ­¥éª¤3: åŠ æƒèåˆç­–ç•¥**
```python
def fuse_results(self, img_results, text_results):
    # è·å–æ¦‚ç‡å’ŒRRFåˆ†æ•°
    img_probs = self._to_probs(img_results)
    text_probs = self._to_probs(text_results) 
    img_rrf = self._rrf(img_results)
    text_rrf = self._rrf(text_results)
    
    # æ”¶é›†æ‰€æœ‰å€™é€‰ç±»åˆ«
    all_categories = set(img_probs.keys()) | set(text_probs.keys()) | set(img_rrf.keys()) | set(text_rrf.keys())
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æœ€ç»ˆèåˆåˆ†æ•°
    fused = []
    for category in all_categories:
        # è·å–å„é¡¹åˆ†æ•°ï¼ˆå¦‚æœæŸæ¨¡æ€ä¸­æ²¡æœ‰è¯¥ç±»åˆ«ï¼Œåˆ†æ•°ä¸º0ï¼‰
        p_img = img_probs.get(category, 0.0)     # å›¾åƒæ¦‚ç‡åˆ†æ•°
        p_txt = text_probs.get(category, 0.0)    # æ–‡æœ¬æ¦‚ç‡åˆ†æ•°  
        rrf_img = img_rrf.get(category, 0.0)     # å›¾åƒRRFåˆ†æ•°
        rrf_txt = text_rrf.get(category, 0.0)    # æ–‡æœ¬RRFåˆ†æ•°
        
        # åŠ æƒèåˆå…¬å¼ï¼ˆæ ¸å¿ƒåˆ†ç±»ä¾æ®ï¼‰
        final_score = (
            0.05 * p_img +           # å›¾åƒæ¦‚ç‡æƒé‡5%
            0.95 * p_txt +           # æ–‡æœ¬æ¦‚ç‡æƒé‡95% (è·¨æ¨¡æ€æ›´é‡è¦)
            0.1 * (rrf_img + rrf_txt) # RRFè¾…åŠ©æƒé‡10%
        )
        
        fused.append((category, final_score))
    
    # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œå¾—åˆ°èåˆæ’åº
    fused.sort(key=lambda x: x[1], reverse=True)
    return fused

# æœ€ç»ˆèåˆç»“æœç¤ºä¾‹ï¼š
fused_results = [
    ("Chihuahua", 0.923),      # æœ€é«˜èåˆåˆ†æ•°
    ("German Shepherd", 0.874), # ç¬¬äºŒé«˜åˆ†æ•°
    ("Husky", 0.756),          # ç¬¬ä¸‰é«˜åˆ†æ•°
    ...
]
```

#### 2.5 æœ€ä½³ç±»åˆ«é€‰æ‹©æœºåˆ¶

**æ­¥éª¤4: é€‰æ‹©èåˆTop-1ä½œä¸ºé¢„æµ‹ç±»åˆ«**
```python
def select_best_category(self, fused_results):
    """
    é€‰æ‹©æœ€ä½³ç±»åˆ«ï¼šç›´æ¥å–èåˆåæ’åºçš„ç¬¬ä¸€å
    """
    if not fused_results:
        return "unknown", 0.0
    
    # èåˆç»“æœå·²ç»æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
    best_category, best_score = fused_results[0]
    
    return best_category, best_score

# åœ¨fast_thinking_pipelineä¸­çš„å®ç°ï¼š
fused_results = self.fuse_results(img_results, text_results)
fused_top1 = fused_results[0][0] if fused_results else "unknown"  # æœ€ä½³ç±»åˆ«
```

**æ­¥éª¤5: è®¡ç®—ç½®ä¿¡åº¦å’Œè¾¹é™…å·®å¼‚**
```python
def calculate_confidence_metrics(self, fused_results):
    """
    è®¡ç®—æœ€ç»ˆçš„ç½®ä¿¡åº¦æŒ‡æ ‡ï¼Œç”¨äºè§¦å‘åˆ¤æ–­
    """
    if len(fused_results) < 1:
        return 0.0, 0.0
    
    # å°†èåˆåˆ†æ•°å†æ¬¡è¿›è¡ŒSoftmaxå½’ä¸€åŒ–
    fused_scores = np.array([score for _, score in fused_results])
    fused_scaled = fused_scores / self.softmax_temp  # æ¸©åº¦ç¼©æ”¾
    fused_scaled = fused_scaled - fused_scaled.max()  # æ•°å€¼ç¨³å®š
    fused_exps = np.exp(fused_scaled)
    fused_probs = fused_exps / (fused_exps.sum() + 1e-12)
    
    # Top-1ç½®ä¿¡åº¦
    fused_top1_prob = float(fused_probs[0])
    
    # è¾¹é™…å·®å¼‚ï¼ˆTop-1 vs Top-2çš„æ¦‚ç‡å·®ï¼‰
    fused_margin = float(fused_probs[0] - fused_probs[1]) if len(fused_probs) > 1 else fused_top1_prob
    
    return fused_top1_prob, fused_margin

# å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼š
fused_top1_prob = 0.72   # Top-1çš„æœ€ç»ˆæ¦‚ç‡
fused_margin = 0.15      # ä¸Top-2çš„å·®è·
```

### ğŸ† æœ€ä½³ç±»åˆ«é€‰æ‹©æ€»ç»“

#### é€‰æ‹©ä¾æ®ä¼˜å…ˆçº§ï¼š

1. **ä¸»è¦ä¾æ®**: èåˆåˆ†æ•°æœ€é«˜çš„ç±»åˆ«
   ```python
   final_score = 0.05 * img_prob + 0.95 * text_prob + 0.1 * (img_rrf + text_rrf)
   best_category = argmax(final_score)
   ```

2. **æƒé‡åˆ†é…åŸç†**:
   - **è·¨æ¨¡æ€æƒé‡95%**: å›¾åƒ-æ–‡æœ¬æ£€ç´¢æ›´é‡è¦ï¼ˆè¯­ä¹‰ç†è§£ï¼‰
   - **çº¯è§†è§‰æƒé‡5%**: å›¾åƒ-å›¾åƒæ£€ç´¢ä½œä¸ºè¡¥å……ï¼ˆè§†è§‰ç›¸ä¼¼æ€§ï¼‰
   - **æ’åºæƒé‡10%**: RRFåˆ†æ•°æä¾›æ’åºç¨³å®šæ€§

3. **ç±»åˆ«åˆå¹¶ç­–ç•¥**:
   - æ”¶é›†ä¸¤ä¸ªæ¨¡æ€çš„æ‰€æœ‰å€™é€‰ç±»åˆ«
   - å¯¹äºåªåœ¨ä¸€ä¸ªæ¨¡æ€ä¸­å‡ºç°çš„ç±»åˆ«ï¼Œå¦ä¸€æ¨¡æ€åˆ†æ•°ä¸º0
   - å¯¹äºä¸¤ä¸ªæ¨¡æ€éƒ½å‡ºç°çš„ç±»åˆ«ï¼Œè¿›è¡ŒåŠ æƒèåˆ

#### å…·ä½“è®¡ç®—ç¤ºä¾‹ï¼š

```python
# å‡è®¾æŸ¥è¯¢ä¸€åªå‰å¨ƒå¨ƒ
img_results = [("Chihuahua", 0.87), ("Poodle", 0.73), ...]
text_results = [("Chihuahua", 0.84), ("German Shepherd", 0.91), ...]

# ç»è¿‡æ¦‚ç‡è½¬æ¢åï¼š
img_probs = {"Chihuahua": 0.45, "Poodle": 0.23, ...}
text_probs = {"Chihuahua": 0.38, "German Shepherd": 0.52, ...}

# ç»è¿‡RRFè½¬æ¢åï¼š
img_rrf = {"Chihuahua": 0.0164, "Poodle": 0.0161, ...}
text_rrf = {"Chihuahua": 0.0161, "German Shepherd": 0.0164, ...}

# æœ€ç»ˆèåˆåˆ†æ•°ï¼š
chihuahua_score = 0.05 * 0.45 + 0.95 * 0.38 + 0.1 * (0.0164 + 0.0161)
                = 0.0225 + 0.361 + 0.00325
                = 0.38675

german_shepherd_score = 0.05 * 0.0 + 0.95 * 0.52 + 0.1 * (0.0 + 0.0164)
                      = 0.0 + 0.494 + 0.00164
                      = 0.49564

# German Shepherdè·èƒœï¼Œæˆä¸ºé¢„æµ‹ç±»åˆ«
```

#### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

1. **è·¨æ¨¡æ€æ£€ç´¢æƒé‡é«˜(95%)**ï¼š
   - CLIPçš„è·¨æ¨¡æ€èƒ½åŠ›æ˜¯æ ¸å¿ƒä¼˜åŠ¿
   - æ–‡æœ¬æè¿°åŒ…å«æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯
   - å¯¹ç»†ç²’åº¦åˆ†ç±»æ›´æœ‰åˆ¤åˆ«åŠ›

2. **RRFè¾…åŠ©ç¨³å®šæ€§(10%)**ï¼š
   - é¿å…å•çº¯ä¾èµ–ç›¸ä¼¼åº¦åˆ†æ•°
   - è€ƒè™‘æ’åºä¿¡æ¯çš„ç¨³å®šæ€§
   - å¯¹ç›¸ä¼¼åº¦åˆ†æ•°çš„å™ªå£°æ›´é²æ£’

3. **æ¦‚ç‡å½’ä¸€åŒ–**ï¼š
   - æ¸©åº¦ç¼©æ”¾(0.07)å¢å¼ºåŒºåˆ†åº¦
   - Softmaxç¡®ä¿æ¦‚ç‡åˆ†å¸ƒçš„åˆç†æ€§
   - ä¾¿äºåç»­çš„ç½®ä¿¡åº¦è®¡ç®—

#### 2.6 ç½®ä¿¡åº¦è¯„ä¼°ä¸æœ€ç»ˆå†³ç­–

**æ­¥éª¤6: å¤šç»´åº¦ç½®ä¿¡åº¦è®¡ç®—**
```python
def evaluate_prediction_confidence(self, fused_results, img_results, text_results):
    """
    å¿«æ€è€ƒé€šè¿‡å¤šä¸ªç»´åº¦è¯„ä¼°é¢„æµ‹çš„å¯é æ€§
    """
    # 1. èåˆç½®ä¿¡åº¦ï¼šTop-1åœ¨æœ€ç»ˆæ’åºä¸­çš„æ¦‚ç‡
    fused_top1_prob = calculate_softmax_probability(fused_results[0])
    
    # 2. è¾¹é™…å·®å¼‚ï¼šTop-1ä¸Top-2çš„æ¦‚ç‡å·®è·
    fused_margin = fused_probs[0] - fused_probs[1]
    
    # 3. æ¨¡æ€ä¸€è‡´æ€§ï¼šä¸¤ä¸ªæ¨¡æ€çš„Top-1æ˜¯å¦ç›¸ä¼¼
    img_top1 = img_results[0][0]
    text_top1 = text_results[0][0] 
    modality_consistent = is_similar(img_top1, text_top1, threshold=0.7)
    
    # 4. Top-Ké‡å åº¦ï¼šä¸¤ä¸ªæ¨¡æ€çš„Top-3æ˜¯å¦æœ‰é‡å 
    img_top3 = [c for c, _ in img_results[:3]]
    text_top3 = [c for c, _ in text_results[:3]]
    topk_overlap = any(c in text_top3 for c in img_top3)
    
    # 5. å„æ¨¡æ€ç½®ä¿¡åº¦ï¼šæ¯ä¸ªæ¨¡æ€å†…éƒ¨çš„Top-1æ¦‚ç‡
    img_confidence = max(softmax_probabilities(img_results).values())
    text_confidence = max(softmax_probabilities(text_results).values())
    
    return {
        'fused_confidence': fused_top1_prob,
        'fused_margin': fused_margin,
        'modality_consistent': modality_consistent,
        'topk_overlap': topk_overlap,
        'img_confidence': img_confidence,
        'text_confidence': text_confidence
    }
```

**æ­¥éª¤7: åŸºäºç½®ä¿¡åº¦çš„æœ€ç»ˆå†³ç­–**
```python
def make_final_decision(self, best_category, confidence_metrics):
    """
    å¿«æ€è€ƒçš„æœ€ç»ˆå†³ç­–ï¼šè¾“å‡ºç»“æœè¿˜æ˜¯è½¬å…¥æ…¢æ€è€ƒ
    """
    # å†³ç­–1: é«˜ç½®ä¿¡åº¦ç›´æ¥è¾“å‡º
    if (confidence_metrics['fused_confidence'] >= 0.6 and 
        confidence_metrics['fused_margin'] >= 0.12):
        return best_category, "high_confidence", False  # æ— éœ€æ…¢æ€è€ƒ
    
    # å†³ç­–2: æ¨¡æ€ä¸€è‡´ä¸”ç½®ä¿¡åº¦è¶³å¤Ÿ
    if (confidence_metrics['modality_consistent'] and
        confidence_metrics['img_confidence'] >= 0.5 and
        confidence_metrics['text_confidence'] >= 0.5):
        return best_category, "consistent", False
    
    # å†³ç­–3: Top-Kæœ‰é‡å ä¸”èåˆç½®ä¿¡åº¦è¾ƒé«˜
    if (confidence_metrics['topk_overlap'] and
        confidence_metrics['fused_confidence'] >= 0.54):
        return best_category, "overlap_confident", False
    
    # å†³ç­–4: UCB-LCBåŠ¨æ€é˜ˆå€¼åˆ¤æ–­
    lcb_value = calculate_lcb(best_category, confidence_scores)
    if lcb_value >= 0.7:
        return best_category, "lcb_confident", False
    
    # éœ€è¦æ…¢æ€è€ƒ
    return "uncertain", "need_slow_thinking", True
```

### é˜¶æ®µ3ï¼šæ™ºèƒ½è§¦å‘æœºåˆ¶

#### 3.1 å¤šå±‚çº§è§¦å‘åˆ¤æ–­
```python
def trigger_lcb(self, img_category, text_category, img_confidence, text_confidence,
                fused_top1, fused_top1_prob, fused_margin, topk_overlap, name_soft_agree):
    
    # å±‚çº§1: é«˜ç½®ä¿¡åº¦å¿«é€Ÿè¿”å›
    if fused_top1_prob >= 0.6 and fused_margin >= 0.12:
        return False, fused_top1, fused_top1_prob  # æ— éœ€æ…¢æ€è€ƒ
    
    # å±‚çº§2: åŒæ¨¡æ€ä¸€è‡´æ€§æ£€æŸ¥
    categories_match = is_similar(img_category, text_category, threshold=0.7) or name_soft_agree
    if categories_match and img_confidence >= 0.5 and text_confidence >= 0.5:
        return False, fused_top1, max(img_confidence, text_confidence)
    
    # å±‚çº§3: Top-Ké‡å éªŒè¯
    if topk_overlap and fused_top1_prob >= 0.54:  # 0.6 * 0.9
        return False, fused_top1, fused_top1_prob
    
    # å±‚çº§4: UCB-LCBåŠ¨æ€é˜ˆå€¼ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
    lcb_value = self.calculate_lcb(fused_top1, confidence_scores)
    if lcb_value >= 0.7:  # LCBé˜ˆå€¼
        return False, fused_top1, fused_top1_prob
    
    # éœ€è¦æ…¢æ€è€ƒ
    return True, "conflict", (img_confidence + text_confidence) / 2
```

#### 3.2 UCB-LCBåŠ¨æ€é˜ˆå€¼è®¡ç®—
```python
def calculate_lcb(self, category: str, confidence_scores: List[float]) -> float:
    """åŸºäºUCBç†è®ºçš„Lower Confidence Boundè®¡ç®—"""
    stats = self.category_stats[category]
    n_raw = stats["n"]  # å†å²é¢„æµ‹æ¬¡æ•°
    m_raw = stats["m"]  # å†å²æ­£ç¡®æ¬¡æ•°
    
    # Betaå…ˆéªŒå¹³æ»‘ï¼ˆè§£å†³å†·å¯åŠ¨ï¼‰
    n = n_raw + self.prior_strength     # é»˜è®¤+2.0
    m = m_raw + self.prior_p * self.prior_strength  # é»˜è®¤+1.2
    p_hat = m / (n + 1e-6)  # ç»éªŒæˆåŠŸç‡
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒç†µï¼ˆåæ˜ æ¨¡å‹çŠ¹è±«ç¨‹åº¦ï¼‰
    if len(confidence_scores) > 1:
        probs = np.array(confidence_scores)
        probs = probs / (probs.sum() + 1e-12)
        entropy = -np.sum(probs * np.log(probs + 1e-12)) / np.log(len(probs) + 1e-12)
    else:
        entropy = 0.0
    
    # UCBç½®ä¿¡åŒºé—´é¡¹
    confidence_term = self.lcb_eta * math.sqrt(
        math.log(max(1, self.total_predictions)) / (2 * n + 1)
    )
    
    # LCBå…¬å¼: p_hat - confidence_term - alpha * entropy
    lcb = p_hat - confidence_term - self.lcb_alpha * entropy
    return max(0.0, min(1.0, lcb))
```

## ğŸ“Š å…³é”®å‚æ•°é…ç½®

### æ¨¡å‹å‚æ•°
```python
# CLIPæ¨¡å‹è·¯å¾„
image_encoder_name = "/home/Dataset/Models/Clip/clip-vit-base-patch32"
text_encoder_name = "/home/Dataset/Models/Clip/clip-vit-base-patch32"

# èåˆå‚æ•°
fusion_weight = 0.05              # å›¾åƒ-æ–‡æœ¬èåˆæƒé‡æ¯”ä¾‹
softmax_temp = 0.07               # Softmaxæ¸©åº¦å‚æ•°
```

### è§¦å‘å‚æ•°
```python
# åŸºç¡€é˜ˆå€¼
fused_conf_threshold = 0.6        # èåˆç½®ä¿¡åº¦é˜ˆå€¼
fused_margin_threshold = 0.12     # èåˆè¾¹é™…å·®å¼‚é˜ˆå€¼
per_modality_conf_threshold = 0.5 # å•æ¨¡æ€ç½®ä¿¡åº¦é˜ˆå€¼

# UCB-LCBå‚æ•°
lcb_threshold = 0.7               # LCBé˜ˆå€¼
prior_strength = 2.0              # Betaå…ˆéªŒå¼ºåº¦
prior_p = 0.6                     # å…ˆéªŒæˆåŠŸæ¦‚ç‡
lcb_eta = 1.0                     # ç½®ä¿¡åŒºé—´ç³»æ•°
lcb_alpha = 0.5                   # ç†µè°ƒèŠ‚ç³»æ•°
```

### æ£€ç´¢å‚æ•°
```python
top_k = 5                         # æ£€ç´¢Top-Kæ•°é‡
topk_for_overlap = 3              # ç”¨äºé‡å åˆ¤æ–­çš„Top-K
similarity_threshold = 0.7        # ç›¸ä¼¼åº¦é˜ˆå€¼
```

## ğŸ¯ å¿«æ€è€ƒçš„å·¥ä½œæœºåˆ¶

### 1. ç‰¹å¾æå–
```python
# å›¾åƒç‰¹å¾æå–ï¼ˆCLIPï¼‰
def extract_image_feat(self, image_path):
    image = Image.open(image_path).convert("RGB")
    inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
    with torch.no_grad():
        feat = self.clip_model.get_image_features(**inputs).cpu().numpy()
    feat = feat.flatten()
    # L2å½’ä¸€åŒ–
    feat = feat / (np.linalg.norm(feat) + 1e-12)
    return feat  # 512ç»´å‘é‡

# æ–‡æœ¬ç‰¹å¾æå–ï¼ˆCLIPï¼‰
def extract_text_feat(self, text):
    inputs = self.clip_processor(text=[text], return_tensors="pt", 
                                padding=True, truncation=True).to(self.device)
    with torch.no_grad():
        feat = self.clip_model.get_text_features(**inputs).cpu().numpy()
    feat = feat.flatten()
    # L2å½’ä¸€åŒ–  
    feat = feat / (np.linalg.norm(feat) + 1e-12)
    return feat  # 512ç»´å‘é‡
```

### 2. ç›¸ä¼¼åº¦è®¡ç®—
```python
# ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆL2å½’ä¸€åŒ–åç­‰ä»·äºç‚¹ç§¯ï¼‰
similarity = np.dot(query_feat, knowledge_base_feat)
```

### 3. èåˆç­–ç•¥æƒé‡
```python
# æœ€ç»ˆèåˆåˆ†æ•°è®¡ç®—
final_score = (
    fusion_weight * img_prob +           # 0.05 * å›¾åƒæ¦‚ç‡
    (1 - fusion_weight) * text_prob +    # 0.95 * æ–‡æœ¬æ¦‚ç‡  
    0.1 * (img_rrf + text_rrf)          # 0.1 * RRFåˆ†æ•°
)
```

## âš¡ æ€§èƒ½ç‰¹ç‚¹

### ä¼˜åŠ¿
1. **é«˜æ•ˆæ€§**ï¼šåŸºäºCLIPçš„å¿«é€Ÿç‰¹å¾æå–å’Œæ£€ç´¢
2. **å‡†ç¡®æ€§**ï¼šåŒæ¨¡æ€ä¿¡æ¯äº’è¡¥æå‡è¯†åˆ«ç²¾åº¦
3. **æ™ºèƒ½æ€§**ï¼šå¤šå±‚è§¦å‘æœºåˆ¶è‡ªé€‚åº”é€‰æ‹©å¤„ç†ç­–ç•¥
4. **é²æ£’æ€§**ï¼šRRF+æ¦‚ç‡èåˆæä¾›ç¨³å®šçš„ç»“æœ

### é€‚ç”¨åœºæ™¯
- **ç®€å•æ ·æœ¬**ï¼šè§†è§‰ç‰¹å¾æ˜æ˜¾ï¼Œç±»é—´å·®å¼‚å¤§
- **é«˜æ•ˆå¤„ç†**ï¼šéœ€è¦å¿«é€Ÿå“åº”çš„åº”ç”¨åœºæ™¯
- **åŸºç¡€åˆ†ç±»**ï¼šå¸¸è§ç±»åˆ«çš„è¯†åˆ«ä»»åŠ¡

### å¤„ç†èƒ½åŠ›
- **è¦†ç›–ç‡**ï¼šå¤„ç†çº¦70%çš„æŸ¥è¯¢æ ·æœ¬
- **å‡†ç¡®ç‡**ï¼šç®€å•æ ·æœ¬ä¸Šè¾¾åˆ°85-90%çš„å‡†ç¡®ç‡
- **é€Ÿåº¦**ï¼šå•æ¬¡æ¨ç†æ—¶é—´ < 100ms

## ğŸ”„ ä¸æ…¢æ€è€ƒçš„åä½œ

å½“å¿«æ€è€ƒçš„è§¦å‘æœºåˆ¶åˆ¤æ–­éœ€è¦æ…¢æ€è€ƒæ—¶ï¼š
1. **ä¼ é€’ä¿¡æ¯**ï¼šå°†åŒæ¨¡æ€æ£€ç´¢ç»“æœã€èåˆç»“æœã€ç½®ä¿¡åº¦ç­‰ä¼ é€’ç»™æ…¢æ€è€ƒ
2. **æä¾›ä¸Šä¸‹æ–‡**ï¼šä¸ºæ…¢æ€è€ƒçš„å›°éš¾ç‚¹åˆ†ææä¾›åˆå§‹åˆ¤æ–­
3. **è´¨é‡åé¦ˆ**ï¼šé€šè¿‡LCBæœºåˆ¶æŒç»­ä¼˜åŒ–è§¦å‘é˜ˆå€¼

## ğŸ“‹ å¿«æ€è€ƒå®Œæ•´æµç¨‹æ€»ç»“

### ç«¯åˆ°ç«¯æ£€ç´¢å¼åˆ†ç±»æµç¨‹

```python
def fast_thinking_pipeline(query_image_path):
    """å¿«æ€è€ƒçš„å®Œæ•´æ£€ç´¢æµç¨‹"""
    
    # === é˜¶æ®µ1: åŒæ¨¡æ€æ£€ç´¢ ===
    # 1.1 å›¾åƒ-å›¾åƒæ£€ç´¢
    query_img_feat = CLIP_image_encode(query_image)  # 512ç»´ç‰¹å¾
    img_similarities = []
    for category, template_img_feat in image_knowledge_base.items():
        similarity = cosine_similarity(query_img_feat, template_img_feat)
        img_similarities.append((category, similarity))
    img_results = sorted(img_similarities, reverse=True)[:5]
    # ç»“æœ: [("Chihuahua", 0.87), ("Poodle", 0.73), ("Husky", 0.69), ...]
    
    # 1.2 å›¾åƒ-æ–‡æœ¬æ£€ç´¢ï¼ˆè·¨æ¨¡æ€ï¼‰
    text_similarities = []
    for category, template_text_feat in text_knowledge_base.items():
        similarity = cosine_similarity(query_img_feat, template_text_feat)  # è·¨æ¨¡æ€
        text_similarities.append((category, similarity))
    text_results = sorted(text_similarities, reverse=True)[:5]
    # ç»“æœ: [("German Shepherd", 0.91), ("Chihuahua", 0.84), ("Husky", 0.68), ...]
    
    # === é˜¶æ®µ2: æ™ºèƒ½èåˆ ===
    # 2.1 æ¦‚ç‡è½¬æ¢ + RRFèåˆ
    img_probs = softmax_with_temperature(img_results, temp=0.07)
    text_probs = softmax_with_temperature(text_results, temp=0.07)
    img_rrf = reciprocal_rank_fusion(img_results)
    text_rrf = reciprocal_rank_fusion(text_results)
    
    # 2.2 åŠ æƒèåˆ
    fused_scores = {}
    for category in all_categories:
        score = (0.05 * img_probs[category] + 
                0.95 * text_probs[category] + 
                0.1 * (img_rrf[category] + text_rrf[category]))
        fused_scores[category] = score
    
    fused_results = sorted(fused_scores.items(), reverse=True)
    fused_top1 = fused_results[0][0]  # æœ€ç»ˆé¢„æµ‹ç±»åˆ«
    fused_confidence = fused_results[0][1]
    
    # === é˜¶æ®µ3: æ™ºèƒ½è§¦å‘åˆ¤æ–­ ===
    # 3.1 å¤šå±‚è§¦å‘æ£€æŸ¥
    if fused_confidence >= 0.6 and margin >= 0.12:
        return fused_top1, fused_confidence, False  # æ— éœ€æ…¢æ€è€ƒ
    
    if categories_consistent and individual_confidences_high:
        return fused_top1, fused_confidence, False
    
    if topk_overlap and fused_confidence >= 0.54:
        return fused_top1, fused_confidence, False
    
    # 3.2 UCB-LCBåŠ¨æ€åˆ¤æ–­
    lcb_value = calculate_lcb(fused_top1, confidence_scores)
    if lcb_value >= 0.7:
        return fused_top1, fused_confidence, False
    
    # éœ€è¦æ…¢æ€è€ƒ
    return "uncertain", low_confidence, True
```

### å…³é”®ç‰¹ç‚¹å¼ºè°ƒ

1. **æ£€ç´¢å¼æ¶æ„**ï¼šä¸ç›´æ¥åˆ†ç±»ï¼Œé€šè¿‡æ£€ç´¢é¢„æ„å»ºçŸ¥è¯†åº“å®ç°åˆ†ç±»
2. **åŒæ¨¡æ€æ£€ç´¢**ï¼šåŒæ—¶è¿›è¡Œå›¾åƒ-å›¾åƒå’Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢
3. **æ™ºèƒ½èåˆ**ï¼šRRF+æ¦‚ç‡+æ¸©åº¦ç¼©æ”¾çš„å¤šé‡èåˆç­–ç•¥  
4. **è‡ªé€‚åº”è§¦å‘**ï¼šåŸºäºUCBç†è®ºçš„åŠ¨æ€é˜ˆå€¼åˆ¤æ–­æœºåˆ¶

### ä¸ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•å¯¹æ¯”

| ç‰¹å¾ | å¿«æ€è€ƒ(æ£€ç´¢å¼) | ä¼ ç»ŸCLIPåˆ†ç±» |
|------|---------------|-------------|
| **å·¥ä½œæ–¹å¼** | æ£€ç´¢çŸ¥è¯†åº“æ¨¡æ¿ | ç›´æ¥è¾“å‡ºæ¦‚ç‡ |
| **æ‰©å±•æ€§** | âœ… æ— éœ€é‡è®­ç»ƒ | âŒ éœ€è¦é‡æ–°è®­ç»ƒ |
| **å°‘æ ·æœ¬** | âœ… æ¯ç±»å‡ ä¸ªæ ·æœ¬ | âŒ éœ€è¦å¤§é‡æ ·æœ¬ |
| **å¯è§£é‡Šæ€§** | âœ… èƒ½çœ‹åˆ°åŒ¹é…æ ·æœ¬ | âŒ é»‘ç›’è¾“å‡º |
| **è®¡ç®—æ•ˆç‡** | âœ… ä»…ç‰¹å¾æå–+æ£€ç´¢ | âœ… ä»…ä¸€æ¬¡å‰å‘ä¼ æ’­ |
| **ç»†ç²’åº¦é€‚åº”** | âœ… ä¸“é—¨ä¼˜åŒ– | âš ï¸ é€šç”¨æ€§è¾ƒå¼º |

## ğŸ“ å®Œæ•´ç¤ºä¾‹ï¼šå¿«æ€è€ƒåˆ†ç±»ä¾æ®å®æˆ˜

### ğŸ• å®é™…æ¡ˆä¾‹ï¼šè¯†åˆ«å‰å¨ƒå¨ƒ

å‡è®¾æŸ¥è¯¢ä¸€å¼ å‰å¨ƒå¨ƒçš„å›¾ç‰‡ï¼Œä»¥ä¸‹æ˜¯å®Œæ•´çš„å¤„ç†è¿‡ç¨‹ï¼š

#### æ­¥éª¤1: åŒæ¨¡æ€æ£€ç´¢å¾—åˆ°TOP-K
```python
# å›¾åƒ-å›¾åƒæ£€ç´¢ç»“æœ
img_results = [
    ("Chihuahua", 0.87),        # è§†è§‰æœ€ç›¸ä¼¼
    ("Poodle", 0.73),
    ("Yorkshire Terrier", 0.69),
    ("Pomeranian", 0.65),
    ("Papillon", 0.61)
]

# å›¾åƒ-æ–‡æœ¬æ£€ç´¢ç»“æœï¼ˆè·¨æ¨¡æ€ï¼‰
text_results = [
    ("Chihuahua", 0.84),        # è¯­ä¹‰æœ€åŒ¹é…
    ("Pomeranian", 0.78),
    ("Yorkshire Terrier", 0.72),
    ("Papillon", 0.68),
    ("Maltese", 0.63)
]
```

#### æ­¥éª¤2: æ¦‚ç‡è½¬æ¢
```python
# æ¸©åº¦ç¼©æ”¾Softmax (temperature=0.07)
img_probs = {
    "Chihuahua": 0.52,         # 0.87ç»è¿‡softmaxå
    "Poodle": 0.21,
    "Yorkshire Terrier": 0.15,
    "Pomeranian": 0.08,
    "Papillon": 0.04
}

text_probs = {
    "Chihuahua": 0.35,         # 0.84ç»è¿‡softmaxå
    "Pomeranian": 0.28,
    "Yorkshire Terrier": 0.19,
    "Papillon": 0.13,
    "Maltese": 0.05
}
```

#### æ­¥éª¤3: RRFåˆ†æ•°è®¡ç®—
```python
# RRF = 1 / (60 + rank)
img_rrf = {
    "Chihuahua": 1/61 = 0.0164,      # æ’å1
    "Poodle": 1/62 = 0.0161,         # æ’å2
    "Yorkshire Terrier": 1/63 = 0.0159,  # æ’å3
    ...
}

text_rrf = {
    "Chihuahua": 1/61 = 0.0164,      # æ’å1  
    "Pomeranian": 1/62 = 0.0161,     # æ’å2
    "Yorkshire Terrier": 1/63 = 0.0159,  # æ’å3
    ...
}
```

#### æ­¥éª¤4: æœ€ç»ˆèåˆåˆ†æ•°è®¡ç®—
```python
# ä¸ºæ¯ä¸ªå€™é€‰ç±»åˆ«è®¡ç®—æœ€ç»ˆèåˆåˆ†æ•°
categories = ["Chihuahua", "Poodle", "Yorkshire Terrier", "Pomeranian", "Papillon", "Maltese"]

final_scores = {}
for category in categories:
    # è·å–å„é¡¹åˆ†æ•°
    p_img = img_probs.get(category, 0.0)
    p_txt = text_probs.get(category, 0.0)  
    rrf_img = img_rrf.get(category, 0.0)
    rrf_txt = text_rrf.get(category, 0.0)
    
    # åŠ æƒèåˆå…¬å¼
    final_score = 0.05 * p_img + 0.95 * p_txt + 0.1 * (rrf_img + rrf_txt)
    final_scores[category] = final_score

# è®¡ç®—ç»“æœï¼š
final_scores = {
    "Chihuahua": 0.05*0.52 + 0.95*0.35 + 0.1*(0.0164+0.0164) = 0.026 + 0.333 + 0.00328 = 0.362,
    "Pomeranian": 0.05*0.08 + 0.95*0.28 + 0.1*(0.0157+0.0161) = 0.004 + 0.266 + 0.00318 = 0.273,
    "Yorkshire Terrier": 0.05*0.15 + 0.95*0.19 + 0.1*(0.0159+0.0159) = 0.0075 + 0.181 + 0.00318 = 0.192,
    "Poodle": 0.05*0.21 + 0.95*0.0 + 0.1*(0.0161+0.0) = 0.0105 + 0 + 0.00161 = 0.012,
    ...
}

# æ’åºç»“æœï¼š
final_ranking = [
    ("Chihuahua", 0.362),     # âœ… æœ€ä½³ç±»åˆ«
    ("Pomeranian", 0.273),
    ("Yorkshire Terrier", 0.192),
    ("Poodle", 0.012),
    ...
]
```

#### æ­¥éª¤5: æœ€ä½³ç±»åˆ«é€‰æ‹©
```python
# é€‰æ‹©èåˆåˆ†æ•°æœ€é«˜çš„ç±»åˆ«
best_category = "Chihuahua"       # æœ€ç»ˆé¢„æµ‹
best_score = 0.362               # æœ€é«˜èåˆåˆ†æ•°
fused_confidence = 0.68          # ç»è¿‡æœ€ç»ˆsoftmaxåçš„æ¦‚ç‡
fused_margin = 0.15              # ä¸ç¬¬äºŒåçš„å·®è·
```

#### æ­¥éª¤6: è§¦å‘åˆ¤æ–­
```python
# è¯„ä¼°æ˜¯å¦éœ€è¦æ…¢æ€è€ƒ
confidence_check = fused_confidence >= 0.6 and fused_margin >= 0.12
# 0.68 >= 0.6 âœ… ä¸” 0.15 >= 0.12 âœ…

decision = "ç›´æ¥è¾“å‡ºï¼Œæ— éœ€æ…¢æ€è€ƒ"
```

### ğŸ¯ å¿«æ€è€ƒåˆ†ç±»ä¾æ®æ€»ç»“

#### æ ¸å¿ƒåˆ†ç±»ä¾æ®å…¬å¼ï¼š
```python
æœ€ä½³ç±»åˆ« = argmax(0.05 * P_image + 0.95 * P_text + 0.1 * (RRF_image + RRF_text))
```

#### åˆ†ç±»ä¾æ®çš„5ä¸ªç»„æˆè¦ç´ ï¼š

1. **å›¾åƒæ¦‚ç‡(5%æƒé‡)**: åŸºäºè§†è§‰ç›¸ä¼¼åº¦çš„Softmaxæ¦‚ç‡
2. **æ–‡æœ¬æ¦‚ç‡(95%æƒé‡)**: åŸºäºè·¨æ¨¡æ€åŒ¹é…çš„Softmaxæ¦‚ç‡
3. **å›¾åƒRRF(5%æƒé‡)**: åŸºäºå›¾åƒæ£€ç´¢æ’åºçš„å€’æ•°èåˆåˆ†æ•°  
4. **æ–‡æœ¬RRF(5%æƒé‡)**: åŸºäºæ–‡æœ¬æ£€ç´¢æ’åºçš„å€’æ•°èåˆåˆ†æ•°
5. **å¤šç»´ç½®ä¿¡åº¦éªŒè¯**: ç”¨äºè§¦å‘åˆ¤æ–­è€Œéåˆ†ç±»é€‰æ‹©

#### é€‰æ‹©æœ€ä½³ç±»åˆ«çš„åˆ¤æ–­æ ‡å‡†ï¼š

- **ä¸»è¦æ ‡å‡†**: èåˆåˆ†æ•°æœ€é«˜ (argmax)
- **æƒé‡åå‘**: è·¨æ¨¡æ€æ£€ç´¢(95%) > çº¯è§†è§‰æ£€ç´¢(5%)
- **ç¨³å®šæ€§ä¿è¯**: RRFåˆ†æ•°æä¾›æ’åºé²æ£’æ€§
- **ç½®ä¿¡åº¦é—¨æ§**: å¤šå±‚ç½®ä¿¡åº¦æ£€æŸ¥å†³å®šæ˜¯å¦è¾“å‡º

å¿«æ€è€ƒé€šè¿‡è¿™ç§**å¤šé‡èåˆ+æ™ºèƒ½è§¦å‘**çš„æœºåˆ¶ï¼Œå®ç°äº†é«˜æ•ˆã€å‡†ç¡®ã€å¯é çš„ç»†ç²’åº¦è§†è§‰è¯†åˆ«ï¼Œæ˜¯æ•´ä¸ªç³»ç»Ÿçš„**ç¬¬ä¸€é“é˜²çº¿**ã€‚
