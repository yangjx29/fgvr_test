/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Namespace(mode='build_knowledge_base', config_file_env='./configs/env_machine.yml', config_file_expt='./configs/expts/pet37_all.yml', num_per_category='1', kshot=None, region_num=None, superclass=None, gallery_out=None, fusion_method='concat', knowledge_base_dir='./experiments/pet37/knowledge_base', query_image=None, test_data_dir=None, results_out='./results.json', use_slow_thinking=None, confidence_threshold=0.8, similarity_threshold=0.7, enable_mllm_intermediate_judge=False)
{'setup': 'ours', 'experiment': 5.0, 'dataset_name': 'pet', 'num_classes': 37, 'num_base': 19, 'num_novel': 18, 'seed': 1, 'batch_size': 32, 'clustering_method': 'multi_clip_voting', 'model_size': 'ViT-B/16', 'model_size_vqa': 'FlanT5-XXL', 'model_size_mllm': 'Qwen2.5-VL-7B', 'model_type_llm': 'gpt-3.5-turbo', 'image_size': 224, 'verbose': False, 'host': 'xiao', 'num_workers': 16, 'device': 'cuda', 'device_count': '1', 'device_id': '0', 'data_dir': './datasets/pet_37', 'expt_dir': './experiments/pet37', 'expt_dir_describe': './experiments/pet37/describe', 'path_vqa_questions': './experiments/pet37/describe/pet_vqa_questions_ours', 'path_vqa_answers': './experiments/pet37/describe/pet_attributes_pairs', 'path_llm_prompts': './experiments/pet37/describe/pet_llm_prompts', 'path_identify_answers': './experiments/pet37/identify', 'expt_dir_guess': './experiments/pet37/guess', 'path_llm_replies_raw': './experiments/pet37/guess/pet_llm_replies_raw', 'path_llm_replies_jsoned': './experiments/pet37/guess/pet_llm_replies_jsoned', 'path_llm_gussed_names': './experiments/pet37/guess/pet_llm_gussed_names', 'expt_dir_gallery': './experiments/pet37/gallery', 'path_references': './experiments/pet37/gallery/pet_image_references', 'path_regions': './experiments/pet37/gallery/pet_image_regions', 'path_descriptions': './experiments/pet37/gallery/pet_image_descriptions_attn', 'expt_dir_grouping': './experiments/pet37/grouping'}
初始化MLLM模型...
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [03:05<12:20, 185.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [09:22<14:54, 298.30s/it]Loading checkpoint shards:  60%|██████    | 3/5 [15:52<11:20, 340.09s/it]Loading checkpoint shards:  80%|████████  | 4/5 [22:46<06:09, 369.21s/it]Loading checkpoint shards: 100%|██████████| 5/5 [23:26<00:00, 250.72s/it]Loading checkpoint shards: 100%|██████████| 5/5 [23:26<00:00, 281.39s/it]
local_model_path: /home/Dataset/Models/Qwen/Qwen2.5-VL-7B-Instruct
初始化知识库构建器...
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [09:32<38:08, 572.14s/it]Loading checkpoint shards:  40%|████      | 2/5 [14:20<20:15, 405.01s/it]Loading checkpoint shards:  60%|██████    | 3/5 [19:24<11:58, 359.17s/it]Loading checkpoint shards:  80%|████████  | 4/5 [24:12<05:30, 330.80s/it]Loading checkpoint shards: 100%|██████████| 5/5 [27:22<00:00, 280.20s/it]Loading checkpoint shards: 100%|██████████| 5/5 [27:22<00:00, 328.52s/it]
Traceback (most recent call last):
  File "/home/hdl/project/fgvr_test/discovering.py", line 393, in <module>
    system = FastSlowThinkingSystem(
  File "/home/hdl/project/fgvr_test/fast_slow_thinking_system.py", line 67, in __init__
    self.kb_builder = KnowledgeBaseBuilder(
  File "/home/hdl/project/fgvr_test/knowledge_base_builder.py", line 51, in __init__
    self.retrieval = MultimodalRetrieval(
  File "/home/hdl/project/fgvr_test/retrieval/multimodal_retrieval.py", line 58, in __init__
    def _load_blip_model(self):
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
  File "/home/hdl/miniconda3/envs/finer_dynamic/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 47.40 GiB of which 74.25 MiB is free. Including non-PyTorch memory, this process has 47.32 GiB memory in use. Of the allocated memory 46.72 GiB is allocated by PyTorch, and 358.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
