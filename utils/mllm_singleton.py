"""
MLLMå•ä¾‹ç®¡ç†å™¨
é˜²æ­¢Qwenæ¨¡å‹é‡å¤åŠ è½½å¯¼è‡´æ˜¾å­˜çˆ†æ»¡
"""

import torch
from typing import Optional

class MLLMSingleton:
    """MLLMå•ä¾‹ç®¡ç†å™¨ï¼Œç¡®ä¿æ•´ä¸ªç³»ç»ŸåªåŠ è½½ä¸€æ¬¡MLLMæ¨¡å‹"""
    
    _instance: Optional['MLLMSingleton'] = None
    _mllm_bot = None
    _model_tag = None
    _device = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MLLMSingleton, cls).__new__(cls)
        return cls._instance
    
    def get_mllm_bot(self, model_tag: str = "Qwen2.5-VL-7B", device: str = "cuda"):
        """
        è·å–MLLMæ¨¡å‹å®ä¾‹ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™å¤ç”¨ï¼Œå¦åˆ™åˆ›å»ºæ–°å®ä¾‹
        
        Args:
            model_tag: æ¨¡å‹æ ‡ç­¾
            device: è®¾å¤‡ç±»å‹
            
        Returns:
            MLLMBotå®ä¾‹
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½ï¼ˆæ¨¡å‹æˆ–è®¾å¤‡å‘ç”Ÿå˜åŒ–ï¼‰
        if (self._mllm_bot is None or 
            self._model_tag != model_tag or 
            self._device != device):
            
            # æ¸…ç†æ—§æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self._mllm_bot is not None:
                print(f"ğŸ—‘ï¸ æ¸…ç†æ—§çš„MLLMæ¨¡å‹: {self._model_tag}")
                del self._mllm_bot
                if device == "cuda":
                    torch.cuda.empty_cache()
            
            # åˆ›å»ºæ–°æ¨¡å‹
            print(f"ğŸš€ åˆå§‹åŒ–MLLMæ¨¡å‹: {model_tag} on {device}")
            print(f"ğŸ“Š å½“å‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
            
            from agents.mllm_bot import MLLMBot
            self._mllm_bot = MLLMBot(
                model_tag=model_tag,
                model_name=model_tag,
                device=device
            )
            self._model_tag = model_tag
            self._device = device
            
            print(f"âœ… MLLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            print(f"ğŸ“Š åŠ è½½åæ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
        else:
            print(f"â™»ï¸ å¤ç”¨å·²åŠ è½½çš„MLLMæ¨¡å‹: {model_tag}")
        
        return self._mllm_bot
    
    def clear_cache(self):
        """æ¸…ç†MLLMç¼“å­˜ï¼Œé‡Šæ”¾æ˜¾å­˜"""
        if self._mllm_bot is not None:
            print(f"ğŸ—‘ï¸ æ¸…ç†MLLMæ¨¡å‹ç¼“å­˜")
            del self._mllm_bot
            self._mllm_bot = None
            self._model_tag = None
            self._device = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def get_memory_usage(self) -> dict:
        """è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            return {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "usage_percent": torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100
            }
        else:
            return {"error": "CUDA not available"}


# å…¨å±€å•ä¾‹å®ä¾‹
_mllm_manager = MLLMSingleton()

def get_mllm_bot(model_tag: str = "Qwen2.5-VL-7B", device: str = "cuda"):
    """
    è·å–MLLMæ¨¡å‹å®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_tag: æ¨¡å‹æ ‡ç­¾
        device: è®¾å¤‡ç±»å‹
        
    Returns:
        MLLMBotå®ä¾‹
    """
    return _mllm_manager.get_mllm_bot(model_tag, device)

def clear_mllm_cache():
    """æ¸…ç†MLLMç¼“å­˜çš„ä¾¿æ·å‡½æ•°"""
    _mllm_manager.clear_cache()

def get_memory_usage():
    """è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µçš„ä¾¿æ·å‡½æ•°"""
    return _mllm_manager.get_memory_usage()
